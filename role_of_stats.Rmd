# Role of Statistics in the Research Process

In this book we are concerned with experimental psychology, in particular the statistical analysis of experiments in psychology and related disciplines such as language science, behavioural science, cognitive science, or neuroscience. The order expressed in the previous sentence -- science first and statistical analysis second -- is one of the overarching principles with which we will think about statistics. Whereas the goal here is to introduce the concepts and techniques required to perform a statistical analysis of (mostly experimental) data, the perspective taken here is that a statistical analysis can only be performed or understood within the scientific context it takes place in.

One consequence of this perspective is that the start point of any statistical analysis needs to be a specific and clear research question. In the case in which both data collection and analysis is guided by such a research question, a statistical analysis is generally an indispensable part of the research process. As we will describe in more detail in this and the coming chapters, statistics is the tool that allows us to draw inferences that go beyond the data we have observed. In more technical terms, statistics is the one tool in the research tool kit that allows us to generalise from the current **sample** to a **population** from which this sample is drawn (with a number of caveats we will also discuss). This ability to generalise is what allows us to connect experimental results with the research questions and underlying theories. In sum, the statistical techniques introduced in this book can provide meaningful and scientifically helpful answers when the data is collected and analysed with a clear research question in mind.

## The Research Process

The research process in psychology and related sciences roughly consists of four interrelated steps: the research question, the operationalisation of the research question and the data collection, the statistical analysis of the data, and finally the communication of the results. Let us explain this process in more detail.

Any research should begin with a **research question**. For the disciplines considered here this is generally a theory or hypothesis about human behaviour or the human mind. For example, a widely accepted idea in decision making and behavioural science is that people exhibit *loss aversion* -- the displeasure resulting from losing £10 is stronger than the pleasure derived from winning £10. We will look at this example in more detail in this chapter but will also consider other examples below. For now, it is sufficient to see that research questions often involve general statement that involve not directly observable quantities (such as pleasure or displeasure).

The next step in the research process is the transformation of the research question into an empirical and statistical hypothesis, we can call this step **operationalisation** or **measurement**. As the example of loss aversion shows, research questions are often general. Consequently, there are a multitude of possible studies that can be performed to investigate one research question. However, for any specific study a researchers needs to decide on one specific study design. What exactly are we measuring to investigate the question we are interested in?

Once the research question has been operationalised and the corresponding data is collected it is time for the **statistical analysis**. Generally, the statistical analysis answers one specific question: Does the data provide evidence for or against the empirical hypothesis derived from the research question? The remainder of the book will show in detail how to perform statistical analyses for common study designs and how to interpret the results in light of the research question and operationalisation.

Once the data is sufficiently analysed, we have reached the final step of the research process, we need to **communicate the results**. There are different forms of results communication depending on your goal and audience (e.g., scientific journal article, dissertation report, conference presentation, or a press release). Whereas the different forms differ in the amount of detail and background that that is provided, they all need to provide a truthful and comprehensive account of the whole research process: What is the research question? How was it investigated? What are the results? What does this mean for the research question? Often, the difficult problem to solve during this step is to provide a comprehensive and truthful account but in a succinct manner. One important tool for doing so is through graphical means -- results figures. Consequently, in this book we will discuss both how to present statistical results in a text and how to create appropriate results figures.

What this abstract overview of the research process shows is three important things.

(1) The primacy of the research question. The research question determines the operationalisation and thus which data is collected. The research question also determines the statistical analysis, but indirectly; the research question determines the empirical hypothesis which is then tested in the analysis.

(2) The statistically analysis is not directly connected to the research question. The statistical analysis is performed on the operationalisation of the research question, but not on the research question itself. What this means is that the statical analysis itself cannot directly inform us about the research question, or in other words, we cannot statistically test the research question itself. Instead, statistics can only tell us something about a specific operationalisation. Whether or not this allows strong inferences about the research question itself depends on the operationalisation. And as shown in the following examples, an important part of the scientific discourse is to argue whether certain operationalisations allow one to address specific research questions. However, this is generally not a statistical question.

(3) Statistics is not the end goal of the research. Instead, the end goal is usually a written communication of the research. In most cases, the statistical analysis is an indispensable part of this communication that can provide evidence for or against a specific empirical analysis. However, to understand the full meaning and implications of a particular statistical result, it is important to know its context -- the research question and its operationalisation. It is the task of the research to communicate this context when communicating the research. Without the context, the impact and meaning of a statistical results is severely limited.

## Example I: Testing a New Therapy

To get a better understanding of the research process and the problems that can arise in it, let us consider a first example research question and how it can be investigated empirically. In many clinical domains, a frequent research question is whether a new therapeutical intervention works better than an existing one. The basic study design for this type of research question is quite clear, a *randomized controlled trial*. We want to compare the new treatment with a control treatment in an experimental setting -- eligible participants are randomly assigned to receive either the new or the existing treatment. Whereas the general design of this study is fixed, important questions in the operationalisation step are how to measure the outcome intervention and how to implement the control treatment.

For example, if we are testing a new therapy against a mental health disorder, are we interested if the new treatment reduces symptom as measured by a questionnaire, reduces the number of sick days, or the number of recurring episodes? Depending on the differences between the new and old treatment it is possible that one treatment does better on one of this measures whereas the other is better for another measure. When performing the research we need to consider these different possibilities before we can collect any data. And if we collect multiple measures, we should determine what our primary outcome is and how we deal with the additional outcomes.

A further problem when comparing therapeutical interventions is how exactly to set up the control condition. If every therapists involved in the study is a proponent of the new treatment -- which commonly happens as tests of new treatments are often done by those developing the new treatment -- they are unlikely to invest the same amount of effort in the control treatment than in the new treatment. This difference in effort, also known as [*allegiance effects*](https://en.wikipedia.org/wiki/Allegiance_bias), can affect the results independent of whether or not the new therapy is actually more effective.

Consider a trial in which we have decided on one outcome measure, but therapists are more invested in the new treatment than in the old treatment. After we have collected the data, we run the statistical analysis which provides evidence that the new treatment performs better than the control treatment on the selected outcome measure. What would this mean for the research question? The researchers performing such a study would probably conclude that the new treatment is better than the old treatment. However, given the difference in allegiance to the two treatments, such a conclusion seems premature. It is also plausible that the therapists, because of their stronger investment with the new treatment, invested more effort into the patients treated with the new treatment. This provides an *alternative explanation* for the results: This difference in effort could be responsible for the difference in the outcome instead of the difference in the two treatments. To provide compelling evidence that the new treatment is indeed better than the old treatment, the researchers would need to test their new treatment in a setting in which this alternative explanation would not work. For example, by showing that the new treatment is also better than the old treatment for a therapist with an allegiance to the old treatment.

## Example II: The Psychology of Loss Aversion

For our second concrete example, let us consider a less applied research question. *Loss aversion* is one of the assumptions underlying *prospect theory* [@kahnemanProspectTheoryAnalysis1979], a mathematically formalised theory combining cognitive psychology with economic theory.[^role_of_stats-1] The concise description of loss aversion is that "losses loom larger than gains" [@kahnemanProspectTheoryAnalysis1979, p. 279]. Less literary, loss aversion means that the negative psychological impact (or feeling associated with) a loss of a certain amount is larger than the positive psychological impact of a gain of the same amount. For example, loss aversion predicts that the displeasure or pain from losing £10 is larger than the pleasure or joy from winning £10.

[^role_of_stats-1]: Even though prospect theory is over 40 years old, it is still very relevant today with many papers appearing every year building on it or using it. Prospect theory was also one of the contributions for which Daniel Kahneman received the Nobel prize in Economics in 2002. Amos Tverksy, the other developer of prospect theory, died of cancer in 1996 (at the age of 59) and therefore could not receive the Nobel prize (as it is only awarded to living recipients).

We can see that loss aversion is a theoretical statement involving *latent* - that is, unobservable - quantities such as negative or positive feelings (i.e., displeasure versus pleasure). We can ask people how they feel, but we cannot easily observe feelings without asking people. So how can we test whether people indeed show loss aversion if we cannot directly observe the theoretical constructs that form the core of it?

One possibility for testing the hypothesis that individual show loss aversion is hinted at above. We could either give people a certain amount, say £10, or take it away form them, and then ask them how they feel. This procedure runs into at least two problems. First, it is clearly ethically unacceptable to perform an experiment that consists of taking £10 away from some our participants. Second, even if we were to overcome the ethical problems (e.g., by first giving participants an endowment and only take money away from that endowment) there would still be the problem of how to measure the psychological impact of the two events. More specifically, let's say we ask the participants whom we have give £10 how much joy they feel after having received the money on a scale from 1 = no joy to 10 = maximum joy. We also ask those participants from whom we have taken £10 how much pain they have felt after the money was taken from then on a scale from 1 = no pain to 10 = maximum pain. Let us also assume that we indeed observe that participants who gained £10 said that their joy as on average a 4, whereas participants that lost £10 said their pain was on average a 5. Would this indicate evidence for loss aversion? Only if the two scales were directly comparable; that is, only if a 4 on the joy scale would be comparable in psychological impact to a 4 on the pain scale. It is easy to imagine situation in which this would not be the case. For example, if the maximum amount of joy we could perceive is larger than the maximum amount of pain, then the same value on the scale does not correspond to similar psychological impact. Given these problems, testing loss aversion using such an operationalisation is not very common [for an exception see @harinckWhenGainsLoom2007].

### Evidence for Loss Aversion: The Reflection Effect

A more common operationalisation for testing loss aversion is the comparison of choice patterns across different *risky choices, lotteries,* or *gambles* (these terms can be understood interchangeably here), a common experimental paradigm in decision making and behavioural economics. A lottery in this sense consists of different options, each of which associated with one or multiple outcomes, from which the participant has to choose one. For example, one of the lotteries used in @kahnemanProspectTheoryAnalysis1979 was the following.

Which option do you prefer?\
*A*: An 80% chance of \$4,000 or a 20% chance of \$0.\
*B*: 100% guarantee of \$3,000.

In the original study [@kahnemanProspectTheoryAnalysis1979, Problem 3] 95 participants were asked this question. Of those, only 20% chose option *A* and 80% option *B*. Because the lottery above contains only gains (i.e., all outcomes are either positive or zero), these results do not allow any conclusions regarding loss aversion.[^role_of_stats-2]

[^role_of_stats-2]: However, the results reveal another interesting psychological phenomenon; participants do not behave fully rational in this task when comparing the choice pattern with the expected values of the two options. The expected value of an option is given by the sum of each possible outcome multiplied by its probability and corresponds to the long running average expected outcome when playing a lottery over and over. For option *A* we get an expected value of $0.8 \times 4000 + 0.2 \times 0 = 3200$, whereas for option *B* we have an get an expected value of $1 \times 3000 = 3000$. Thus, if we were to play this lottery over and over, we would win an average of \$3,200 each type when choosing option *A*, but only win \$3,000 for each choice of option *B*. A purely rational decision make should therefore choose option *A* over option *B*. In contrast to this, participants who are asked to make one such choice choose option *B* over option *A*.

The study by @kahnemanProspectTheoryAnalysis1979 also contained the corresponding loss only lottery (i.e., all outcomes were either negative or zero):

Which option do you prefer?\
*A*: An 80% chance of -\$4,000 or a 20% chance of \$0.\
*B*: 100% guarantee of -\$3,000.

The same 95 participants as before were asked this question and their results differed markedly from the previous results. For the loss only lottery 92% chose option *A* and only 8% option *B*. Thus, even though the only mathematical difference between both lotteries is the sign of the outcomes (i.e., the probabilities and absolute value of the outcomes are identical across lotteries), participants' preferences are almost reversed. In the gain case, participants prefer the sure option (Option *B*), but in the loss case they prefer the risky outcome (Option *A*).

This results pattern -- preferring the sure option in the gain case, but the risky option in the loss case -- is also known as the *reflection effect* and taken as evidence for loss aversion [@kahnemanProspectTheoryAnalysis1979]. In the gain case, prefer the sure outcome to the risky outcome because they are unwilling to risk gaining nothing, even though the risky option contains the larger potential outcome. In contrast, in the loss case people are more willing to gamble on the risky option than pick the sure outcome, because they are unwilling to lose anything at all, even though the risky option has the potential of resulting in a larger loss than the sure option.

So do these results provide compelling evidence that there is such a thing as loss aversion? Results such as those from @kahnemanProspectTheoryAnalysis1979 that show how differently people behave whether they are dealing with a gain versus when dealing with a loss certainly appear to support this theoretical idea [e.g., @camererThreeCheersPsychological2005]. And this also make sense intuitively. Most people (me included) feel that in the gain case, they would prefer option *A* and in the loss case they would prefer option *B* even though they know and understand the mathematical details of the lotteries. However, as in the previous example, the evidence for loss aversion discussed here hinges on a particular operationalisation of the theoretical idea. From these results, we have not really learned anything about the different psychological impact of a gain and a loss of the same magnitude in the mind of people. The only thing we have learned is that for certain lotteries people change their behaviour depending on whether the outcomes are gains and losses. Can we find an alternative for this data pattern that does not involve loss aversion?

### Alternative Explanation: Loss Aversion or Loss Seeking?

One clever alternative explanation for the why it looks like there is loss aversion was provided by @walasekHowMakeLoss2015. In their study, participants were also presented with lotteries, but they differed in a few aspects from the lotteries used by @kahnemanProspectTheoryAnalysis1979. An example of one of their lotteries is shown in Figure \@ref(fig:lottery-example) below. As shown in the figure, each lottery involved mixed outcomes, that is both gains and losses (i.e., -\$18 and +\$20 in the example). Furthermore, each lottery only consisted of one option and not two. So instead of having to choose between two options, participants only had to choose whether to accept and play a lottery or not. Finally, both possible outcome always had a 50% probability of occuring. To make this logic clearer to participants, they were told that accepting the lottery shown in Figure \@ref(fig:lottery-example) was equal to flipping a coin that has -\$18 on one side and +\$20 on the other side. Depending on which outcome comes out on top, their money would change accordingly. If participants rejected a lottery, they neither lost nor won any money.

```{r lottery-example, fig.cap='Screenshot of lottery task used to investigate loss aversion. This screenshot is from Walasek and Stewart (2015, Figure 1).', echo=FALSE}
knitr::include_graphics("figures/walasek15-task.png")
```

As many experiments that fall within a cognitive domain, the study of @walasekHowMakeLoss2015 consisted of a series of similar trials in which participants had to do the same task (i.e., accept or reject the shown lottery). What differed across trials were the values of the two possible outcomes. For example, in one of the conditions of the experiment losses ranged from -\$6 to -\$20 in increments of -\$2 (resulting in 8 different possible losses) and gains ranged from \$12 to \$40 in increments of \$4 (resulting in 8 different possible gains). Across all trials for one participant in this condition, all possible losses were combined with all possible gains so that in total participants had to decide for $8 \times 8 = 64$ lotteries whether they accepted or rejected it.

When analysing their data, @walasekHowMakeLoss2015 focussed on a mathematical model-based analysis using a variant of prospect theory. Whereas there are clear merits to such an approach, as it it able to provide informative summaries of large amounts of data, it can make it difficult to focus on interesting patterns in subsets of the data. Furthermore, analysis using a mathematical models such as prospect theory requires statistical knowledge that is beyond the scope of the current book. Consequently, we will look at a simple data pattern to see whether the data can tell us something about loss aversion.

The 64 trials each participant of @walasekHowMakeLoss2015 worked on contain a subset from which one can directly investigate this question. Whereas most of the lotteries shown to participants were asymmetric -- that is, the potential loss differed numerically from the potential gain (such as for the example in Figure \@ref(fig:lottery-example)) -- a small subset of lotteries were symmetric, for these lotteries the amounts for the potential loss was equal to the amount of a potential gain. More specifically, the symmetric lotteries were -\$12/+\$12, -\$16/+\$16, and -\$20/+\$20.

For the condition described above in which losses ranged up to -\$20 and gains ranged up to +\$40, the 191 participants accepted the symmetric lotteries only 21% of the time. This result makes sense in light of loss aversion. When losing a certain amount of money is worse than winning the same amount of money, I should reject a symmetric lottery in which I am equally likely to lose or to win a certain amount of money.

The clever manipulation of @walasekHowMakeLoss2015 was that they included three further conditions in which they changed the range of possible outcomes. In addition to the -\$20/+\$40 condition discussed above, the 202 participants in the \$-20/+\$20 condition saw lotteries with losses ranging to -\$20 and gains also ranging to +\$20 only. Another group of 190 participants, the -\$40/+\$40 condition, saw lotteries with losses ranging to -\$40 and gains also ranging to +\$40. Finally, @walasekHowMakeLoss2015 also included a -\$40/+\$20 condition with 198 participants in which the losses ranged to -\$40, but the gains only to +\$20 (i.e., the complement to the -\$20/+\$40 condition). Importantly, in all conditions the number of possible outcomes for losses and gains was 8 (so the step size was either $\pm$\$2 or $\pm$\$4). The table below shows the possible outcome for each condition

| Condition           |       |       |       |       |       |       |       |       |
|---------------------|-------|-------|-------|-------|-------|-------|-------|-------|
| -\$20/+\$40: Gains  | \$12  | \$16  | \$20  | \$24  | \$28  | \$32  | \$36  | \$40  |
| -\$20/+\$40: Losses | -\$6  | -\$8  | -\$10 | -\$12 | -\$14 | -\$16 | -\$18 | -\$20 |
| -\$20/+\$20: Gains  | \$6   | \$8   | \$10  | \$12  | \$14  | \$16  | \$18  | \$20  |
| -\$20/+\$20: Losses | -\$6  | -\$8  | -\$10 | -\$12 | -\$14 | -\$16 | -\$18 | -\$20 |
| -\$40/+\$40: Gains  | \$12  | \$16  | \$20  | \$24  | \$28  | \$32  | \$36  | \$40  |
| -\$40/+\$40: Losses | -\$12 | -\$16 | -\$20 | -\$24 | -\$28 | -\$32 | -\$36 | -\$40 |
| -\$40/+\$20: Gains  | \$6   | \$8   | \$10  | \$12  | \$14  | \$16  | \$18  | \$20  |
| -\$40/+\$20: Losses | -\$12 | -\$16 | -\$20 | -\$24 | -\$28 | -\$32 | -\$36 | -\$40 |

: Possible outcomes for the lotteries in Experiment 1 of @walasekHowMakeLoss2015. In each condition, each participant saw 64 lotteries resulting from combining all possible gains with all possible losses in that condition.

As a consequence of this design, what changed across conditions was whether the symmetric lotteries were *relatively good* or *relatively bad*. To understand this, we need to look at the remaining asymmetric lotteries. In the -\$20/+\$40 condition discussed so far, there were more lotteries in which the possible gain was larger than the possible loss (e.g., a -\$18/+\$20 lottery) than lotteries for which the possible loss was larger than the gain (e.g., a -\$20/+\$18 lottery). Consequently, the symmetric lotteries were relatively bad (i.e., compared to the many lotteries in which the possible gain is larger than the possible loss). In the -\$20/+\$20 and -\$40/+\$40 conditions, the asymmetric lotteries were balanced. In half of the asymmetric lotteries the possible gain was larger than the possible loss, whereas for the other half the possible loss was larger than the possible gain. Consequently, the symmetric lotteries were neither relatively good nor relatively bad. Finally, in the -\$40/+\$20 condition the pattern was flipped with respect to the -\$20/+\$40 condition. There were only few lotteries in which the possible gain was larger than the possible loss compared to the many lotteries for which the possible loss was larger than the gain. Consequently, the symmetric lotteries were relatively good.

So does it matter whether the symmetric lotteries are relatively good or not? Indeed it does. As a reminder, people were unlikely to accept the symmetric lotteries in the -\$20/+\$40 condition in which the symmetric lotteries were relatively bad. Participants in this condition only accepted 21% of the symmetric lotteries. In the -\$20/+\$20 condition in which the symmetric lotteries were neither relatively good nor relatively bad, participants accepted 50% of the symmetric gambles. Similarly, in the -\$40/+\$40 condition participants accepted 42% of the symmetric gambles. Finally, in the -\$40/+\$20 condition in which the symmetric lotteries were relatively good, participants accepted 71% of the symmetric gambles.

What these results show is that the choice pattern for lotteries are not always in line with the idea of loss aversion. Only in a context in which a symmetric lottery is relatively bad do we see evidence in line with the idea of loss aversion. If we are in a context in which a symmetric lottery is relatively good, we see the opposite pattern that one could term *loss seeking*. What does this mean for loss aversion? The original idea of @kahnemanProspectTheoryAnalysis1979 that what matters is the magnitude of a loss or gain surely is not in line with the results of @walasekHowMakeLoss2015. Instead, @walasekHowMakeLoss2015 argue that what is relevant to determine the psychological impact of a gain or loss is the relative magnitude or *rank* of a possible outcome: Compared to other gains or losses I regularly experience, is this a large gain or large loss?

Before moving on and linking this example of the psychology of loss aversion to the general goal of this book, let us answer one last question. If what matters is the rank (as suggested by @walasekHowMakeLoss2015) and not the magnitude of an outcome (as proposed by @kahnemanProspectTheoryAnalysis1979), why do we see evidence for an effect of magnitude in the study of @kahnemanProspectTheoryAnalysis1979 that did not manipulate the context of gains and losses? An answer to this question is provided by @stewartDecisionSampling2006. They argue (and also provide empirical evidence) that in our daily lives we experience more small losses (e.g., buying something at the bakery) and more larger gains (e.g., the monthly salary). As a consequence, for a gain and loss with the same magnitude the relative position of the gain compared to all other gains is lower than the corresponding relative position of the loss compared to all other losses. From this difference, we should generally observe a pattern consistent with loss aversion but for a different theoretical reason than proposed by @kahnemanProspectTheoryAnalysis1979.

## Epistemic Gaps: The Difference Between What we Want to Know and What we can Know

The goal of this chapter is to provide an introduction to the role of statistics in the research process. Why do we need statistics and what can it tell us about the research questions we are interested in? However, so far we have not talked much about statistics but mostly introduced examples of the research process. The reason for this is that it is important to understand what we generally want to know when we do empirical work in psychology or related disciplines. We need statistics because we have a research question for which we want an answer.

If you do not yet know about statistics in detail, the application of statistical methods can appear like a magical machinery that provides us with an answer to the research question we have. You throw your data in, turn the statistics machinery on, and get an answer to your research question out. Sadly, this image of statistics is false. The reason is that there are at least two *epistemic gaps* in the research process that prevent us from getting a straight answer to our research question. In this chapter we will introduce these gaps to ensure you can get a realistic image of the role of statistics in the research process.

At this point, you might wonder what an *epistemic gap* is. *Epistemology* is a branch of analytical philosophy that is concerned with knowledge (e.g., what is knowledge, how de we know that we know) and "epistemic" is the corresponding adjective. Consequently, an epistemic gap describes the difference between what we want to know and what we can actually know.[^role_of_stats-3] What we can know from scientific research is a question addressed in philosophy of science. Based on the examples above we will take a look at some arguments from philosophy of science that show that unfortunately, what we can know is often quite different from what we would like to know. A competent application of statistics requires that one is aware of this problem and avoids over-interpreting the results from ones research.

[^role_of_stats-3]: In some parts of philosophy, the term "epistemic gap" has a different meaning relating to questions of consciusness and feelings.

### Epistemic Gap 1: Underdetermination of Theory by Data

Above we have provided two examples of the research process in psychology, one applied research question (Is a new therapeutical intervention better?) and one theoretical (or *basic*) research question (Is there evidence for loss aversion?). In both cases we have seen that answering the research question requires carefully thinking about the operationalisation. How exactly should we set up a study to test this question? And once we have decided on one operationalisation, we have seen that we can find alternative explanations that can explain the results without making the assumptions of the original research question. In other words, even though the operationalisation was carefully chosen, it could not unambiguously answer our research question.

The fact that we could not compellingly answer the research questions despite employing a carefully chosen operationalisation is not a problem that is unique to our two examples. In contrast, an important insight from philosophy of science is that this is a problem of *any* empirical study. This issue is also known as *underdetermination of theory by data* or the [Duhem-Quine thesis](https://en.wikipedia.org/wiki/Duhem%E2%80%93Quine_thesis) and always occurs if there is a difference between the research question and the corresponding operationalisation. And as we have seen in the examples, there essentially always is.

There are different aspects or different angles with which we can look at and understand underdetermination. The first aspect has to do with the specification of the research question and its operationalisation. Research questions usually involve unobservable *constructs* -- such as emotions (e.g., fear), memory, attention, comprehension, or learning -- or vague phrases, such as "works better" or "improves". An empirical investigation of such questions however requires a precise specification and operationalisation. By going from the research question to the concrete operationalisation, there is no guarantee that the operationalisation captures the intended meaning of the research question.

For example, consider again our first example of the test of a therapeutical intervention. Imagine we found that the new intervention decreased patient scores on a symptom questionnaire more strongly than the old treatment, but did not reduce the number of sick days due to the disorder. Should we interpret this in the way that the new treatment "works better" than the old treatment? In some sense it does, but in another not. The problem is that the nuances that result from operationalising a research question concretely do not always align with the broad way in which we like to think about research questions.

At this point you might think this does not yet sound like such a big problem. We just need to define our research questions precisely enough and then we are able to learn something about our research question. Sadly, this is easier said then done. The first problem is that it is often impossible to precisely define our research question, because we have not yet found a way to precisely define the constructs that are involved in it (this is known as the *problem of coordination*, [@kellenProblemCoordinationPursuit2021]).

For example, if you have they hypothesis that a specific emption, say fear, is related to some behavioral pattern, say aggression, you run into the problem that there is not generally agreed upon definition of either of these constructs. There probably exist questionnaires for measuring fearfulness and aggressive tendencies, but these questionnaires do not represent the corresponding constructs or a definition of them. If you were to ask a sample of participants to fill out these questionnaires and found that the scores of the participants in these two questionnares are related, it would not allow you to conclude that fearfulness and aggression are related. The only conclusion that would be allowed is that fearfulness as measured with the questionnaire is related to aggression as measured with the questionnaire. Of course, as scientists we would like to make the general conclusion that the constructs are related, but such an inference is not logically allowed.

The general problem that we run into is the Duhem-Quine thesis; any empirical hypothesis that is tested in a study contains of two parts: The theoretical prediction as well as a set of *auxiliary assumptions* that link the theoretical prediction with the data. To stay with our example, our theoretical prediction could be that fear and aggression are related. The auxiliary assumption are all additional assumptions that are needed to test this question empirically as decided on as part of the operationalisation: that the questionnaire is a valid measure of the constructs (which is a big assumption), that the data collection took place without any unforeseen problems, that we have tested enough participants to find an effect, that we use appropriate statistical procedures, etc. As can be seen, the list of auxiliary assumption is somewhat limitless and difficult to enumerate fully. It also contains quite mundane assumptions such that we have to assume that the research actually took place and is not just made up by the researcher (for an exception, see [the case of Diederik Stapel](https://scienceintegritydigest.com/2019/07/16/fabrication-the-diederik-stapel-case/)).

The core of the Duhem-Quine thesis is that any empirical result does not pertain solely to the theoretical prediction of interest, but the union (or conjunction) of the theoretical prediction of interest with the auxiliary assumptions. If the results are in line with the empirical hypothesis, that only supports the theoretical prediction if all auxiliary assumptions are true. Likewise, if the results are not in line with the empirical hypothesis, this only provides evidence against the theoretical prediction if all auxiliary assumptions are true. However, testing whether all auxiliary assumptions are true cannot be done in the same study that tests the empirical hypothesis we set out to test (because we can always come up with more and more auxiliary assumptions not specifically tested). Consequently, any individual result on its own cannot provide conclusive evidence for or against a particular theoretical prediction, there can always be an alternative explanation that differs from the theory or hypothesis one has.[^role_of_stats-4] That is what is meant by the underdetermination of theory by data.

[^role_of_stats-4]: A further consequence from the Duhem-Quine thesis is that there can be no such thing as a theory-free observation. Any empirical result that is interpreted in light of some hypothesis or research question comes with more or less explicit auxiliary assumptions.

Whereas this issue might seem like a purely philosophical discussion, it is far from it. Most actual scientific discussions in the literature are about the auxiliary assumptions that are part of the operationalisation of a research question. For example, in the example of the therapeutical intervention, the idea that therapists apply both old and new therapy in exactly the same manner is an auxiliary assumption that is questioned by the concept of therapeutical allegiance. Likewise, the argument for loss aversion as proposed by @kahnemanProspectTheoryAnalysis1979 hinges on the auxiliary assumption that participants interpret the possible outcomes of the lotteries in terms of their magnitude or absolute value. As shown by @walasekHowMakeLoss2015, at least in some cases this auxiliary assumption does not appear to hold and participants instead interpret the relative value of the possible outcomes of the lotteries. It will be easy to find similar examples for the research area you are interested in.

To sum this up, the problem of underdetermination and the first epistemic gap is that any particular results never uniquely supports or challenges one theoretical position or hypothesis. For any result that appears to support a theory there is another theory that makes the same prediction because an auxiliary hypothesis could be false and thus require a different theory. Likewise, for any results that seems to disagree with a theory, the theory can always be protected by claiming one of the auxiliary assumptions is incorrect. And this is also exactly what happens in real scientific discourse. As an example, when John Bargh, a prominent social psychologist from Yale, was confronted with results that disagreed with one of his most prominent findings [@doyenBehavioralPrimingIt2012] he attacked (in a now deleted blog post that still can be found [here](https://replicationindex.com/wp-content/uploads/2020/07/bargh-nothingintheirheads.pdf)) the "incompetent or ill-informed researchers" and claimed their study "had many important differences from our procedure, all of which worked to eliminate the effect​". As this section has section has shown, questioning the methods (i.e., the auxiliary assumptions) is a legitimate defence that protects ones theory. Of course, one can question the auxiliary assumptions of the original results that appeared to support the theory in the same way. In the case of Bargh, it appears that this is exactly what happened. Most other psychologists have stopped believing his original finding [e.g., @harrisTrainWreckAny2021].

### Epistemic Gap 2: Signal and Noise

The first epistemic gap is that there is no strong logical link between the theories underlying our research questions and the operationalisation of the research question. Thus, in terms of the steps in our research process it concerns the relationship between step 1, the research question, and step 2, the operationalisation and data collection. The next epistemic gap concerns the relationship of steps 2 and step 3, the statistical analysis.

As described above (\@ref(the-research-process)), the important task during the operationalisation is to transform the research questions into an empirical hypothesis. That is, figuring out and describing which possible outcome would support our theoretical hypothesis (i.e., the outcome predicted by our theory). As part of this we should also clearly designate a possible outcome that, if it were to occur, would speak against our theoretical hypothesis. Once we have decided on this, we collect the data and then run the statistical analysis. The goal is that the statistical analysis provides us with evidence with respect to the empirical hypothesis. Does the data support the empirical hypothesis or does it not?

Before providing an overview of how this is done, let us go back to the second example above, the study of @walasekHowMakeLoss2015. In contrast to the original formulation of loss aversion [@kahnemanProspectTheoryAnalysis1979], which is based on the magnitude or absolute value of a gain or loss, the theoretical prediction of @walasekHowMakeLoss2015 was that what drives people's behaviour is the relative value of a gain or loss. To test this, they presented participants with lotteries in different conditions in which the range of gains and losses differed. In one condition there were small losses and large gains and in another condition there were large losses and small gains (we ignore the other two conditions here). The hypothesis that follows from this design is that the relative attractiveness of the symmetric lotteries (in which magnitude of possible loss = magnitude of possible gain) differs in both conditions. In the small loss/large gains condition the symmetric lottery is relative unattractive and in the large loss/small gains condition it is relatively attractive. The resulting empirical prediction is that participants should be less willing to accept the symmetric lotteries in the small loss/large gains condition than in the large loss/small gains condition. In line with this prediction, participants in the small loss/large gains condition accepted only 21% of the symmetric lotteries whereas participants in the large loss/small gains condition accepted 72% of the symmetric lotteries.

From just looking at the bare numbers, the results appear to support the empirical prediction. Participants are roughly 50% less likely to accept the symmetric lotteries in the small loss/large gains condition than in the large loss/small gains condition. However, how can we be sure this particularly observed difference is not a chance occurrence. Maybe we just got unlucky and the participants in the small loss/large gains condition are for some reasons generally less likely to accept any lotteries than the participants in the large loss/small gains condition. Maybe the former participants all had a horrible night of sleep and are in a really bad mood at the time of testing and therefore reject all gambles whereas this was not the case for the latter. If this were the case, the observed difference would not actually tell us anything about our research questions.

The problem described in the previous paragraph is at the heart of the statistical approach described in this book. The core problem is that the responses we get from participants in experiments are inherently noisy. Human participants can do things for an unlimited number of reasons. Some of these reasons are related to our research question and its operationalisation and other reasons are not. For example, if participants read the lotteries carefully and think before they provide an answer, it is likely that the values of the possible outcome play a role in their answer. In this case, their responses are relevant for our research question. But what if participants are distracted by a message on their phone and do not read the problem fully? Or what if they intend to accept a lottery and accidentally reject it (i.e., press the wrong button)? We can also imagine that it matters if participants are relatively rich or relatively poor. For someone with a million on the bank, it might not really matter if they lose or win \$16 so they might be inherently more likely to gamble on such a lottery than someone for which this is more than the hourly wage. In all these cases, the values of the lotteries have a minor effect and thus the responses are more or less irrelevant for our research question.

For the question of whether or not the results support our empirical prediction we therefore would like to distinguish between those responses that are relevant for our research question -- we can call this the *signal* -- and those responses that are generated more or less randomly and are irrelevant for our research question -- we can call this the *noise*. If we had a procedure that could distinguish signal from noise we could then simply see whether the signal supports our empirical prediction. If it did, the data would provide support for the hypothesis and if not the data would not support the empirical predictions. Sadly, such a procedure does not and cannot exist (as we would then know why people do what they do, which is the reason we do research in the first place).

In the absence of a procedure that can definitely separate the contribution of signal and noise, the statistical approach introduced here compares an *estimate of the signal* with an *estimate of the noise*. Let us assume for a moment the estimated signal supports our empirical prediction as in our example (i.e., we predict that participants are less likely to accept a lottery in the small loss/large gains condition and this is what the data shows). We then compare this estimated signal against the estimated noise. If the estimated signal is large given the estimated level of noise, we assume that the data supports the empirical prediction. If the estimated signal is not large given the estimated noise, we assume the data does not support our empirical prediction.

So how can we estimate the signal and the noise? Estimating the signal is straight forward. We just use the observed difference between the conditions as our estimate of the signal. So for the example from @walasekHowMakeLoss2015 this would be the observed difference in accepting the symmetric lotteries between the two conditions which was roughly 50%. Estimating the noise is a bit more complicated and will be described in detail in later chapter. For now it is enough to understand that it is affected by two components: (1) The variability in responses within each condition and (2) the overall sample size (i.e., number of participants). If the variability within each condition becomes smaller (i.e., measurement becomes more precise) and the sample size stays the same, the levels of noise decreases. Likewise, if the sample size increases with a constant level of variability, the level of noise decreases.

Another important question is what counts as "large" when comparing the estimated signal to the estimated noise. In the following chapters we will introduce a decision threshold to make this judgement.[^role_of_stats-5] If the signal to noise ratio is above the threshold, we act as if there were a signal and change our believes. If it is not, we cannot make a decision. This decision threshold is chosen such that across many such decisions we control our rate of making false positive decisions. In particular, the decision threshold is chosen such that if we were in a situation in which there was no signal, we only incorrectly assumed that there is a signal in 5% of decisions.

[^role_of_stats-5]: To foreshadow the content of the book, this decision threshold is based on the $p$-value. A sometimes controversial but ubiquitous statistical procedures that you will find in most empirical publications.

Taken together, the statistical procedures we are using attempt to answer the question whether there is a signal that supports the empirical hypothesis given that human data is inherently random and noisy. The problem in this is that the estimated signal -- the observed difference between the conditions -- is also affected by the noise. We never know if the observed difference is due to the signal we are interested in or just based on noise. To overcome this problem we compare the observed signal with the observed level of noise. If the observed level is large relative to the observed noise, we decide the data supports the presence of the signal. In other words, we never really know if the current data *really* supports our prediction or not, we just act as if it does. There always is a non-zero chance that the effect is due to the noise. Because we only have this one data set we are analysing, we cannot be 100% certain our estimate of the signal and the noise is fully accurate. However, as we will describe in detail later, across decisions that use this statistical decision procedure, it controls our rate of making false positive decisions (i.e., assume there is a signal when there is none).

So as in the case for the first epistemic gap, the second epistemic gap also shows that what we cannot really learn what we wanted to know, if the data supports the empirical prediction or not. If the signal is large relative to the noise, we have evidence that it does, but this evidence is never fully conclusive. The evidence might be strong, and we will later see how we can identify that, but there always should be some remaining doubt in the back of our head. Maybe we just got unlucky and the participants in our study responded in a way that made it look like there is a signal, but there isn't. With just one data set in hand, this cannot really be ruled out

## Summary

In this chapter we have provided a conceptual overview of the research process in psychology and related disciplines. In this concept, the research always begins with a research question. What is it that I want to know? Often this research questions stems from a particular theory that we want to test, but it can also be a purely applied hypothesis.

The next important step is the operationalisation of this question followed by the data collection. That means we need to find appropriate tasks or measures and develop a study design with which we can test the empirical hypothesis following from our research question. As we have seen in the discussion of the first epistemic gap, the consequence of the separation of research question and operationalisation is that, strictly speaking, our study only let's us learn more about the tasks and measures we are using. Because of the problem of underdetermination of theory by data, even an apparently positive result does not allow us to infer that it supports our theoretical hypothesis. The core problem is that the empirical hypothesis is a combination of theoretical hypothesis and auxiliary assumptions and we cannot rule out that one of the auxiliary assumptions is false.

With the collected data in hand, the next step is to perform the statistical analysis. Here, we hope to find evidence that informs us about our empirical hypothesis. The procedure we will use in this book attempts to distinguish between the signal in the data, the part of the data relevant to our empirical hypothesis, and the noise, the randomness that is inherent in using human participants.[^role_of_stats-6] However, the second epistemic gap entails that even with such a statistical procedure, we cannot find fully conclusive evidence. The problem is that we cannot estimate the true amount of noise in the data. Research participants have a myriad of potential reasons of why they show a certain behaviour and these reasons need not be related to our research question. With more precise measures and more participants, we can control this level of noise to some degree, but ultimately cannot be sure whether we did not just get unlucky and what we see is due to noise and not because of our hypothesis.

[^role_of_stats-6]: Even though we say human participants here, this also applies to non-human participants (i.e., animals) or other biological organisms.

The final step in the research process is the communication of our results. This step essentially combines all previous step. We need to communicate the research question, the operationalisation, the data collection process and sample, and the results from the statistical analysis. Whereas the communication of the results is ultimately the goal of any research project, it is also the step during which we have to be mindful of the limits of our research. The biggest danger is that we forget the epistemic gaps that are inherent in any empirical research and oversell our results. We of course want that our research allows us to answer our (potentially big and broad) research questions, but we should be honest with ourselves and our audience and stick to the reality that we are primarily learning something about our operationalisation. When we get a statistical result that appears to support our empirical prediction we want treat it as if it is true, but we should be clear that there always is a chance that our result might be a fluke.

### What Shouldn't and What Should we do?

Let us end this chapter with some concrete examples of what we shouldn't and what we should do. To motivate ones research question, it is a good idea to start with the big picture. What are the real world issues and theoretical problems we want to address? Whereas this is a good idea, we should not mistake our operationalisation of the research question with this big picture.

For example, in a recent paper on which I was a co-author [@baumannLinearThresholdModel2020], we developed a new task, the "ticket shopping task", to investigate sequential decision making problems (in this section, the details of the tasks are not really relevant so we will only mention the names, but do not describe tasks in detail). Whereas I feel we could make a good case that we could figure out to some degree what people do in this task, this does not mean that this is what people generally do in sequential decision making problem.

Likewise, whereas there might be both a model-free and model-based reinforcement learning system in the brain, the popular "two-step task" [@dawModelBasedInfluencesHumans2011] is unlikely to reveal all its mechanism or even prove its existence.[^role_of_stats-7] In fact, there is good evidence participants in this task show many behaviours that are unrelated to the theoreical question of interest [e.g., @akamSimplePlansSophisticated2015].

[^role_of_stats-7]: I am not suggesting the original authors [@dawModelBasedInfluencesHumans2011] necessarily made these claims. This is just an example of where a particular task is portrayed in the literature as representing the underlying theoretical constructs (i.e., model-based and model-free reinforcement learning). As we have discussed, this is not a logically justifiable position. A particular task cannot represent a construct unless all auxiliary conditions are carefully spelled out and justified.

Another example comes from the domain of moral reasoning. Here, a prominent procedure is the use of the "trolley problem" and similar vignettes [@greeneFMRIInvestigationEmotional2001]. Whereas these vignettes have revealed interesting results, it seems questionable to assume that participants' responses represent the extent of their moral reasoning or that they are particularly predictive of participants real-world behaviour.[^role_of_stats-8] Similar arguments can be made for many other experimental domains but also whenever particular questionnaires are used (e.g., as discussed in the examples above).

[^role_of_stats-8]: Again, I do not want to accuse the authors cited here [@greeneFMRIInvestigationEmotional2001] of making such claims. However, the literature again seems to treat "moral reasoning" and "responses to trolley-type problems" as near synonyms which seems problematic given the.

It makes sense to look a bit critical at the argument we have made here. Surely, if we use a task such as the "ticket shopping task" or the "two-step task" we learn something about the underlying research question and theory? We surely do, the question that is difficult to answer is exactly what we learn.

As an empirical example to illustrate this problem, let us consider the research on risk preferences in decision making. The idea of risk preferences is that some people might be more willing to take risks (e.g., when gambling or when choosing an investment) than others. There exist a number of different tasks to investigate risk preferences experimentally, such as the balloon analogue risk task [BART; @lejuezEvaluationBehavioralMeasure2002] or the Columbia card task [@fignerAffectiveDeliberativeProcesses2009], as well as a number of different questionnaires. A large study with around 1500 participants who each performed eight different tasks and filled out twelve different questionnaires designed to measure risk preferences [@pedroniRiskElicitationPuzzle2017; @freyRiskPreferenceShares2017] could show that participants' behaviour across tasks and questionnaires was surprisingly unrelated. Whereas participants who scored high on one questionnaire also scored high on other questionnaires (i.e., the different questionnaires shared a common risk trait), the scores on the questionnaires were largely unrelated to the behaviours in the different tasks. Furthermore, behaviours across the different risk tasks were unrelated to each other (i.e., a participant who was specifically risky in one tasks was not particularly risky in another task). In other words, even though the tasks and questionnaires all appear to measure risk preferences their failure to find a consistent pattern across participants suggests they fail to do so in a coherent manner. One might wonder if the fact that the questionnaires were related among each other represents some sort of silver lining here. I would not share this interpretation and instead attribute this to [common-method variance](https://en.wikipedia.org/wiki/Common-method_variance). The important result is that the questionnaires were also unrelated to the behaviour in the tasks. This tells me that at this point we do not really understand what risk preferences are or how to measure them.

To sum this up, it is important to keep in mind that the thing we learn something about in our research is primarily our operationalisation and measurement. If we want to make a case that we also learn something about the underlying research question, we have to make a good case for this and spell out which alternative explanations we rule out and which auxiliary assumption we can take for granted. This usually requires considering other results than our own study. In short, do not confuse the task or measurement with the theory or research question.

When communicating the statistical results we also need to avoid overselling the results. As a general principle, we should report the results in a humble manner. To this end, we should avoid language that suggests a level of confidence that we cannot provide. This means, statistical results never "prove" or "confirm" our empirical prediction. Instead, they may "support" it or "suggest" certain interpretations.

As we have discussed, a statistical analysis never gives us perfect confidence that a particular result supports an empirical prediction. The only way to get some confidence that we did not get unlucky with our sample of participants is that the study is repeated by other -- a so called *replication*. If a completely independent set of researchers finds the same pattern of results, this provides us with rather strong evidence that the study design indeed gives rise to this data pattern. As the final part of this chapter, let us review the pattern of replication results for our two main examples. The results of @kahnemanProspectTheoryAnalysis1979, using their original lotteries, were recently successfully replicated in a large scale study involving 19 countries and more than 4000 participants [@ruggeriReplicatingPatternsProspect2020].[^role_of_stats-9] For the @walasekHowMakeLoss2015 study, the pattern is not yet as clear. Whereas I am not aware of any published exact (or direct) replication of their study, the original authors have attempted to extend their findings somewhat [@walasekContextdependentSensitivityLosses2019] and found similar but less pronounced differences between condition. However, using a different study design @schneiderEffectsSurroundingPositive2016 also showed that the acceptance rated of symmetric lotteries are strongly affected by which other lotteries are presented.

[^role_of_stats-9]: What this replication means for the underlying theory, prospect theory, is a different question. David Kellen has written about this in a blog post: <https://socialsciences.nature.com/posts/the-limited-value-of-replicating-classic-patterns-of-prospect-theory>
