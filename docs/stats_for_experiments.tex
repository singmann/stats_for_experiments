% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Introduction to Statistics for Experimental Psychology with R},
  pdfauthor={Henrik Singmann},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Introduction to Statistics for Experimental Psychology with \texttt{R}}
\author{Henrik Singmann}
\date{2021-05-15}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{overview}{%
\chapter{Overview}\label{overview}}

At this point in time, the book is being written so only few chapters are already available, beginning with chapter \ref{standard1}.

\hypertarget{standard1}{%
\chapter{The Standard Approach for One Independent Variable}\label{standard1}}

In this chapter we are introducing the standard statistical approach for analysing experimental data with one independent variable (i.e., one factor). The simple case for this is a study comparing two experimental conditions on one dependent variable. We will exemplify the standard approach for this design using a recent and straightforward experiment.

\hypertarget{ex:urry}{%
\section{Example Data: Note Taking Experiment}\label{ex:urry}}

Heather Urry and 87 of her undergraduate and graduate students \citep{urry2021} (yes, all 87 students are co-authors!) compared the effectiveness of taking notes on a laptop versus longhand (i.e., pen and paper) for learning from lectures. 142 participants (which differed from the 88 authors) first viewed one of several 15 minutes lectures (TED talks) during which they were asked to take notes either on a laptop or with pen and paper. As this was a proper experiment, participants were randomly assigned to either the laptop (\(N = 68\)) or longhand condition (\(N = 74\)). After a 30 minutes delay, participants were quizzed on the content of the lecture. The answers from each participant were then independently rated from several raters (which agreed very strongly with each other) using a standardised scoring key resulting in one memory performance score per participant ranging from 0 (= no memory) to 100 (= perfect memory). Figure \ref{fig:laptop-dist} below shows the performance scores across both note taking conditions.

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{stats_for_experiments_files/figure-latex/laptop-dist-1} 

}

\caption{Distribution of performance scores from Urry et al. (2021) across the two note taking conditions.}\label{fig:laptop-dist}
\end{figure}

In Figure \ref{fig:laptop-dist}, each black point shows the score of one participant so the full distribution of the data is visible. The shape of the distribution is also shown via a violin plot (i.e., the black outline around the points) to which we have added three lines representing three summary statistics of the data. From top to bottom these lines are the 75\% quantile, the 50\% quantile (i.e., the median), and the 25\% quantile. The red points show the mean and the associated error bars show the standard error of the mean\footnote{We will discuss standard errors in detail in the coming chapters. Until then, it is enough to understand the standard error as a representation of the statistical precision of the mean. In other words, the larger the standard error, the less sure we are about the ``true'' value of the mean (again, what we mean with true will be discussed later).}. We will discuss this data in more detail in the next chapter. For now it is enough to see that the two means are quite similar, although the mean in the laptop condition is slightly larger, by 2.0 points (mean laptop = 68.2, mean longhand = 66.2).

\hypertarget{the-basic-statistical-model}{%
\section{The Basic Statistical Model}\label{the-basic-statistical-model}}

The previous paragraph provide us with \emph{descriptive statistics} describing the results in the experiment by \citet{urry2021}: There is a memory performance difference of 2.0 on the scale from 0 to 100 between the laptop and the longhand condition for the sample of 142 participants. However, as researchers we are usually not primarily interested what happens in our sample. What we would like to know if our results generalises to the \emph{population} from which this sample is drawn. In this case, we would like to know whether there is a memory difference between note taking with a laptop or in longhand format for students (as this is the population the sample is drawn from).

\hypertarget{the-logic-of-inferential-statistics}{%
\subsection{The Logic of Inferential Statistics}\label{the-logic-of-inferential-statistics}}

Going beyond the the present sample is the goal of \emph{inferential statistics}. There are different inferential statistical approaches, and we are focussing on the most popular one, \emph{null hypothesis significance testing} (NHST). We will describe NHST more thoroughly in the next chapter, but will already introduce its main ideas here. For NHST, the question of generalising from sample to population is about two different possible true states of the world: There either is no difference in conditions means in the population (i.e., there only is a mean difference in our sample due to chance) or there is a difference in the condition means in the population. To decide between these possibilities, we set up a statistical model for the data. This statistical model allows us to assess if there is no difference in the population -- we call this possible state of the world the \emph{null hypothesis}. More specifically, the statistical model allows us to test how compatible the data is with the null hypothesis of no difference. The test of the null hypothesis proceeds as follows: (1) We assume that the state of the world in which there is no difference in the population means is true. (2) Based on this assumption we calculate how likely it is to observe a difference as large as the one we have observed in our sample. (3) If the probability of observing a difference as large as the one we have is very small, we take this as evidence that the null hypothesis of no difference is not true -- we reject the null hypothesis. (4) We act as if there were a difference in the population. In this case (i.e., we reject the null hypothesis and act as if there is a difference), we say our experimental manipulation \emph{has an effect}.

As is clear from this description, the logic underlying inferential statistics using NHST is not trivial. To make it clearer, let us apply the logic to our example data. We want to know whether the observed mean memory difference between note taking with a laptop and note taking in long hand format in our sample generalises to the population of students that take note in either of these formats. To do so, we set up a statistical model for our data. This model allows us to test how compatible our data is with the null hypothesis that there is no mean memory difference in the population. Specifically, the model allows us to calculate the probability of obtaining a memory difference as large as the one we have observed assuming the there is no mean memory difference in the population. If our data is incompatible with the null hypothesis (i.e., it is unlikely to obtain a memory difference as large as the one we have observed if the null hypothesis were true), we reject the null hypothesis of no mean memory difference in the population. Because we reject the null hypothesis we then act as if there was a mean memory difference in the population. In other words, we then act as if the type of note taking had an effect on memory performance in the population.

Even though the logic of NHST is not necessarily intuitive, it is clearly helpful for researchers. After running an experiment we really would like to know if the difference observed in our experiment (i.e., in our sample) is meaningful in the sense that it generalises to the population from which we have sampled our participants. And in almost every actual experiment there is some mean difference between the condition (i.e., it is extreme unlikely that both conditions have exactly the same mean). Thus, we pretty much always face this question. NHST allows us to test whether the observed difference is compatible with a world in which there is no difference. If this is unlikely, we decide (i.e., act as if) there were a difference.

NHST is the de facto standard procedure for inferential statistics in the cognitive and behavioural sciences. Understanding the logic of NHST will enable you to understand the majority of empirical papers and will also allow you to apply inferential statistics in your own research. Nevertheless, there exist a long list of popular criticisms of NHST \citep[e.g.,][]{rozeboom1960, meehl1978, cohen1994, nickerson2000, wagenmakers2007}. We will discuss these criticisms in more detail in the next chapter, but for now it is important to realise that NHST does not allow to test, or prove, whether there is a mean difference in the population. The only thing NHST calculates is a probability of how compatible the data is with the null hypothesis. If this probability is low that does not necessarily mean that there is a difference. Likewise, if this probability is high that does not necessarily mean there is no difference. All inferences we draw based on NHST results are necessarily probabilistic in itself (i.e., can be false). So the most important rule when interpreting the results from NHST is to \href{https://youtu.be/tvTRZJ-4EyI}{be humble}. NHST never ``proves'' or ``confirms'' anything. Instead NHST results ``suggest'' or ``indicate'' certain interpretations. If we do not over-interpret results, but stay instead stay humble in our interpretations, we are unlikely to fall prey to the common (and often justifiable) criticisms of the NHST framework.

\hypertarget{the-statistical-model}{%
\subsection{The Statistical Model}\label{the-statistical-model}}

To apply inferential statistics in the NHST framework to our data, we begin by setting up a \emph{statistical model} to the data. A statistical model attempts to explain (or predict) the observed values of the dependent variable (DV) from the independent variable (IV).\footnote{All statistical models considered in this book are solely based on observable quantities. We predict the DV (i.e., what we measure) from the IVs we either manipulate (in experiments) or observe/measure (for non-experimental DVs). However, statistical models can also use non-observable (latent) quantities to explain the DV. Popular examples in psychology are structural equation models \citep[e.g.,][]{kline2015} or cognitive models \citep[e.g.,][]{lee2013}.} In the experimental context this means predicting our observed outcome, the DV, from the experimental manipulation, the IV.

The basic statistical model partitions the observed DV into three parts that: the overall mean, which for reasons that will become clear later is called the \emph{intercept}, the effect of the IV, and the part of the data that cannot be explained by the model, the \emph{residuals}. When summing these three parts together, they result in the observed value. In mathematical form we can express this as

\begin{equation}
\text{DV} = \underbrace{\text{intercept}}_{\text{overall mean}} + \text{IV-effect} + \text{residual}.
\label{eq:statmodel}
\end{equation}

(For those not used to reading mathematical expressions, the point at the end of the equation is simply a full stop that ends the sentence and has no mathematical meaning.) As someone without a mathematics background myself, I know that equations in a text are often more intimidating than immediately useful. Consequently, before moving on it makes sense to go through this equation in more detail. Furthermore, all statistical analyses discussed in this book are applications of Equation \eqref{eq:statmodel}. This equation forms the foundation for the statistical analysis of experimental data and thus understanding it will unlock all analyses discussed in this book. Consequently, it makes sense to spend more time on it.

Let us consider the the variables in Equation \eqref{eq:statmodel} in more detail. When doing so, we also consider how many different possible values each variable can take on.\footnote{Equation \eqref{eq:statmodel} is displayed in simplified and not fully correct form. Mathematically correct would be that either each variable (i.e., \(\text{DV}\), \(\text{intercept}\), \(\text{IV-effect}\), and \(\text{residual}\)) has an index, such as \(i\), that goes from 1 to \(N\) (where \(N\) is the total number of observations) or that each variables is a vector (i.e., holds multiple values) of length \(N\).}

\begin{itemize}
\item
  \(\text{DV}\): The dependent variable, DV, are the observed values, one for each observation/participant. For the example data this are all the 142 black data points shown in Figure \ref{fig:laptop-dist}. Thus, our statistical model tries to explain the individually observed values.
\item
  \(\text{intercept}\): The intercept represents the overall mean. Consequently, we only have one intercept (i.e., the intercept is the same for each observation). In experimental designs we define this as the mean of all condition means. For the example data the intercept is (68.2 + 66.2) / 2 = 67.2.
\item
  \(\text{IV-effect}\): The IV-effect represents the effect of our independent variable which we define as the difference between the condition means and the intercept (i.e., the deviation of the condition means from the intercept). Thus, we always have as many different IV-effects as we have conditions. For the example data with only two conditions, we only have two different IV-effects, both of which with the same magnitude and only differ in sign, 1.0 for the laptop condition and -1.0 for the longhand condition. If we add these values to the intercept, we get the condition means. As we will discuss further below, this is the most relevant part for answering the statistical question of interest.
\item
  \(\text{residual}\): The residuals are the idiosyncratic aspects of the data that are left unexplained by the statistical model. As the model only predicts the condition means (i.e., intercepts plus independent variable), these are the deviations of the individual observations from the condition means. Thus, as for the DV, we have as many residuals as we have values of the DV.
\end{itemize}

\hypertarget{model-predictions}{%
\subsection{Model Predictions}\label{model-predictions}}

A simplification of Equation \eqref{eq:statmodel} that makes it clearer what the statistical model predicts is obtained if we ignore the residuals for a moment. As a reminder, the residuals are the part of the data that remains unexplained. In other words, these represent all the idiosyncratic parts of the data independent of our manipulation (e.g., some participants have better memory than others independent of how they took notes). What remains from our statistical model if we ignore all idiosyncratic aspects are only the predictions based on our IV. In the case of experimental data, the IV is the experimental condition. Thus, what a statistical model actually predicts is the means of the experimental conditions. We can again formalise this as

\begin{equation}
\hat{\text{DV}} = \text{intercept} + \text{IV-effect}.
\label{eq:predmodel}
\end{equation}

Here, the hat symbol (\(\hat{}\)) means predicted value. Thus in contrast to the actual DV above, we only have the predicted DV in this equation.

When performing statistical analyses it sometimes help to remind oneself that all a standard statistical model predicts are the condition means. We generally do not make predictions about individual participants or consider other factors that are not part of the model. We only predict, and are interested in, the condition means.

\hypertarget{statistical-model-for-the-example-data}{%
\subsection{Statistical Model for the Example Data}\label{statistical-model-for-the-example-data}}

Let us take a look at the first six participants and their values for all the variables in the basic statistical model to get a better understanding of Equation \eqref{eq:statmodel}.

\begin{tabular}{l|l|r|l|r|r|r|r}
\hline
pid & condition & performance & ----- & intercept & iv\_effect & prediction & residual\\
\hline
1 & laptop & 65.8 &  & 67.2 & 1 & 68.2 & -2.4\\
\hline
2 & longhand & 75.8 &  & 67.2 & -1 & 66.2 & 9.6\\
\hline
4 & longhand & 50.0 &  & 67.2 & -1 & 66.2 & -16.2\\
\hline
5 & laptop & 89.0 &  & 67.2 & 1 & 68.2 & 20.8\\
\hline
6 & longhand & 75.6 &  & 67.2 & -1 & 66.2 & 9.4\\
\hline
8 & longhand & 83.3 &  & 67.2 & -1 & 66.2 & 17.1\\
\hline
\end{tabular}

The first three columns show the data. \texttt{pid} is the participant identifier (id) column. As it is often the case for real data, some ids are missing (here 3 and 7) for various reasons (e.g., potential participants were interested in the study and received an id, but then did not finish or start the experiment) so the first 6 rows already go up to \texttt{pid} = 8. \texttt{condition} tells us in which note taking condition a participant was and \texttt{performance} is their performance score on the scale from 0 to 100 which serves as the DV in the statistical model (i.e., the left-hand side in Equation \eqref{eq:statmodel}).

The four right most columns contain the values of the variables on the right-hand side of Equation \eqref{eq:statmodel}, the \texttt{intercept}, the \texttt{iv\_effect}, and the \texttt{residual}. In addition, the \texttt{prediction} column shows the left-hand side of Equation \eqref{eq:predmodel}. As described above, every observation (i.e., row) has a idiosyncratic DV and residual. We also see that all values share one intercept, and the IV-effect is condition specific. As a consequence, the \texttt{prediction} column (which is the sum of intercept and iv-effect) also has two values, one for each condition. Finally, we can see that the sum of the three values on the right-hand side of Equation \eqref{eq:statmodel} equals the observed value of the DV. For example, consider \texttt{pid} = 4. If we enter the values into Equation \eqref{eq:statmodel} we have

\[
50.0 = 67.2 + (-1) + (-16.2).
\]

From this example data we can also understand better what the residuals mean, they are the difference between the observed value and the predicted value, \(\text{residual} = \text{DV} - \hat{\text{DV}}\). Consider again \texttt{pid} = 4. Here we have

\[
-16.2 = 50- 66.2.
\]

We can also see how the residual captures the idiosyncratic aspects of our data that cannot be explained by the condition means. For example, some participants -- such as \texttt{pid} = 5 and \texttt{pid} = 8 -- have large positive residuals indicating that they have good memory independent of their note taking condition. Likewise, \texttt{pid} = 4 has a large negative residual indicating comparatively worse memory (again independent of the note taking condition).

\hypertarget{understanding-the-statistical-model}{%
\subsection{Understanding the Statistical Model}\label{understanding-the-statistical-model}}

Now that we have described the parts of the statistical model we are almost ready to fit the model and interpret the output. Before doing so it makes sense to look at all the parts again individually and try to understand why we set up the statistical model in the way we do. Remember, our goal is to evaluate whether there is an effect of the condition in the population from which the data is sampled. To do so, we set up a model that partitions the observed data into three parts, the intercept representing the overall mean, the condition specific effect (IV-effect) representing the difference of the condition from the intercept, and the residuals representing the idiosyncratic part not explained by the model. The reason for doing so is that it allows us to zoom in on what matters for our statistical question, the condition specific effect. To answer the question if there is a difference between the conditions in the population, we can now focus on this part of the model. The overall level of performance captured in the intercept and the residuals can (for now) be ignored for this question. Consequently, the statistical test reported below is a statistical test of the condition effect. Thus, the reason for setting up the statistical model in this way is to make it easy to get an answer to the question that interests us: Is there an effect of our experimental manipulation/conditions on memory performance? To answer this question we only need to consider the condition effect.

\hypertarget{estimating-the-statistical-model-in-r}{%
\section{Estimating the Statistical Model in R}\label{estimating-the-statistical-model-in-r}}

\hypertarget{package-and-data-setup}{%
\subsection{Package and Data Setup}\label{package-and-data-setup}}

At the moment (i.e., during the development phase) the code requires the development version of \texttt{afex} (as only this has all data sets used here) which can be installed with the following code.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(}\StringTok{"devtools"}\NormalTok{)) \{}
  \FunctionTok{install.packages}\NormalTok{(}\StringTok{"devtools"}\NormalTok{)}
\NormalTok{\} }\ControlFlowTok{else}\NormalTok{ \{}
  \FunctionTok{install\_github}\NormalTok{(}\StringTok{"singmann/afex@v1"}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

For the statistical analyses reported in this book we generally use the \href{https://cran.r-project.org/package=afex}{\texttt{afex}} package \citep{R-afex} in combination with the \href{https://cran.r-project.org/package=emmeans}{\texttt{emmeans}} package \citep{lenth2021}. \texttt{afex} stands for ``analysis of factorial experiments'' and simplifies many of the things we want to do (full disclaimer: I am the main developer of \texttt{afex}). Most analyses can also be performed with different functions, but it is often easiest to use \texttt{afex} functions as they are developed particularly for cognitive and behavioural researchers working with experimental data. More specifically, \texttt{afex} functions provide the expected results for experimental data sets out-of-the-box without the need to change any settings (which is not true for the corresponding non-\texttt{afex} functions). \texttt{emmeans} stands for ``estimated marginal means'' and is the package we use once a statistical model is estimated to further investigate the results. \texttt{afex} and \texttt{emmeans} are fully integrated with each other which allows to test practically any hypotheses of interest with a combination of these two packages in a straight forward manner. We already introduce the interplay of these two packages here, and the next chapters will showcase the full power of this combination.

We also regular use functions from the \href{https://cran.r-project.org/package=tidyverse}{\texttt{tidyverse}} package (e.g., for plotting). \texttt{tidyverse} is a collection of packages developed mainly by \texttt{RStudio} and their head data scientist Hadley Wickham. A full introduction of the \texttt{tidyverse} is beyond the scope of the present book, interested readers are encouraged to read the introductory book, \citet{wickham2017}, which is also \href{https://r4ds.had.co.nz/}{available for free online}.

We begin the analysis by loading the three packages first (use \texttt{install.packages(c("afex",\ "emmeans",\ "tidyverse"))} in case they are not yet installed). We also change the default \texttt{ggplot2} theme using \texttt{theme\_set()} to a nicer one.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"afex"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"emmeans"}\NormalTok{)}
\FunctionTok{library}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\FunctionTok{theme\_set}\NormalTok{(}\FunctionTok{theme\_bw}\NormalTok{(}\AttributeTok{base\_size =} \DecValTok{15}\NormalTok{) }\SpecialCharTok{+} 
            \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position=}\StringTok{"bottom"}\NormalTok{, }
                  \AttributeTok{panel.grid.major.x =} \FunctionTok{element\_blank}\NormalTok{()))}
\end{Highlighting}
\end{Shaded}

The next step would be loading in the data. This is made easy here as the data from \citet{urry2021} is part of \texttt{afex}, under the name \texttt{laptop\_urry2021}. So we can load it with the \texttt{data()} function. We then also get an overview of the variables in this data set using \texttt{str()}, which returns the structure of a \texttt{data.frame}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"laptop\_urry2021"}\NormalTok{)}
\FunctionTok{str}\NormalTok{(laptop\_urry2021)}
\CommentTok{\#\textgreater{} \textquotesingle{}data.frame\textquotesingle{}:    142 obs. of  4 variables:}
\CommentTok{\#\textgreater{}  $ pid        : Factor w/ 142 levels "1","2","4","5",..: 1 2 3 4 5 6 7 8 9 10 ...}
\CommentTok{\#\textgreater{}  $ condition  : Factor w/ 2 levels "laptop","longhand": 1 2 2 1 2 2 1 2 2 1 ...}
\CommentTok{\#\textgreater{}  $ talk       : Factor w/ 5 levels "algorithms","ideas",..: 4 4 2 5 1 3 5 2 5 4 ...}
\CommentTok{\#\textgreater{}  $ performance: num  65.8 75.8 50 89 75.6 ...}
\end{Highlighting}
\end{Shaded}

The \texttt{str} function shows four variables, three of which we have already mentioned above:

\begin{itemize}
\item
  \texttt{pid}: participant identifier, a \texttt{factor} with 142 levels, one for each participant.
\item
  \texttt{condition}: \texttt{factor} identifying which note taking condition a participant belongs to, with two levels, \texttt{laptop} and \texttt{longhand}.
\item
  \texttt{talk}: A \texttt{factor} identifying which TED talk a participant saw, with 5 level.
\item
  \texttt{performance}: Numeric variable with participants memory performance on a scale from 0 (= no memory) to 100 (= perfect memory).
\end{itemize}

\hypertarget{estimating-the-statistical-model}{%
\subsection{Estimating the Statistical Model}\label{estimating-the-statistical-model}}

For estimating a basic statistical model using \texttt{afex} we can use the \texttt{aov\_car()} function. The next code snippet show how to do so for the example data, when saving the output in object \texttt{res1} .

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res1 }\OtherTok{\textless{}{-}} \FunctionTok{aov\_car}\NormalTok{(performance }\SpecialCharTok{\textasciitilde{}}\NormalTok{ condition }\SpecialCharTok{+} \FunctionTok{Error}\NormalTok{(pid), laptop\_urry2021)}
\CommentTok{\#\textgreater{} Contrasts set to contr.sum for the following variables: condition}
\end{Highlighting}
\end{Shaded}

The first argument to \texttt{aov\_car()} is a \texttt{formula} specifying the statistical model, \texttt{performance\ \textasciitilde{}\ condition\ +\ Error(pid)}. The second argument identifies the \texttt{data.frame} containing the data (i.e., all the variables appearing in the \texttt{formula}), \texttt{laptop\_urry2021}. We can also see that calling \texttt{aov\_car()} produces a status message informing us that contrasts are set to \texttt{contr.sum} for the variables in the model. This message is only shown for information purposes and can be safely ignored (we want \texttt{contr.sum} as contrasts for our variables, but as this is not the default \texttt{R} behaviour a message is shown).

A \texttt{formula} in \texttt{R} is defined by the presence of the tilde-operator \texttt{\textasciitilde{}} and the main way for specifying statistical models. It allows specifying statistical models in a similar way to the mathematical formulation, specifically the prediction equation of the statistical model, Equation \eqref{eq:predmodel}. Therefore, a \texttt{formula} provides a comparatively intuitive approach for specifying a statistical model. On the left hand side of the \texttt{\textasciitilde{}} we have the dependent variable, \texttt{performance}. On the right hand side we have the variables we want to use to predict the dependent variable.

In the present case, the right-hand side consists of two parts concatenated by a \texttt{+}, the independent variable \texttt{condition} and an \texttt{Error()} term with the participant identifier variable \texttt{pid}. Thus, there are two difference between the \texttt{formula} used here and the prediction Equation \eqref{eq:predmodel}, the \texttt{formula} misses an explicit intercept and we have specified an \texttt{Error()} term that is missing in Equation \eqref{eq:predmodel}. Let us address these two difference in turn. The intercept is not actually missing from this equation, but implicitly included. More specifically, an intercept is specified using a \texttt{1} in a \texttt{formula}. However, unless an intercept is explicitly suppressed -- which can be done by including \texttt{0} in the formula (and which should only be done if there are very good statistical reason to do so; i.e., it makes very rarely sense) -- it is always assumed to be part of the models. Consequently, including it explicitly produces equivalent results. The following code shows this by comparing the previous result without explicit intercept, \texttt{res1} with an \texttt{aov\_car} call with explicit intercept using the \texttt{all.equal()} function. This function can be used to compare arbitrary \texttt{R} objects and only returns \texttt{TRUE} if they are equal.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res1b }\OtherTok{\textless{}{-}} \FunctionTok{aov\_car}\NormalTok{(performance }\SpecialCharTok{\textasciitilde{}} \DecValTok{1} \SpecialCharTok{+}\NormalTok{ condition }\SpecialCharTok{+} \FunctionTok{Error}\NormalTok{(pid), laptop\_urry2021)}
\CommentTok{\#\textgreater{} Contrasts set to contr.sum for the following variables: condition}
\FunctionTok{all.equal}\NormalTok{(res1, res1b)}
\CommentTok{\#\textgreater{} [1] TRUE}
\end{Highlighting}
\end{Shaded}

The \texttt{Error()} term is a mandatory part of the model \texttt{formula} when using \texttt{aov\_car()} and is used to specify the participant identifier variable (i.e., \texttt{pid} in this case). For a simple example as the present one that seems unnecessary, but later in the book we will see why the requirement of the \texttt{Error()} term is useful.

Before looking at the results, let us quickly explain why the function for specifying models is called \texttt{aov\_car()}. A regular statistical model such as the ones considered here that solely includes factors (i.e., categorical variables) as independent variables is also known as \emph{analysis of variance}, which is usually shortened to ANOVA.\footnote{Describing in details why statistical models with solely factors as IVs are called analysis of variance even though we are comparing conditions means (and not variances) is beyond the present work. The short answer is that this has historical reasons. One can calculate the statistical tests in these models by hand by comparing different variance terms. For a more extensive explanations, interested readers are encouraged to read the excellent explanation in \citet{howellStatisticalMethodsPsychology2013} (any edition should have it).} The basic \texttt{R} function for ANOVA models is simply called \texttt{aov()}. However, \texttt{aov()} does not in all cases return the expected results for all types of ANOVA models considered in this book (i.e., in some situations \texttt{aov()} can return results that would be considered inappropriate, even when used carfeully). An alternative to \texttt{aov()} is the \texttt{Anova()} function from package \href{https://cran.r-project.org/package=car}{\texttt{car}} \citep{foxCompanionAppliedRegression2019} (where \texttt{car} stands for the book title, ``Companion to Applied Regression''). \texttt{Anova()} always returns the expected and appropriate ANOVA results when used correctly. However, calling \texttt{Anova()} requires at least two function calls and can become tricky with more complicated models discussed in later chapters. \texttt{aov\_car()} combines the simplicity of model specification of the \texttt{aov()} function with the appropriate statistical results from the \texttt{Anova()} function from the \texttt{car} package (i.e., \texttt{aov\_car()} calls \texttt{Anova()} internally).

\hypertarget{interpreting-the-results}{%
\subsection{Interpreting the Results}\label{interpreting-the-results}}

We can now look at the results of our statistical model. For this, we simply call the object that contains the results \texttt{res1} (we would get the same output when calling \texttt{print(res1)} or \texttt{nice(res1)}).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{res1}
\CommentTok{\#\textgreater{} Anova Table (Type 3 tests)}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Response: performance}
\CommentTok{\#\textgreater{}      Effect     df    MSE    F  ges p.value}
\CommentTok{\#\textgreater{} 1 condition 1, 140 269.66 0.52 .004    .471}
\CommentTok{\#\textgreater{} {-}{-}{-}}
\CommentTok{\#\textgreater{} Signif. codes:  0 \textquotesingle{}***\textquotesingle{} 0.001 \textquotesingle{}**\textquotesingle{} 0.01 \textquotesingle{}*\textquotesingle{} 0.05 \textquotesingle{}+\textquotesingle{} 0.1 \textquotesingle{} \textquotesingle{} 1}
\end{Highlighting}
\end{Shaded}

The default \texttt{aov\_car()} output is an ``Anova Table'' we will see throughout the book. We can also see that the results table contains ``Type 3 tests'', but we will ignore this for now. The only other option, Type 2 tests, produces the same results for the example data. We will get back to the meaning of ``type of test'' in later chapters when it makes a difference and ignore this part until then.

The next line of the results table is only reference information. We see that the response variable, which we also know as DV, is \texttt{performance}, just as we intended.

We then get a table of effects, which in this case only has one row, the effect of \texttt{condition}. This row contains all the information for our null hypothesis significance test (NHST) for the condition effect. The most important column in this output is the last column, \texttt{p.value}, or \(p\)-value. The \(p\)-value in this column is the main results of NHST and allows us to judge the compatibility of the data with the null hypothesis. It is the probability of obtaining a difference as extreme as observed when assuming that the null hypothesis of no difference is true. We see that in this case the \(p\)-value is not small, it is .47. Thus, the data are not incompatible with the null hypothesis.

In general, researchers have adopted a significance level of .05. This means that if a \(p\)-value is smaller than .05 we treat this as evidence that the data is incompatible with the null hypothesis. In this case we would say the result is ``significant''. However, as in our case the result is not smaller than .05 the result is ``not significant'' (I would avoid saying ``insignificant'' if the \(p\)-value is larger than .05, as ``significant'' is a technical term here). Thus, in the present case we do not reject the null hypothesis. The present data therefore do not provide evidence that the observed difference between the two modes of note taking generalises from the sample to the population according to NHST.

There are two further important columns whose results generally need to be reported, \texttt{df}, which stands for ``degrees of freedom'' (or \emph{df}), and \texttt{F}. Understanding these columns in detail is beyond the scope of the present chapter, so we will only introduce them briefly. There are two degrees of freedom reported here, the first value, 1, is the numerator degree of freedom. It is always given by number of conditions minus 1. In the present case, we have two conditions, \texttt{laptop} and \texttt{longhand}, so the numerator \emph{df} are 2 - 1 = 1. The second value is the denominator \emph{df}, which are generally given by number of participants minus numerator df minus 1. Here we have 142 participants and therefore 142 - 1 - 1 = 140. In general, the larger the denominator \emph{df} (i.e., the more participants we have) the better we can detect incompatibility with the null hypothesis (i.e., the easier it is to get small \(p\)-values). The \(F\)-value is a value expressing the observed incompatibility of the data with the null hypothesis. If \(F \leq 1\), the data are compatible with the null hypothesis. If \(F > 1\) the data are to some degree incompatible with the null hypothesis, with larger values indicating more incompatibility. The \(p\)-value is calculated from \emph{df} and \(F\)-value. Consequently, the results are usually reported in the following way: \(F(1, 140) = 0.52\), \(p = .471\).

The next column that is important is \texttt{ges} which stands for generalised eta-squared, using the mathematical notation with Greek letters, \(\eta^2_G\). \(\eta^2_G\) is a \emph{standardised effect size} that tells us something about the absolute magnitude of the observed effect \citep{olejnikGeneralizedEtaOmega2003, bakemanRecommendedEffectSize2005}. More specifically, \(\eta^2_G\) is supposed to be a measure of the proportion of variance in the DV that can be accounted for by a specific factor or IV in the model. For example, in the present case the condition effect is supposed to explain 0.4\% of the variance in performance. In general, we should avoid standardised effect sizes such as \(\eta^2_G\) and instead report \emph{simple effect sizes}. A simple effect size is expressed in units of our measured DV. For example, throughout this chapter we have mentioned that the observed difference in memory performance between both note taking conditions is 2.0 on the scale from 0 to 100. Here, the difference of 2.0 is a simple effect size. We will have to say more about effect sizes later, but as some journal editors or publishing guidelines require standardised effect sizes (which is statistically not a reasonable recommendation in my eyes) the default output contains it.

Finally, the default output contains the \texttt{MSE} column, which stands for ``mean squared errors''. This column is mainly included for historical reasons. Traditionally, ANOVA models could relatively easily be calculated by hand or by calculator based on different variance terms (hence the name, analysis of variance). One of this term is the mean squared error from which, in combination with the residual squared error, the \(F\)-value can be calculated. In my undergrad studies I still learned to calculate ANOVA by hand, but this seems rather unnecessary nowadays. Hence, we will simply ignore this column. Interested reader can find a detailed explanation about the meaning of MSE for example in \citet{howellStatisticalMethodsPsychology2013} or \citet{baguleySeriousStatsGuide2012}.

One thing we note in the results table is that it does not contain any information about the intercept. However, as discussed above, the intercept is included in the model. The reason for omitting the intercept from the default output is that it is generally not of primary interest. In experimental research usually the main interest is in the effect of our independent variables, the effect of the experimental manipulation. The statistical model that separates the intercept (i.e., overall mean) from the condition effect allows to zoom in on the relevant part. In line with this, the default output of \texttt{aov\_car} does the same. Later chapters will show how we can also get information about the intercept.

Estimating a statistical model with \texttt{aov\_car()} provides us with the inferential statistical results, the null hypothesis tests for the IV-effects shown above. To get these, we just need to call the object containing the results at the \texttt{R} prompt (e.g., calling \texttt{res1} in the present case). However, we can use the results object also for others parts of the statistical analyses, for data visualisation and follow-up analyses.

\hypertarget{data-visualisation}{%
\subsection{Data Visualisation}\label{data-visualisation}}

For data visualisation we can use the \texttt{afex} function \texttt{afex\_plot()} which is built on top of the \texttt{ggplot2} package. \texttt{afex\_plot()} requires an estimated model object (e.g., as returned from \texttt{aov\_car()}) and specifying which factors of the model we want to plot. In the present case, we only have one factor, \texttt{condition}, so we can only choose this one. Importantly, all factors passed to \texttt{afex\_plot()} need to be passed as character strings (i.e., enclosed with \texttt{"..."}).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{afex\_plot}\NormalTok{(res1, }\StringTok{"condition"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}

{\centering \includegraphics[width=0.7\linewidth]{stats_for_experiments_files/figure-latex/fig1-1} 

}

\caption{afex\_plot() figure for data from Urry et al. (2021)}\label{fig:fig1}
\end{figure}

This simple call to \texttt{afex\_plot()} produces already a rather good looking results figure combining the individual-level data points (in the background in grey) with the condition means (in black). Individual data points in the background that have the same or very similar values are displaced on the x-axis so they do not lie on top of each other. This is achieved through package \href{https://cran.r-project.org/package=ggbeeswarm}{\texttt{ggbeeswarm}} (which needs to be installed once: \texttt{install.packages("ggbeeswarm")}). The plot also per default shows 95\% confidence intervals of the means, which we will explain in detail in a later chapter.

As \texttt{afex\_plot()} returns a \texttt{ggplot2} plot object, we can manipulate the plot to make it nicer.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p1 }\OtherTok{\textless{}{-}} \FunctionTok{afex\_plot}\NormalTok{(res1, }\StringTok{"condition"}\NormalTok{)}
\NormalTok{p1 }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"note taking condition"}\NormalTok{, }\AttributeTok{y =} \StringTok{"memory performance (0 {-} 100)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_cartesian}\NormalTok{(}\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{100}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{group =} \DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.7\linewidth]{stats_for_experiments_files/figure-latex/fig2-1} \end{center}

For example, in the code snippet above we first save the plot object as \texttt{p1} and then call a number of \texttt{ggplot2} function on this plot object to alter the plot appearance (in \texttt{ggplot2} graphical elements are added to a plot using \texttt{+}). Function \texttt{labs()} is used to change the axis labels, \texttt{coord\_cartesian()} changes the extent of the y-axis (i.e., the plot now show the full possible range of memory performance score), and \texttt{geom\_line(aes(group\ =\ 1)} adds a line connecting the two means. This figure could now be used in a results report or manuscript as is.

\hypertarget{follow-up-analysis}{%
\subsection{Follow-Up Analysis}\label{follow-up-analysis}}

Follow-up analysis refers to an inspection of the predicted condition means and their relationships. In the case of a single independent variable with two levels (e.g., laptop versus longhand) their is not much to investigate in this regard. We can nevertheless show the general procedure. For follow-up analyses we generally begin with function \texttt{emmeans()} from package \texttt{emmeans} \citep{lenth2021}. Function \texttt{emmeans()} then returns the estimated marginal means, which is a slightly complicated way of saying condition means, plus additional statistical information.

Similarly to \texttt{afex\_plot()}, \texttt{emmeans()} requires an estimated model object as well as the specification of a factor in the model for which we want to get the condition means:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{emmeans}\NormalTok{(res1, }\StringTok{"condition"}\NormalTok{)}
\CommentTok{\#\textgreater{}  condition emmean   SE  df lower.CL upper.CL}
\CommentTok{\#\textgreater{}  laptop      68.2 1.99 140     64.3     72.1}
\CommentTok{\#\textgreater{}  longhand    66.2 1.91 140     62.4     70.0}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} Confidence level used: 0.95}
\end{Highlighting}
\end{Shaded}

For now we only focus on the estimates means in column \texttt{emmean} and ignore the additional inferential statistical information in columns \texttt{SE} to \texttt{upper.CL}. We can see that the reported means match the means given in the text at the very beginning of the chapter, \ref{ex:urry}.

The power of \texttt{emmeans} is not only to provide the condition means, but it also allows us to perform calculation on the condition means. For example, in the case of a factor with two levels we can easily calculate the difference between the condition means as our simple effect size. For this, we can save the object returned by \texttt{emmeans()} and then call the \texttt{pairs()} function on this object which gives us all pairwise comparisons of conditions means of which there is only one in the present case (we would get the same results by combining both calls into one: \texttt{pairs(emmeans(res1,\ "condition"))}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{em1 }\OtherTok{\textless{}{-}} \FunctionTok{emmeans}\NormalTok{(res1, }\StringTok{"condition"}\NormalTok{)}
\FunctionTok{pairs}\NormalTok{(em1)}
\CommentTok{\#\textgreater{}  contrast          estimate   SE  df t.ratio p.value}
\CommentTok{\#\textgreater{}  laptop {-} longhand     1.99 2.76 140 0.722   0.4715}
\end{Highlighting}
\end{Shaded}

The output shows a mean difference of 1.99 which slightly differs from the 2.0 reported above, which is slightly concerning. However, the results reported above are rounded to one decimal only. If we do so for the present results, we also get an estimated difference of 2.0 (we will not explain this code in detail here):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{em1 }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pairs}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{as.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{format}\NormalTok{(}\AttributeTok{digits =} \DecValTok{1}\NormalTok{, }\AttributeTok{nsmall =} \DecValTok{1}\NormalTok{)}
\CommentTok{\#\textgreater{}            contrast estimate  SE    df t.ratio p.value}
\CommentTok{\#\textgreater{} 1 laptop {-} longhand      2.0 2.8 140.0     0.7     0.5}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary}{%
\section{Summary}\label{summary}}

The goal of this chapter was to introduce the standard statistical approach for analysing experimental data with one independent variable with two levels -- an experiment with two conditions. Practically every time when we run such an experiment, we observe that there is some mean difference in the dependent variable between the two conditions. For our example data by \citet{urry2021} there was a memory difference of 2.0 points between the two note taking conditions (laptop versus longhand) on the response scale from 0 to 100.

The important statistical question we then have is whether there is any evidence suggesting that the observed difference in our sample generalises to the population. The sample are the participants in our experiment and the population refers to all possible participants that could have been sampled. For \citet{urry2021} this population could be loosely described as students taking notes or maybe more precisely undergraduate students at research intensive (R1) US universities. The question we would like to get a statistical answer to is: Should we believe that there generally is a memory difference between note taking with a laptop versus longhand? To answer this question we need \emph{inferential statistics}.

The inferential statistical approach we are using is called \emph{null hypothesis significance testing} or NHST. However, NHST does not directly address the question whether there is evidence for a difference in the population. Instead, NHST tests the compatibility of the data with the \emph{null hypothesis} -- the assumption that there is no difference between the condition in the population. The most important result from NHST is the \(p\)-value. The \(p\)-value is a measure of the compatibility of the data with the null hypothesis; it is the probability of obtaining a results as extreme as observed assuming the null hypothesis is true. If the \(p\)-value is smaller than .05 we reject the null hypothesis that there is no difference. In this case we decide that there is evidence for a difference (although this does not follow with logical necessity).

To apply NHST to the data we set up a \emph{statistical model} that observed partitions the data into three parts (Equation \eqref{eq:statmodel}): the intercept representing the overall mean, the effect of the independent variable (i.e., the difference of the condition means from the intercept), and the residuals representing the idiosyncratic aspects not explained by the other parts of the model. This partitioning allows us to zoom in on the part of the data that we are interested in, the effect of our independent variable, the experimental manipulation.

To estimate a statistical model to the data we used function \texttt{aov\_car()} from the \texttt{afex} package. \texttt{aov\_car()} allows us to specify the statistical model using a formula of the form \texttt{dv\ \textasciitilde{}\ iv\ +\ Error(pid)} (where \texttt{pid} refers to the variable in the data with the participant identifier) mimicking the mathematical specification of the statistical model. The default output returns an ANOVA table which provides a null hypothesis significance test for our \texttt{iv}, the independent variable. The returned table is called an ANOVA table because statistical models that only contain factors are called analysis of variance or ANOVA. In the present case, the statistical model only has a single factor, note taking condition, with two levels, laptop versus longhand. In the returned ANOVA table, we do not only have the \(p\)-value for our experimental factor, but additional inferential statistical information such as the degrees of freedom, \emph{df}, and the \(F\)-value.

We can also use the object returned from \texttt{aov\_car()} for plotting using function \texttt{afex\_plot()}. This function produces a plot combining the individual-level data points with the condition means. This provides a comprehensive display of the data of the experiment. As the function returns a \texttt{ggplot2} object, this plot can be be easily modified to create a figure that can be used in a results report.

We can also use the object returned from \texttt{aov\_car} for follow-up analyses using \texttt{emmeans}. With \texttt{emmeans} we can easily obtain the condition means (or estimated marginal means) on the dependent variable. Based on these condition means we can calculate the observed effect size (i.e., the mean difference).

Applying the statistical model to the data from \citet{urry2021} showed a non significant difference, \(F(1, 140) = 0.52\), \(p = .471\). This suggests that there is no difference in memory performance after watching a talk and taking notes with either a laptop or in longhand format.

  \bibliography{book.bib,packages.bib}

\end{document}
