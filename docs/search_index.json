[["index.html", "Introduction to Statistics for Experimental Psychology with R Chapter 1 Overview", " Introduction to Statistics for Experimental Psychology with R Henrik Singmann 2021-08-04 Chapter 1 Overview At this point in time, the book is being written so only few chapters are already available. Chapter 2 provides a general introduction. Chapter 3 introduces the basic statistical approach. "],["role-of-statistics-in-the-research-process.html", "Chapter 2 Role of Statistics in the Research Process 2.1 The Research Process 2.2 Example I: Testing a New Therapy 2.3 Example II: The Psychology of Loss Aversion 2.4 Epistemic Gaps: The Difference Between What we Want to Know and What we can Know 2.5 Summary", " Chapter 2 Role of Statistics in the Research Process In this book we are concerned with experimental psychology, in particular the statistical analysis of experiments in psychology and related disciplines such as language science, behavioural science, cognitive science, or neuroscience. The order expressed in the previous sentence  science first and statistical analysis second  is one of the overarching principles with which we will think about statistics. Whereas the goal here is to introduce the concepts and techniques required to perform a statistical analysis of (mostly experimental) data, the perspective taken here is that a statistical analysis can only be performed or understood within the scientific context it takes place in. One consequence of this perspective is that the start point of any statistical analysis needs to be a specific and clear research question. In the case in which both data collection and analysis is guided by such a research question, a statistical analysis is generally an indispensable part of the research process. As we will describe in more detail in this and the coming chapters, statistics is the tool that allows us to draw inferences that go beyond the data we have observed. In more technical terms, statistics is the one tool in the research tool kit that allows us to generalise from the current sample to a population from which this sample is drawn (with a number of caveats we will also discuss). This ability to generalise is what allows us to connect experimental results with the research questions and underlying theories. In sum, the statistical techniques introduced in this book can provide meaningful and scientifically helpful answers when the data is collected and analysed with a clear research question in mind. 2.1 The Research Process The research process in psychology and related sciences roughly consists of four interrelated steps: the research question, the operationalisation of the research question and the data collection, the statistical analysis of the data, and finally the communication of the results. Let us explain this process in more detail. Any research should begin with a research question. For the disciplines considered here this is generally a theory or hypothesis about human behaviour or the human mind. For example, a widely accepted idea in decision making and behavioural science is that people exhibit loss aversion  the displeasure resulting from losing £10 is stronger than the pleasure derived from winning £10. We will look at this example in more detail in this chapter but will also consider other examples below. For now, it is sufficient to see that research questions often involve general statement that involve not directly observable quantities (such as pleasure or displeasure). The next step in the research process is the transformation of the research question into an empirical and statistical hypothesis, we can call this step operationalisation or measurement. As the example of loss aversion shows, research questions are often general. Consequently, there are a multitude of possible studies that can be performed to investigate one research question. However, for any specific study a researchers needs to decide on one specific study design. What exactly are we measuring to investigate the question we are interested in? Once the research question has been operationalised and the corresponding data is collected it is time for the statistical analysis. Generally, the statistical analysis answers one specific question: Does the data provide evidence for or against the empirical hypothesis derived from the research question? The remainder of the book will show in detail how to perform statistical analyses for common study designs and how to interpret the results in light of the research question and operationalisation. Once the data is sufficiently analysed, we have reached the final step of the research process, we need to communicate the results. There are different forms of results communication depending on your goal and audience (e.g., scientific journal article, dissertation report, conference presentation, or a press release). Whereas the different forms differ in the amount of detail and background that that is provided, they all need to provide a truthful and comprehensive account of the whole research process: What is the research question? How was it investigated? What are the results? What does this mean for the research question? Often, the difficult problem to solve during this step is to provide a comprehensive and truthful account but in a succinct manner. One important tool for doing so is through graphical means  results figures. Consequently, in this book we will discuss both how to present statistical results in a text and how to create appropriate results figures. What this abstract overview of the research process shows is three important things. The primacy of the research question. The research question determines the operationalisation and thus which data is collected. The research question also determines the statistical analysis, but indirectly; the research question determines the empirical hypothesis which is then tested in the analysis. The statistically analysis is not directly connected to the research question. The statistical analysis is performed on the operationalisation of the research question, but not on the research question itself. What this means is that the statical analysis itself cannot directly inform us about the research question, or in other words, we cannot statistically test the research question. Instead, statistics can only tell us something about a specific operationalisation. Whether or not this allows strong inferences about the research question depends on the operationalisation. And as shown in the following examples, an important part of the scientific discourse is to argue whether certain operationalisations allow one to address specific research questions. However, this is generally not a statistical question. Statistics is not the end goal of the research. Instead, the end goal is usually a written communication of the research. In most cases, the statistical analysis is an indispensable part of this communication that can provide evidence for or against a specific empirical hypothesis. However, to understand the full meaning and implications of a particular statistical result, it is important to know its context  the research question and its operationalisation. It is the task of the research to communicate this context when communicating the research. Without the context, the impact and meaning of a statistical result is severely limited. 2.2 Example I: Testing a New Therapy To get a better understanding of the research process and the problems that can arise in it, let us consider a first example research question and how it can be investigated empirically. In many clinical domains, a frequent research question is whether a new therapeutical intervention works better than an existing one. The basic study design for this type of research question is quite clear, a randomized controlled trial. We want to compare the new treatment with a control treatment in an experimental setting  eligible participants are randomly assigned to receive either the new or the existing treatment. Whereas the general design of this study is fixed, important questions in the operationalisation step are how to measure the outcome intervention and how to implement the control treatment. For example, if we are testing a new therapy against a mental health disorder, are we interested if the new treatment reduces symptom as measured by a questionnaire, reduces the number of sick days, or the number of recurring episodes? Depending on the differences between the new and old treatment it is possible that one treatment does better on one of this measures whereas the other is better for another measure. When performing the research we need to consider these different possibilities before we can collect any data. And if we collect multiple measures, we should determine what our primary outcome is and how we deal with the additional outcomes. A further problem when comparing therapeutical interventions is how exactly to set up the control condition. If every therapists involved in the study is a proponent of the new treatment  which commonly happens as tests of new treatments are often done by those developing the new treatment  they are unlikely to invest the same amount of effort in the control treatment than in the new treatment. This difference in effort, also known as allegiance effects, can affect the results independent of whether or not the new therapy is actually more effective. Consider a trial in which we have decided on one outcome measure, but therapists are more invested in the new treatment than in the old treatment. After we have collected the data, we run the statistical analysis which provides evidence that the new treatment performs better than the control treatment on the selected outcome measure. What would this mean for the research question? The researchers performing such a study would probably conclude that the new treatment is better than the old treatment. However, given the difference in allegiance to the two treatments, such a conclusion seems premature. It is also plausible that the therapists, because of their stronger investment with the new treatment, invested more effort into the patients treated with the new treatment. This provides an alternative explanation for the results: This difference in effort could be responsible for the difference in the outcome instead of the difference in the two treatments. To provide compelling evidence that the new treatment is indeed better than the old treatment, the researchers would need to test their new treatment in a setting in which this alternative explanation would not work. For example, by showing that the new treatment is also better than the old treatment for a therapist with an allegiance to the old treatment. 2.3 Example II: The Psychology of Loss Aversion For our second concrete example, let us consider a less applied research question. Loss aversion is one of the assumptions underlying prospect theory (Kahneman and Tversky 1979), a mathematically formalised theory combining cognitive psychology with economic theory.1 The concise description of loss aversion is that losses loom larger than gains (Kahneman and Tversky 1979, 279). Less literary, loss aversion means that the negative psychological impact (or feeling associated with) a loss of a certain amount is larger than the positive psychological impact of a gain of the same amount. For example, loss aversion predicts that the displeasure or pain from losing £10 is larger than the pleasure or joy from winning £10. We can see that loss aversion is a theoretical statement involving latent - that is, unobservable - quantities such as negative or positive feelings (i.e., displeasure versus pleasure). We can ask people how they feel, but we cannot easily observe feelings without asking people. So how can we test whether people indeed show loss aversion if we cannot directly observe the theoretical constructs that form the core of it? One possibility for testing the hypothesis that individual show loss aversion is hinted at above. We could either give people a certain amount, say £10, or take it away form them, and then ask them how they feel. This procedure runs into at least two problems. First, it is clearly ethically unacceptable to perform an experiment that consists of taking £10 away from some our participants. Second, even if we were to overcome the ethical problems (e.g., by first giving participants an endowment and only take money away from that endowment) there would still be the problem of how to measure the psychological impact of the two events. More specifically, lets say we ask the participants whom we have give £10 how much joy they feel after having received the money on a scale from 1 = no joy to 10 = maximum joy. We also ask those participants from whom we have taken £10 how much pain they have felt after the money was taken from then on a scale from 1 = no pain to 10 = maximum pain. Let us also assume that we indeed observe that participants who gained £10 said that their joy as on average a 4, whereas participants that lost £10 said their pain was on average a 5. Would this indicate evidence for loss aversion? Only if the two scales were directly comparable; that is, only if a 4 on the joy scale would be comparable in psychological impact to a 4 on the pain scale. It is easy to imagine situation in which this would not be the case. For example, if the maximum amount of joy we could perceive is larger than the maximum amount of pain, then the same value on the scale does not correspond to similar psychological impact. Given these problems, testing loss aversion using such an operationalisation is not very common (for an exception see Harinck et al. 2007). 2.3.1 Evidence for Loss Aversion: The Reflection Effect A more common operationalisation for testing loss aversion is the comparison of choice patterns across different risky choices, lotteries, or gambles (these terms can be understood interchangeably here), a common experimental paradigm in decision making and behavioural economics. A lottery in this sense consists of different options, each of which associated with one or multiple outcomes, from which the participant has to choose one. For example, one of the lotteries used in Kahneman and Tversky (1979) was the following. Which option do you prefer? A: An 80% chance of $4,000 or a 20% chance of $0. B: 100% guarantee of $3,000. In the original study (Kahneman and Tversky 1979, Problem 3) 95 participants were asked this question. Of those, only 20% chose option A and 80% option B. Because the lottery above contains only gains (i.e., all outcomes are either positive or zero), these results do not allow any conclusions regarding loss aversion.2 The study by Kahneman and Tversky (1979) also contained the corresponding loss only lottery (i.e., all outcomes were either negative or zero): Which option do you prefer? A: An 80% chance of -$4,000 or a 20% chance of $0. B: 100% guarantee of -$3,000. The same 95 participants as before were asked this question and their results differed markedly from the previous results. For the loss only lottery 92% chose option A and only 8% option B. Thus, even though the only mathematical difference between both lotteries is the sign of the outcomes (i.e., the probabilities and absolute value of the outcomes are identical across lotteries), participants preferences are almost reversed. In the gain case, participants prefer the sure option (Option B), but in the loss case they prefer the risky outcome (Option A). This results pattern  preferring the sure option in the gain case, but the risky option in the loss case  is also known as the reflection effect and taken as evidence for loss aversion (Kahneman and Tversky 1979). In the gain case, prefer the sure outcome to the risky outcome because they are unwilling to risk gaining nothing, even though the risky option contains the larger potential outcome. In contrast, in the loss case people are more willing to gamble on the risky option than pick the sure outcome, because they are unwilling to lose anything at all, even though the risky option has the potential of resulting in a larger loss than the sure option. So do these results provide compelling evidence that there is such a thing as loss aversion? Results such as those from Kahneman and Tversky (1979) that show how differently people behave whether they are dealing with a gain versus when dealing with a loss certainly appear to support this theoretical idea (e.g., C. Camerer 2005). And this also make sense intuitively. Most people (me included) feel that in the gain case, they would prefer option A and in the loss case they would prefer option B even though they know and understand the mathematical details of the lotteries. However, as in the previous example, the evidence for loss aversion discussed here hinges on a particular operationalisation of the theoretical idea. From these results, we have not really learned anything about the different psychological impact of a gain and a loss of the same magnitude in the mind of people. The only thing we have learned is that for certain lotteries people change their behaviour depending on whether the outcomes are gains and losses. Can we find an alternative for this data pattern that does not involve loss aversion? 2.3.2 Alternative Explanation: Loss Aversion or Loss Seeking? One clever alternative explanation for the why it looks like there is loss aversion was provided by Walasek and Stewart (2015). In their study, participants were also presented with lotteries, but they differed in a few aspects from the lotteries used by Kahneman and Tversky (1979). An example of one of their lotteries is shown in Figure 2.1 below. As shown in the figure, each lottery involved mixed outcomes, that is both gains and losses (i.e., -$18 and +$20 in the example). Furthermore, each lottery only consisted of one option and not two. So instead of having to choose between two options, participants only had to choose whether to accept and play a lottery or not. Finally, both possible outcome always had a 50% probability of occurring. To make this logic clearer to participants, they were told that accepting the lottery shown in Figure 2.1 was equal to flipping a coin that has -$18 on one side and +$20 on the other side. Depending on which outcome comes out on top, their money would change accordingly. If participants rejected a lottery, they neither lost nor won any money. Figure 2.1: Screenshot of lottery task used to investigate loss aversion. This screenshot is from Walasek and Stewart (2015, Figure 1). As many experiments that fall within a cognitive domain, the study of Walasek and Stewart (2015) consisted of a series of similar trials in which participants had to do the same task (i.e., accept or reject the shown lottery). What differed across trials were the values of the two possible outcomes. For example, in one of the conditions of the experiment losses ranged from -$6 to -$20 in increments of -$2 (resulting in 8 different possible losses) and gains ranged from $12 to $40 in increments of $4 (resulting in 8 different possible gains). Across all trials for one participant in this condition, all possible losses were combined with all possible gains so that in total participants had to decide for \\(8 \\times 8 = 64\\) lotteries whether they accepted or rejected it. When analysing their data, Walasek and Stewart (2015) focussed on a mathematical model-based analysis using a variant of prospect theory. Whereas there are clear merits to such an approach, as it it able to provide informative summaries of large amounts of data, it can make it difficult to focus on interesting patterns in subsets of the data. Furthermore, analysis using a mathematical models such as prospect theory requires statistical knowledge that is beyond the scope of the current book. Consequently, we will look at a simple data pattern to see whether the data can tell us something about loss aversion. The 64 trials each participant of Walasek and Stewart (2015) worked on contain a subset from which one can directly investigate this question. Whereas most of the lotteries shown to participants were asymmetric  that is, the potential loss differed numerically from the potential gain (such as for the example in Figure 2.1)  a small subset of lotteries were symmetric, for these lotteries the amounts for the potential loss was equal to the amount of a potential gain. More specifically, the symmetric lotteries were -$12/+$12, -$16/+$16, and -$20/+$20. For the condition described above in which losses ranged up to -$20 and gains ranged up to +$40, the 191 participants accepted the symmetric lotteries only 21% of the time. This result makes sense in light of loss aversion. When losing a certain amount of money is worse than winning the same amount of money, I should reject a symmetric lottery in which I am equally likely to lose or to win a certain amount of money. The clever manipulation of Walasek and Stewart (2015) was that they included three further conditions in which they changed the range of possible outcomes. In addition to the -$20/+$40 condition discussed above, the 202 participants in the $-20/+$20 condition saw lotteries with losses ranging to -$20 and gains also ranging to +$20 only. Another group of 190 participants, the -$40/+$40 condition, saw lotteries with losses ranging to -$40 and gains also ranging to +$40. Finally, Walasek and Stewart (2015) also included a -$40/+$20 condition with 198 participants in which the losses ranged to -$40, but the gains only to +$20 (i.e., the complement to the -$20/+$40 condition). Importantly, in all conditions the number of possible outcomes for losses and gains was 8 (so the step size was either \\(\\pm\\)$2 or \\(\\pm\\)$4). The table below shows the possible outcome for each condition Possible outcomes for the lotteries in Experiment 1 of Walasek and Stewart (2015). In each condition, each participant saw 64 lotteries resulting from combining all possible gains with all possible losses in that condition. Condition -$20/+$40: Gains $12 $16 $20 $24 $28 $32 $36 $40 -$20/+$40: Losses -$6 -$8 -$10 -$12 -$14 -$16 -$18 -$20 -$20/+$20: Gains $6 $8 $10 $12 $14 $16 $18 $20 -$20/+$20: Losses -$6 -$8 -$10 -$12 -$14 -$16 -$18 -$20 -$40/+$40: Gains $12 $16 $20 $24 $28 $32 $36 $40 -$40/+$40: Losses -$12 -$16 -$20 -$24 -$28 -$32 -$36 -$40 -$40/+$20: Gains $6 $8 $10 $12 $14 $16 $18 $20 -$40/+$20: Losses -$12 -$16 -$20 -$24 -$28 -$32 -$36 -$40 As a consequence of this design, what changed across conditions was whether the symmetric lotteries were relatively good or relatively bad. To understand this, we need to look at the remaining asymmetric lotteries. In the -$20/+$40 condition discussed so far, there were more lotteries in which the possible gain was larger than the possible loss (e.g., a -$18/+$20 lottery) than lotteries for which the possible loss was larger than the gain (e.g., a -$20/+$18 lottery). Consequently, the symmetric lotteries were relatively bad (i.e., compared to the many lotteries in which the possible gain is larger than the possible loss). In the -$20/+$20 and -$40/+$40 conditions, the asymmetric lotteries were balanced. In half of the asymmetric lotteries the possible gain was larger than the possible loss, whereas for the other half the possible loss was larger than the possible gain. Consequently, the symmetric lotteries were neither relatively good nor relatively bad. Finally, in the -$40/+$20 condition the pattern was flipped with respect to the -$20/+$40 condition. There were only few lotteries in which the possible gain was larger than the possible loss compared to the many lotteries for which the possible loss was larger than the gain. Consequently, the symmetric lotteries were relatively good. So does it matter whether the symmetric lotteries are relatively good or not? Indeed it does. As a reminder, people were unlikely to accept the symmetric lotteries in the -$20/+$40 condition in which the symmetric lotteries were relatively bad. Participants in this condition only accepted 21% of the symmetric lotteries. In the -$20/+$20 condition in which the symmetric lotteries were neither relatively good nor relatively bad, participants accepted 50% of the symmetric gambles. Similarly, in the -$40/+$40 condition participants accepted 42% of the symmetric gambles. Finally, in the -$40/+$20 condition in which the symmetric lotteries were relatively good, participants accepted 71% of the symmetric gambles. What these results show is that the choice pattern for lotteries are not always in line with the idea of loss aversion. Only in a context in which a symmetric lottery is relatively bad do we see evidence in line with the idea of loss aversion. If we are in a context in which a symmetric lottery is relatively good, we see the opposite pattern that one could term loss seeking. What does this mean for loss aversion? The original idea of Kahneman and Tversky (1979) that what matters is the magnitude of a loss or gain surely is not in line with the results of Walasek and Stewart (2015). Instead, Walasek and Stewart (2015) argue that what is relevant to determine the psychological impact of a gain or loss is the relative magnitude or rank of a possible outcome: Compared to other gains or losses I regularly experience, is this a large gain or large loss? Before moving on and linking this example of the psychology of loss aversion to the general goal of this book, let us answer one last question. If what matters is the rank (as suggested by Walasek and Stewart (2015)) and not the magnitude of an outcome (as proposed by Kahneman and Tversky (1979)), why do we see evidence for an effect of magnitude in the study of Kahneman and Tversky (1979) that did not manipulate the context of gains and losses? An answer to this question is provided by Stewart, Chater, and Brown (2006). They argue (and also provide empirical evidence) that in our daily lives we experience more small losses (e.g., buying something at the bakery) and more larger gains (e.g., the monthly salary). As a consequence, for a gain and loss with the same magnitude the relative position of the gain compared to all other gains is lower than the corresponding relative position of the loss compared to all other losses. From this difference, we should generally observe a pattern consistent with loss aversion but for a different theoretical reason than proposed by Kahneman and Tversky (1979). 2.4 Epistemic Gaps: The Difference Between What we Want to Know and What we can Know The goal of this chapter is to provide an introduction to the role of statistics in the research process. Why do we need statistics and what can it tell us about the research questions we are interested in? However, so far we have not talked much about statistics but mostly introduced examples of the research process. The reason for this is that it is important to understand what we generally want to know when we do empirical work in psychology or related disciplines. We need statistics because we have a research question for which we want an answer. If you do not yet know about statistics in detail, the application of statistical methods can appear like a magical machinery that provides us with an answer to the research question we have. You throw your data in, turn the statistics machinery on, and get an answer to your research question out. Sadly, this image of statistics is false. The reason is that there are at least two epistemic gaps in the research process that prevent us from getting a straight answer to our research question. In this chapter we will introduce these gaps to ensure you can get a realistic image of the role of statistics in the research process. At this point, you might wonder what an epistemic gap is. Epistemology is a branch of analytical philosophy that is concerned with knowledge (e.g., what is knowledge, how de we know that we know) and epistemic is the corresponding adjective. Consequently, an epistemic gap describes the difference between what we want to know and what we can actually know.3 What we can know from scientific research is a question addressed in philosophy of science. Based on the examples above we will take a look at some arguments from philosophy of science that show that unfortunately, what we can know is often quite different from what we would like to know. A competent application of statistics requires that one is aware of this problem and avoids over-interpreting the results from ones research. 2.4.1 Epistemic Gap 1: Underdetermination of Theory by Data Above we have provided two examples of the research process in psychology, one applied research question (Is a new therapeutical intervention better?) and one theoretical (or basic) research question (Is there evidence for loss aversion?). In both cases we have seen that answering the research question requires carefully thinking about the operationalisation. How exactly should we set up a study to test this question? And once we have decided on one operationalisation, we have seen that we can find alternative explanations that can explain the results without making the assumptions of the original research question. In other words, even though the operationalisation was carefully chosen, it could not unambiguously answer our research question. The fact that we could not compellingly answer the research questions despite employing a carefully chosen operationalisation is not a problem that is unique to our two examples. In contrast, an important insight from philosophy of science is that this is a problem of any empirical study. This issue is also known as underdetermination of theory by data or the Duhem-Quine thesis and always occurs if there is a difference between the research question and the corresponding operationalisation. And as we have seen in the examples, there essentially always is. There are different aspects or different angles with which we can look at and understand underdetermination. The first aspect has to do with the specification of the research question and its operationalisation. Research questions usually involve unobservable constructs  such as emotions (e.g., fear), memory, attention, comprehension, or learning  or vague phrases, such as works better or improves. An empirical investigation of such questions however requires a precise specification and operationalisation. By going from the research question to the concrete operationalisation, there is no guarantee that the operationalisation captures the intended meaning of the research question. For example, consider again our first example of the test of a therapeutical intervention. Imagine we found that the new intervention decreased patient scores on a symptom questionnaire more strongly than the old treatment, but did not reduce the number of sick days due to the disorder. Should we interpret this in the way that the new treatment works better than the old treatment? In some sense it does, but in another not. The problem is that the nuances that result from operationalising a research question concretely do not always align with the broad way in which we like to think about research questions. At this point you might think this does not yet sound like such a big problem. We just need to define our research questions precisely enough and then we are able to learn something about our research question. Sadly, this is easier said then done. The first problem is that it is often impossible to precisely define our research question, because we have not yet found a way to precisely define the constructs that are involved in it (this is known as the problem of coordination, (Kellen et al. 2021)). For example, if you have they hypothesis that a specific emption, say fear, is related to some behavioral pattern, say aggression, you run into the problem that there is not generally agreed upon definition of either of these constructs. There probably exist questionnaires for measuring fearfulness and aggressive tendencies, but these questionnaires do not represent the corresponding constructs or a definition of them. If you were to ask a sample of participants to fill out these questionnaires and found that the scores of the participants in these two questionnares are related, it would not allow you to conclude that fearfulness and aggression are related. The only conclusion that would be allowed is that fearfulness as measured with the questionnaire is related to aggression as measured with the questionnaire. Of course, as scientists we would like to make the general conclusion that the constructs are related, but such an inference is not logically allowed. The general problem that we run into is the Duhem-Quine thesis; any empirical hypothesis that is tested in a study contains of two parts: The theoretical prediction as well as a set of auxiliary assumptions that link the theoretical prediction with the data. To stay with our example, our theoretical prediction could be that fear and aggression are related. The auxiliary assumption are all additional assumptions that are needed to test this question empirically as decided on as part of the operationalisation: that the questionnaire is a valid measure of the constructs (which is a big assumption), that the data collection took place without any unforeseen problems, that we have tested enough participants to find an effect, that we use appropriate statistical procedures, etc. As can be seen, the list of auxiliary assumption is somewhat limitless and difficult to enumerate fully. It also contains quite mundane assumptions such that we have to assume that the research actually took place and is not just made up by the researcher (for an exception, see the case of Diederik Stapel). The core of the Duhem-Quine thesis is that any empirical result does not pertain solely to the theoretical prediction of interest, but the union (or conjunction) of the theoretical prediction of interest with the auxiliary assumptions. If the results are in line with the empirical hypothesis, that only supports the theoretical prediction if all auxiliary assumptions are true. Likewise, if the results are not in line with the empirical hypothesis, this only provides evidence against the theoretical prediction if all auxiliary assumptions are true. However, testing whether all auxiliary assumptions are true cannot be done in the same study that tests the empirical hypothesis we set out to test (because we can always come up with more and more auxiliary assumptions not specifically tested). Consequently, any individual result on its own cannot provide conclusive evidence for or against a particular theoretical prediction, there can always be an alternative explanation that differs from the theory or hypothesis one has.4 That is what is meant by the underdetermination of theory by data. Whereas this issue might seem like a purely philosophical discussion, it is far from it. Most actual scientific discussions in the literature are about the auxiliary assumptions that are part of the operationalisation of a research question. For example, in the example of the therapeutical intervention, the idea that therapists apply both old and new therapy in exactly the same manner is an auxiliary assumption that is questioned by the concept of therapeutical allegiance. Likewise, the argument for loss aversion as proposed by Kahneman and Tversky (1979) hinges on the auxiliary assumption that participants interpret the possible outcomes of the lotteries in terms of their magnitude or absolute value. As shown by Walasek and Stewart (2015), at least in some cases this auxiliary assumption does not appear to hold and participants instead interpret the relative value of the possible outcomes of the lotteries. It will be easy to find similar examples for the research area you are interested in. To sum this up, the problem of underdetermination and the first epistemic gap is that any particular results never uniquely supports or challenges one theoretical position or hypothesis. For any result that appears to support a theory there is another theory that makes the same prediction because an auxiliary hypothesis could be false and thus require a different theory. Likewise, for any results that seems to disagree with a theory, the theory can always be protected by claiming one of the auxiliary assumptions is incorrect. And this is also exactly what happens in real scientific discourse. As an example, when John Bargh, a prominent social psychologist from Yale, was confronted with results that disagreed with one of his most prominent findings (Doyen et al. 2012) he attacked (in a now deleted blog post that still can be found here) the incompetent or ill-informed researchers and claimed their study had many important differences from our procedure, all of which worked to eliminate the effect. As this section has section has shown, questioning the methods (i.e., the auxiliary assumptions) is a legitimate defence that protects ones theory. Of course, one can question the auxiliary assumptions of the original results that appeared to support the theory in the same way. In the case of Bargh, it appears that this is exactly what happened. Most other psychologists have stopped believing his original finding (e.g., Harris, Rohrer, and Pashler 2021). 2.4.2 Epistemic Gap 2: Signal and Noise The first epistemic gap is that there is no strong logical link between the theories underlying our research questions and the operationalisation of the research question. Thus, in terms of the steps in our research process it concerns the relationship between step 1, the research question, and step 2, the operationalisation and data collection. The next epistemic gap concerns the relationship of steps 2 and step 3, the statistical analysis. As described above (2.1), the important task during the operationalisation is to transform the research questions into an empirical hypothesis. That is, figuring out and describing which possible outcome would support our theoretical hypothesis (i.e., the outcome predicted by our theory). As part of this we should also clearly designate a possible outcome that, if it were to occur, would speak against our theoretical hypothesis. Once we have decided on this, we collect the data and then run the statistical analysis. The goal is that the statistical analysis provides us with evidence with respect to the empirical hypothesis. Does the data support the empirical hypothesis or does it not? Before providing an overview of how this is done, let us go back to the second example above, the study of Walasek and Stewart (2015). In contrast to the original formulation of loss aversion (Kahneman and Tversky 1979), which is based on the magnitude or absolute value of a gain or loss, the theoretical prediction of Walasek and Stewart (2015) was that what drives peoples behaviour is the relative value of a gain or loss. To test this, they presented participants with lotteries in different conditions in which the range of gains and losses differed. In one condition there were small losses and large gains and in another condition there were large losses and small gains (we ignore the other two conditions here). The hypothesis that follows from this design is that the relative attractiveness of the symmetric lotteries (in which magnitude of possible loss = magnitude of possible gain) differs in both conditions. In the small loss/large gains condition the symmetric lottery is relative unattractive and in the large loss/small gains condition it is relatively attractive. The resulting empirical prediction is that participants should be less willing to accept the symmetric lotteries in the small loss/large gains condition than in the large loss/small gains condition. In line with this prediction, participants in the small loss/large gains condition accepted only 21% of the symmetric lotteries whereas participants in the large loss/small gains condition accepted 72% of the symmetric lotteries. From just looking at the bare numbers, the results appear to support the empirical prediction. Participants are roughly 50% less likely to accept the symmetric lotteries in the small loss/large gains condition than in the large loss/small gains condition. However, how can we be sure this particularly observed difference is not a chance occurrence. Maybe we just got unlucky and the participants in the small loss/large gains condition are for some reasons generally less likely to accept any lotteries than the participants in the large loss/small gains condition. Maybe the former participants all had a horrible night of sleep and are in a really bad mood at the time of testing and therefore reject all gambles whereas this was not the case for the latter. If this were the case, the observed difference would not actually tell us anything about our research questions. The problem described in the previous paragraph is at the heart of the statistical approach described in this book. The core problem is that the responses we get from participants in experiments are inherently noisy. Human participants can do things for an unlimited number of reasons. Some of these reasons are related to our research question and its operationalisation and other reasons are not. For example, if participants read the lotteries carefully and think before they provide an answer, it is likely that the values of the possible outcome play a role in their answer. In this case, their responses are relevant for our research question. But what if participants are distracted by a message on their phone and do not read the problem fully? Or what if they intend to accept a lottery and accidentally reject it (i.e., press the wrong button)? We can also imagine that it matters if participants are relatively rich or relatively poor. For someone with a million on the bank, it might not really matter if they lose or win $16 so they might be inherently more likely to gamble on such a lottery than someone for which this is more than the hourly wage. In all these cases, the values of the lotteries have a minor effect and thus the responses are more or less irrelevant for our research question. For the question of whether or not the results support our empirical prediction we therefore would like to distinguish between those responses that are relevant for our research question  we can call this the signal  and those responses that are generated more or less randomly and are irrelevant for our research question  we can call this the noise. If we had a procedure that could distinguish signal from noise we could then simply see whether the signal supports our empirical prediction. If it did, the data would provide support for the hypothesis and if not the data would not support the empirical predictions. Sadly, such a procedure does not and cannot exist (as we would then know why people do what they do, which is the reason we do research in the first place). In the absence of a procedure that can definitely separate the contribution of signal and noise, the statistical approach introduced here compares an estimate of the signal with an estimate of the noise. Let us assume for a moment the estimated signal supports our empirical prediction as in our example (i.e., we predict that participants are less likely to accept a lottery in the small loss/large gains condition and this is what the data shows). We then compare this estimated signal against the estimated noise. If the estimated signal is large given the estimated level of noise, we assume that the data supports the empirical prediction. If the estimated signal is not large given the estimated noise, we assume the data does not support our empirical prediction. So how can we estimate the signal and the noise? Estimating the signal is straight forward. We just use the observed difference between the conditions as our estimate of the signal. So for the example from Walasek and Stewart (2015) this would be the observed difference in accepting the symmetric lotteries between the two conditions which was roughly 50%. Estimating the noise is a bit more complicated and will be described in detail in later chapter. For now it is enough to understand that it is affected by two components: (1) The variability in responses within each condition and (2) the overall sample size (i.e., number of participants). If the variability within each condition becomes smaller (i.e., measurement becomes more precise) and the sample size stays the same, the levels of noise decreases. Likewise, if the sample size increases with a constant level of variability, the level of noise decreases. Another important question is what counts as large when comparing the estimated signal to the estimated noise. In the following chapters we will introduce a decision threshold to make this judgement.5 If the signal to noise ratio is above the threshold, we act as if there were a signal and change our believes. If it is not, we cannot make a decision. This decision threshold is chosen such that across many such decisions we control our rate of making false positive decisions. In particular, the decision threshold is chosen such that if we were in a situation in which there was no signal, we only incorrectly assumed that there is a signal in 5% of decisions. Taken together, the statistical procedures we are using attempt to answer the question whether there is a signal that supports the empirical hypothesis given that human data is inherently random and noisy. The problem in this is that the estimated signal  the observed difference between the conditions  is also affected by the noise. We never know if the observed difference is due to the signal we are interested in or just based on noise. To overcome this problem we compare the observed signal with the observed level of noise. If the observed level is large relative to the observed noise, we decide the data supports the presence of the signal. In other words, we never really know if the current data really supports our prediction or not, we just act as if it does. There always is a non-zero chance that the effect is due to the noise. Because we only have this one data set we are analysing, we cannot be 100% certain our estimate of the signal and the noise is fully accurate. However, as we will describe in detail later, across decisions that use this statistical decision procedure, it controls our rate of making false positive decisions (i.e., assume there is a signal when there is none). So as in the case for the first epistemic gap, the second epistemic gap also shows that what we cannot really learn what we wanted to know, if the data supports the empirical prediction or not. If the signal is large relative to the noise, we have evidence that it does, but this evidence is never fully conclusive. The evidence might be strong, and we will later see how we can identify that, but there always should be some remaining doubt in the back of our head. Maybe we just got unlucky and the participants in our study responded in a way that made it look like there is a signal, but there isnt. With just one data set in hand, this cannot really be ruled out 2.5 Summary In this chapter we have provided a conceptual overview of the research process in psychology and related disciplines. In this concept, the research always begins with a research question. What is it that I want to know? Often this research questions stems from a particular theory that we want to test, but it can also be a purely applied hypothesis. The next important step is the operationalisation of this question followed by the data collection. That means we need to find appropriate tasks or measures and develop a study design with which we can test the empirical hypothesis following from our research question. As we have seen in the discussion of the first epistemic gap, the consequence of the separation of research question and operationalisation is that, strictly speaking, our study only lets us learn more about the tasks and measures we are using. Because of the problem of underdetermination of theory by data, even an apparently positive result does not allow us to infer that it supports our theoretical hypothesis. The core problem is that the empirical hypothesis is a combination of theoretical hypothesis and auxiliary assumptions and we cannot rule out that one of the auxiliary assumptions is false. With the collected data in hand, the next step is to perform the statistical analysis. Here, we hope to find evidence that informs us about our empirical hypothesis. The procedure we will use in this book attempts to distinguish between the signal in the data, the part of the data relevant to our empirical hypothesis, and the noise, the randomness that is inherent in using human participants.6 However, the second epistemic gap entails that even with such a statistical procedure, we cannot find fully conclusive evidence. The problem is that we cannot estimate the true amount of noise in the data. Research participants have a myriad of potential reasons of why they show a certain behaviour and these reasons need not be related to our research question. With more precise measures and more participants, we can control this level of noise to some degree, but ultimately cannot be sure whether we did not just get unlucky and what we see is due to noise and not because of our hypothesis. The final step in the research process is the communication of our results. This step essentially combines all previous step. We need to communicate the research question, the operationalisation, the data collection process and sample, and the results from the statistical analysis. Whereas the communication of the results is ultimately the goal of any research project, it is also the step during which we have to be mindful of the limits of our research. The biggest danger is that we forget the epistemic gaps that are inherent in any empirical research and oversell our results. We of course want that our research allows us to answer our (potentially big and broad) research questions, but we should be honest with ourselves and our audience and stick to the reality that we are primarily learning something about our operationalisation. When we get a statistical result that appears to support our empirical prediction we want treat it as if it is true, but we should be clear that there always is a chance that our result might be a fluke. 2.5.1 What Shouldnt and What Should we do? Let us end this chapter with some concrete examples of what we shouldnt and what we should do. To motivate ones research question, it is a good idea to start with the big picture. What are the real world issues and theoretical problems we want to address? Whereas this is a good idea, we should not mistake our operationalisation of the research question with this big picture. For example, in a recent paper on which I was a co-author (Baumann et al. 2020), we developed a new task, the ticket shopping task, to investigate sequential decision making problems (in this section, the details of the tasks are not really relevant so we will only mention the names, but do not describe tasks in detail). Whereas I feel we could make a good case that we could figure out to some degree what people do in this task, this does not mean that this is what people generally do in sequential decision making problem. Likewise, whereas there might be both a model-free and model-based reinforcement learning system in the brain, the popular two-step task (Daw et al. 2011) is unlikely to reveal all its mechanism or even prove its existence.7 In fact, there is good evidence participants in this task show many behaviours that are unrelated to the theoreical question of interest (e.g., Akam, Costa, and Dayan 2015). Another example comes from the domain of moral reasoning. Here, a prominent procedure is the use of the trolley problem and similar vignettes (Greene et al. 2001). Whereas these vignettes have revealed interesting results, it seems questionable to assume that participants responses represent the extent of their moral reasoning or that they are particularly predictive of participants real-world behaviour.8 Similar arguments can be made for many other experimental domains but also whenever particular questionnaires are used (e.g., as discussed in the examples above). It makes sense to look a bit critical at the argument we have made here. Surely, if we use a task such as the ticket shopping task or the two-step task we learn something about the underlying research question and theory? We surely do, the question that is difficult to answer is exactly what we learn. As an empirical example to illustrate this problem, let us consider the research on risk preferences in decision making. The idea of risk preferences is that some people might be more willing to take risks (e.g., when gambling or when choosing an investment) than others. There exist a number of different tasks to investigate risk preferences experimentally, such as the balloon analogue risk task [BART; Lejuez et al. (2002)] or the Columbia card task (Figner et al. 2009), as well as a number of different questionnaires. A large study with around 1500 participants who each performed eight different tasks and filled out twelve different questionnaires designed to measure risk preferences (Pedroni et al. 2017; Frey et al. 2017) could show that participants behaviour across tasks and questionnaires was surprisingly unrelated. Whereas participants who scored high on one questionnaire also scored high on other questionnaires (i.e., the different questionnaires shared a common risk trait), the scores on the questionnaires were largely unrelated to the behaviours in the different tasks. Furthermore, behaviours across the different risk tasks were unrelated to each other (i.e., a participant who was specifically risky in one tasks was not particularly risky in another task). In other words, even though the tasks and questionnaires all appear to measure risk preferences their failure to find a consistent pattern across participants suggests they fail to do so in a coherent manner. One might wonder if the fact that the questionnaires were related among each other represents some sort of silver lining here. I would not share this interpretation and instead attribute this to common-method variance. The important result is that the questionnaires were also unrelated to the behaviour in the tasks. This tells me that at this point we do not really understand what risk preferences are or how to measure them. To sum this up, it is important to keep in mind that the thing we learn something about in our research is primarily our operationalisation and measurement. If we want to make a case that we also learn something about the underlying research question, we have to make a good case for this and spell out which alternative explanations we rule out and which auxiliary assumption we can take for granted. This usually requires considering other results than our own study. In short, do not confuse the task or measurement with the theory or research question. When communicating the statistical results we also need to avoid overselling the results. As a general principle, we should report the results in a humble manner. To this end, we should avoid language that suggests a level of confidence that we cannot provide. This means, statistical results never prove or confirm our empirical prediction. Instead, they may support it or suggest certain interpretations. As we have discussed, a statistical analysis never gives us perfect confidence that a particular result supports an empirical prediction. The only way to get some confidence that we did not get unlucky with our sample of participants is that the study is repeated by other  a so called replication. If a completely independent set of researchers finds the same pattern of results, this provides us with rather strong evidence that the study design indeed gives rise to this data pattern. As the final part of this chapter, let us review the pattern of replication results for our two main examples. The results of Kahneman and Tversky (1979), using their original lotteries, were recently successfully replicated in a large scale study involving 19 countries and more than 4000 participants (Ruggeri et al. 2020).9 For the Walasek and Stewart (2015) study, the pattern is not yet as clear. Whereas I am not aware of any published exact (or direct) replication of their study, the original authors have attempted to extend their findings somewhat (Walasek and Stewart 2019) and found similar but less pronounced differences between condition. However, using a different study design Schneider, Kauffman, and Ranieri (2016) also showed that the acceptance rated of symmetric lotteries are strongly affected by which other lotteries are presented. References "],["standard1.html", "Chapter 3 The Standard Approach for One Independent Variable 3.1 Example Data: Note Taking Experiment 3.2 The Logic of Inferential Statistics 3.3 The Basic Statistical Model 3.4 Estimating the Statistical Model in R 3.5 Summary", " Chapter 3 The Standard Approach for One Independent Variable In this chapter we are introducing the standard statistical approach for analysing experimental data with one independent variable (i.e., one factor). The simple case for this is a study comparing two experimental conditions on one dependent variable. We will exemplify the standard approach for this design using a recent and straightforward experiment. 3.1 Example Data: Note Taking Experiment Heather Urry and 87 of her undergraduate and graduate students (Urry et al. 2021) (yes, all 87 students are co-authors!) compared the effectiveness of taking notes on a laptop versus longhand (i.e., pen and paper) for learning from lectures. 142 participants (which differed from the 88 authors) first viewed one of several 15 minutes lectures (TED talks) during which they were asked to take notes either on a laptop or with pen and paper. As this was a proper experiment, participants were randomly assigned to either the laptop (\\(N = 68\\)) or longhand condition (\\(N = 74\\)). After a 30 minutes delay, participants were quizzed on the content of the lecture. The answers from each participant were then independently rated from several raters (which agreed very strongly with each other) using a standardised scoring key resulting in one memory score per participant representing the percentage of information remembered ranging from 0 (= no memory) to 100 (= perfect memory).10 Figure 3.1 below shows the memory scores across both note taking conditions. Figure 3.1: Distribution of memory scores from Urry et al. (2021) across the two note taking conditions. In Figure 3.1, each black point shows the memory score of one participant so the full distribution of the data is visible. The shape of the distribution is also shown via a violin plot (i.e., the black outline around the points) to which we have added three lines representing three summary statistics of the data. From top to bottom these lines are the 75% quantile, the 50% quantile (i.e., the median), and the 25% quantile. The red points show the mean and the associated error bars show the standard error of the mean11. We see that the two means are quite similar, although the mean in the laptop condition is slightly larger, by 2.0 points (mean laptop = 68.2, mean longhand = 66.2). 3.2 The Logic of Inferential Statistics The previous paragraph provide us with descriptive statistics describing the results in the experiment by Urry et al. (2021): There is a memory difference of 2.0 on the scale from 0 to 100 between the laptop and the longhand condition for the sample of 142 participants. However, as researchers we are usually not primarily interested what happens in our sample. What we would like to know if our results generalises to the population from which this sample is drawn. In this case, we would like to know whether there is a memory difference between note taking with a laptop or in longhand format for students (as this is roughly the population the sample is drawn from). Going beyond the the present sample is the goal of inferential statistics. There are different inferential statistical approaches, and we are focussing on the most popular one, null hypothesis significance testing (NHST). We will describe NHST more thoroughly in the next chapter, but will already introduce its main ideas here. For NHST, the question of generalising from sample to population is about two different possible true states of the world: There either is no difference in conditions means in the population (i.e., there only is a mean difference in our sample due to chance) or there is a difference in the condition means in the population. To decide between these possibilities, we set up a statistical model for the data. This statistical model allows us to assess if there is no difference in the population  we call this possible state of the world the null hypothesis. More specifically, the statistical model allows us to test how compatible the data is with the null hypothesis of no difference. The test of the null hypothesis proceeds as follows: (1) We assume that the state of the world in which there is no difference in the population means is true. (2) Based on this assumption we calculate how likely it is to observe a difference as large as the one we have observed in our sample. (3) If the probability of observing a difference as large as the one we have is very small, we take this as evidence that the null hypothesis of no difference is not true  we reject the null hypothesis. (4) We act as if there were a difference in the population. In this case (i.e., we reject the null hypothesis and act as if there is a difference), we say our experimental manipulation has an effect. As is clear from this description, the logic underlying inferential statistics using NHST is not trivial. To make it clearer, let us apply the logic to the example data. We want to know whether the observed mean memory difference between note taking with a laptop and note taking in long hand format in our sample generalises to the population of students that take note in either of these formats. To do so, we set up a statistical model for our data. This model allows us to test how compatible our data is with the null hypothesis that there is no mean memory difference in the population. Specifically, the model allows us to calculate the probability of obtaining a memory difference as large as the one we have observed assuming the there is no mean memory difference in the population. If our data is incompatible with the null hypothesis (i.e., it is unlikely to obtain a memory difference as large as the one we have observed if the null hypothesis were true), we reject the null hypothesis of no mean memory difference in the population. Because we reject the null hypothesis we then act as if there was a mean memory difference in the population. In other words, we then act as if the type of note taking had an effect on memory after lectures in the population. Even though the logic of NHST is not necessarily intuitive, it is clearly helpful for researchers. After running an experiment we really would like to know if the difference observed in our experiment (i.e., in our sample) is meaningful in the sense that it generalises to the population from which we have sampled our participants. And in almost every actual experiment there is some mean difference between the condition (i.e., it is extreme unlikely that both conditions have exactly the same mean). Thus, we pretty much always face this question. NHST allows us to test whether the observed difference is compatible with a world in which there is no difference. If this is unlikely, we decide (i.e., act as if) there were a difference. What we can see from spelling out the logic in detail is that there are quite a few inferential steps we have to make to get to what we want. We design experiments with the goal in mind to find a difference between the different experimental conditions. However, we then do not test this directly. Instead, we test the compatibility of the data with the converse of what we are actually interested in  the null hypothesis of no effect. If this test fails (i.e., shows that the data is likely incompatible with the null hypothesis) we then make two inferential steps. First we reject the null hypothesis and then we act as if there were a difference. Both of these inferential steps are not necessitated logically. What this means is that inferences based on NHST alone are never extremely strong. NHST is the de facto standard procedure for inferential statistics across empirical sciences (i.e., not only in psychology and related disciplines). Understanding the logic of NHST will enable you to understand the majority of empirical papers and will also allow you to apply inferential statistics in your own research. Nevertheless, there exist a long list of popular criticisms of NHST (e.g., Rozeboom 1960; Meehl 1978; Cohen 1994; Nickerson 2000; Wagenmakers 2007). We will discuss these criticisms in more detail in later chapters, but for now it is important to realise that NHST does not allow to test, or prove, whether there is a mean difference in the population. The only thing NHST calculates is a probability of how compatible the data is with the null hypothesis. If this probability is low that does not necessarily mean that there is a difference. Likewise, if this probability is high that does not necessarily mean there is no difference. All inferences we draw based on NHST results are probabilistic in itself (i.e., can be false). So the most important rule when interpreting the results from NHST is to be humble. NHST never proves or confirms anything. Instead NHST results suggest or indicate certain interpretations. If we do not over-interpret results, but stay instead stay humble in our interpretations, we are unlikely to fall prey to the common (and often justifiable) criticisms of the NHST framework. 3.3 The Basic Statistical Model To apply inferential statistics in the NHST framework to our data, we begin by setting up a statistical model to the data. A statistical model attempts to explain (or predict) the observed values of the dependent variable (DV) from the independent variable (IV).12 In the experimental context this means predicting our observed outcome, the DV, from the experimental manipulation, the IV. The basic statistical model partitions the observed DV into three parts that: the overall mean, which for reasons that will become clear later is called the intercept, the effect of the IV, and the part of the data that cannot be explained by the model, the residuals. When summing these three parts together, they result in the observed value. In mathematical form we can express this as \\[\\begin{equation} \\text{DV} = \\underbrace{\\text{intercept}}_{\\text{overall mean}} + \\text{IV-effect} + \\text{residual}. \\tag{3.1} \\end{equation}\\] (For those not used to reading mathematical expressions, the point at the end of the equation is simply a full stop that ends the sentence and has no mathematical meaning.) As someone without a mathematics background myself, I know that equations in a text are often more intimidating than immediately useful. Consequently, before moving on it makes sense to go through this equation in more detail. Furthermore, all statistical analyses discussed in this book are applications of Equation (3.1). This equation forms the foundation for the statistical analysis of experimental data and thus understanding it will unlock all analyses discussed in this book. Consequently, it makes sense to spend more time on it. Let us consider the the variables in Equation (3.1) in more detail. When doing so, we also consider how many different possible values each variable can take on.13 The following Figure, a variant of Figure 3.1, shows the elements graphically and we explain them in the text just below. Figure 3.2: Data from Urry et al. (2021) showing the overall mean (intercept, blue dotted line), the condition specific effects (difference between dashed red lines for the condition means and the blue line), and the residuals (grey lines from condition means to data points). \\(\\text{DV}\\): The dependent variable, DV, are the observed values, one for each observation/participant. For the example data this are all the 142 black data points shown in Figure 3.2. Thus, our statistical model tries to explain the individually observed values. \\(\\text{intercept}\\): The intercept represents the overall mean. Consequently, we only have one intercept (i.e., the intercept is the same for each observation). In experimental designs we define this as the mean of all condition means. For the example data the intercept is (68.2 + 66.2) / 2 = 67.2 and is shown as a blue dotted line in Figure 3.2. \\(\\text{IV-effect}\\): The IV-effect represents the effect of our independent variable which we define as the difference between the condition means and the intercept (i.e., the deviation of the condition means from the intercept). Thus, we always have as many different IV-effects as we have conditions. For the example data with only two conditions, we only have two different IV-effects, both of which with the same magnitude and only differ in sign, 1.0 for the laptop condition and -1.0 for the longhand condition. If we add these values to the intercept, we get the condition means. As we will discuss further below, this is the most relevant part for answering the statistical question of interest. In Figure 3.2, the red dashed line (and the red points) show the condition means, thus the condition effects are the differences between the blue line and the red lines. \\(\\text{residual}\\): The residuals are the idiosyncratic aspects of the data that are left unexplained by the statistical model. As the model only predicts the condition means (i.e., intercepts plus independent variable), these are the deviations of the individual observations from the condition means. Thus, as for the DV, we have as many residuals as we have values of the DV. In Figure 3.2, the residuals are shown as grey lines from the condition means to each data point. This is all the information (or variability in the data) our model cannot explain. 3.3.1 Model Predictions A simplification of Equation (3.1) that makes it clearer what the statistical model predicts is obtained if we ignore the residuals for a moment. As a reminder, the residuals are the part of the data that remains unexplained. In other words, these represent all the idiosyncratic parts of the data independent of our manipulation (e.g., some participants have better memory than others independent of how they took notes). What remains from our statistical model if we ignore all idiosyncratic aspects are only the predictions based on our IV. In the case of experimental data, the IV is the experimental condition. Thus, what a statistical model actually predicts is the means of the experimental conditions. We can again formalise this as \\[\\begin{equation} \\hat{\\text{DV}} = \\text{intercept} + \\text{IV-effect}. \\tag{3.2} \\end{equation}\\] Here, the hat symbol (\\(\\hat{}\\)) means predicted value. Thus in contrast to the actual DV above, we only have the predicted DV in this equation. When performing statistical analyses it sometimes help to remind oneself that all a standard statistical model predicts are the condition means. We generally do not make predictions about individual participants or consider other factors that are not part of the model. We only predict, and are interested in, the condition means. 3.3.2 Statistical Model for the Example Data Let us take a look at the first six participants and their values for all the variables in the basic statistical model to get a better understanding of Equation (3.1). pid condition overall  intercept iv_effect prediction residual 1 laptop 65.8 67.2 1 68.2 -2.4 2 longhand 75.8 67.2 -1 66.2 9.6 4 longhand 50.0 67.2 -1 66.2 -16.2 5 laptop 89.0 67.2 1 68.2 20.8 6 longhand 75.6 67.2 -1 66.2 9.4 8 longhand 83.3 67.2 -1 66.2 17.1 The first three columns show the data. pid is the participant identifier (id) column. As it is often the case for real data, some ids are missing (here 3 and 7) for various reasons (e.g., potential participants were interested in the study and received an id, but then did not finish or start the experiment) so the first 6 rows already go up to pid = 8. condition tells us in which note taking condition a participant was and overall is their memory score on the scale from 0 to 100 which serves as the DV in the statistical model (i.e., the left-hand side in Equation (3.1)). The four right most columns contain the values of the variables on the right-hand side of Equation (3.1), the intercept, the iv_effect, and the residual. In addition, the prediction column shows the left-hand side of Equation (3.2). As described above, every observation (i.e., row) has a idiosyncratic DV and residual. We also see that all values share one intercept, and the IV-effect is condition specific. As a consequence, the prediction column (which is the sum of intercept and iv-effect) also has two values, one for each condition. Finally, we can see that the sum of the three values on the right-hand side of Equation (3.1) equals the observed value of the DV. For example, consider pid = 4. If we enter the values into Equation (3.1) we have \\[ 50.0 = 67.2 + (-1) + (-16.2). \\] From this example data we can also understand better what the residuals mean, they are the difference between the observed value and the predicted value, \\(\\text{residual} = \\text{DV} - \\hat{\\text{DV}}\\). Consider again pid = 4. Here we have \\[ -16.2 = 50- 66.2. \\] We can also see how the residual captures the idiosyncratic aspects of our data that cannot be explained by the condition means. For example, some participants  such as pid = 5 and pid = 8  have large positive residuals indicating that they have good memory independent of their note taking condition. Likewise, pid = 4 has a large negative residual indicating comparatively worse memory (again independent of the note taking condition). 3.3.3 Understanding the Statistical Model Now that we have described the parts of the statistical model we are almost ready to fit the model and interpret the output. Before doing so it makes sense to look at all the parts again individually and try to understand why we set up the statistical model in the way we do. Remember, our goal is to evaluate whether there is an effect of the experimental manipulation (i.e., a difference between the two note taking conditions) in the population from which the data is sampled. To do so, we set up a model that partitions the observed data into three parts, the intercept representing the overall mean, the condition specific effect (IV-effect) representing the difference of the condition means from the intercept, and the residuals representing the idiosyncratic part not explained by the model. The reason for doing so is that it allows us to zoom in on what matters for our statistical question, the condition specific effect. To answer the question if there is a difference between the conditions in the population, we can now focus on this part of the model. The overall level of performance captured in the intercept and the residuals can (for now) be ignored for this question. Consequently, the statistical test reported below is a statistical test of the condition effect. Thus, the reason for setting up the statistical model in this way is to make it easy to get an answer to the question that interests us: Is there an effect of the note taking manipulation/conditions on memory? To answer this question we only need to consider the condition effect. 3.4 Estimating the Statistical Model in R 3.4.1 Package and Data Setup For the statistical analyses reported in this book we generally use the afex package (Singmann et al. 2021) in combination with the emmeans package (Lenth 2021). afex stands for analysis of factorial experiments and simplifies many of the things we want to do (full disclaimer: I am the main developer of afex). Most analyses can also be performed with different functions, but it is often easiest to use afex functions as they are developed particularly for cognitive and behavioural researchers working with experimental data. More specifically, afex functions provide the expected results for experimental data sets out-of-the-box without the need to change any settings (which is not true for the corresponding non-afex functions). emmeans stands for estimated marginal means and is the package we use once a statistical model is estimated to further investigate the results. afex and emmeans are fully integrated with each other which allows to test practically any hypotheses of interest with a combination of these two packages in a straight forward manner. We already introduce the interplay of these two packages here, and the next chapters will showcase the full power of this combination. We also regular use functions from the tidyverse package (e.g., for plotting). tidyverse is a collection of packages developed mainly by RStudio and their head data scientist Hadley Wickham. A full introduction of the tidyverse is beyond the scope of the present book, interested readers are encouraged to read the introductory book, Wickham and Grolemund (2017), which is also available for free online. We begin the analysis by loading the three packages first (use install.packages(c(\"afex\", \"emmeans\", \"tidyverse\")) in case they are not yet installed). We also change the default ggplot2 theme using theme_set() to a nicer one. library(&quot;afex&quot;) library(&quot;emmeans&quot;) library(&quot;tidyverse&quot;) theme_set(theme_bw(base_size = 15) + theme(legend.position=&quot;bottom&quot;, panel.grid.major.x = element_blank())) The next step would be loading in the data. This is made easy here as the data from Urry et al. (2021) is part of afex, under the name laptop_urry. So we can load it with the data() function. We then also get an overview of the variables in this data set using str(), which returns the structure of a data.frame. data(&quot;laptop_urry&quot;) str(laptop_urry) #&gt; &#39;data.frame&#39;: 142 obs. of 6 variables: #&gt; $ pid : Factor w/ 142 levels &quot;1&quot;,&quot;2&quot;,&quot;4&quot;,&quot;5&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ condition : Factor w/ 2 levels &quot;laptop&quot;,&quot;longhand&quot;: 1 2 2 1 2 2 1 2 2 1 ... #&gt; $ talk : Factor w/ 5 levels &quot;algorithms&quot;,&quot;ideas&quot;,..: 4 4 2 5 1 3 5 2 5 4 ... #&gt; $ overall : num 65.8 75.8 50 89 75.6 ... #&gt; $ factual : num 61.7 68.3 33.3 85.7 69.2 ... #&gt; $ conceptual: num 70 83.3 66.7 92.3 82.1 ... The str function shows six variables, three of which we have already mentioned above: pid: participant identifier, a factor with 142 levels, one for each participant. condition: factor identifying which note taking condition a participant belongs to, with two levels, laptop and longhand. talk: A factor identifying which TED talk a participant saw, with 5 level. overall: Numeric variable with participants overall memory performance on a scale from 0 (= no memory) to 100 (= perfect memory). This variable is called overall because it is the average of two separate memory performance scores given below. factual: Numeric variable with participants memory score for factual questions (ignored in this chapter). conceptual: Numeric variable with participants memory score for conceptual questions (analysed in the next chapter). 3.4.2 Estimating the Statistical Model For estimating a basic statistical model using afex we can use the aov_car() function. The next code snippet show how to do so for the example data, when saving the output in object res1 . res1 &lt;- aov_car(overall ~ condition + Error(pid), laptop_urry) #&gt; Contrasts set to contr.sum for the following variables: condition The first argument to aov_car() is a formula specifying the statistical model, overall ~ condition + Error(pid). The second argument identifies the data.frame containing the data (i.e., all the variables appearing in the formula), laptop_urry. We can also see that calling aov_car() produces a status message informing us that contrasts are set to contr.sum for the IVs in the model. This message is only shown for information purposes and can be safely ignored (we want contr.sum as contrasts for our variables, but as this is not the default R behaviour a message is shown). A formula in R is defined by the presence of the tilde-operator ~ and the main way for specifying statistical models. It allows specifying statistical models in a similar way to the mathematical formulation, specifically the prediction equation of the statistical model, Equation (3.2). Therefore, a formula provides a comparatively intuitive approach for specifying a statistical model. On the left hand side of the ~ we have the dependent variable, overall. On the right hand side we have the variables we want to use to predict the dependent variable. In the present case, the right-hand side consists of two parts concatenated by a +, the independent variable condition and an Error() term with the participant identifier variable pid. Thus, there are two difference between the formula used here and the prediction Equation (3.2), the formula misses an explicit intercept and we have specified an Error() term that is missing in Equation (3.2). Let us address these two difference in turn. The intercept is not actually missing from this equation, but implicitly included. More specifically, an intercept is specified using a 1 in a formula. However, unless an intercept is explicitly suppressed  which can be done by including 0 in the formula (and which should only be done if there are very good statistical reason to do so; i.e., it makes very rarely sense)  it is always assumed to be part of the models. Consequently, including it explicitly produces equivalent results. The following code shows this by comparing the previous result without explicit intercept, res1 with an aov_car call with explicit intercept using the all.equal() function. This function can be used to compare arbitrary R objects and only returns TRUE if they are equal. res1b &lt;- aov_car(overall ~ 1 + condition + Error(pid), laptop_urry) #&gt; Contrasts set to contr.sum for the following variables: condition all.equal(res1, res1b) #&gt; [1] TRUE The Error() term is a mandatory part of the model formula when using aov_car() and is used to specify the participant identifier variable (i.e., pid in this case). For a simple example as the present one that seems unnecessary, but later in the book we will see why the requirement of the Error() term is useful. Before looking at the results, let us quickly explain why the function for specifying models is called aov_car(). A regular statistical model such as the ones considered here that solely includes factors (i.e., categorical variables) as independent variables is also known as analysis of variance, which is usually shortened to ANOVA.14 The basic R function for ANOVA models is simply called aov(). However, aov() does not in all cases return the expected results for all types of ANOVA models considered in this book (i.e., in some situations aov() can return results that would be considered inappropriate, even when used carfeully). An alternative to aov() is the Anova() function from package car (Fox and Weisberg 2019) (where car stands for the book title, Companion to Applied Regression). Anova() always returns the expected and appropriate ANOVA results when used correctly. However, calling Anova() requires at least two function calls and can become tricky with more complicated models discussed in later chapters. aov_car() combines the simplicity of model specification of the aov() function with the appropriate statistical results from the Anova() function from the car package (i.e., aov_car() calls Anova() internally). 3.4.3 Interpreting the Results We can now look at the results of our statistical model. For this, we simply call the object that contains the results res1 (we would get the same output when calling print(res1) or nice(res1)). res1 #&gt; Anova Table (Type 3 tests) #&gt; #&gt; Response: overall #&gt; Effect df MSE F ges p.value #&gt; 1 condition 1, 140 269.66 0.52 .004 .471 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 The default aov_car() output is an Anova Table we will see throughout the book. We can also see that the results table contains Type 3 tests, but we will ignore this for now. The only other option, Type 2 tests, produces the same results for the example data. We will get back to the meaning of type of test in later chapters when it makes a difference and ignore this part until then. The next line of the results table is only reference information. We see that the response variable, which we also know as DV, is overall, just as we intended. We then get a table of effects, which in this case only has one row, the effect of condition. This row contains all the information for our null hypothesis significance test (NHST) for the condition effect. The most important column in this output is the last column, p.value, or \\(p\\)-value. The \\(p\\)-value in this column is the main results of NHST and allows us to judge the compatibility of the data with the null hypothesis. It is the probability of obtaining a difference as extreme as observed when assuming that the null hypothesis of no difference is true. We see that in this case the \\(p\\)-value is not small, it is .47. Thus, the data are not incompatible with the null hypothesis and does not suggest that there is a memory difference between note taking with a laptop or in longhand format during lectures. In general, researchers have adopted a significance level of .05. This means that if a \\(p\\)-value is smaller than .05 we treat this as evidence that the data is incompatible with the null hypothesis. In this case we would say the result is significant. However, as in our case the result is not smaller than .05 the result is not significant (I would avoid saying insignificant if the \\(p\\)-value is larger than .05, as significant is a technical term here). Thus, in the present case we do not reject the null hypothesis. The present data therefore do not provide evidence that the observed difference between the two modes of note taking generalises from the sample to the population according to NHST. There are two further important columns whose results generally need to be reported, df, which stands for degrees of freedom (or df), and F. Understanding these columns in detail is beyond the scope of the present chapter, so we will only introduce them briefly. There are two degrees of freedom reported here, the first value, 1, is the numerator degree of freedom. It is always given by number of conditions minus 1. In the present case, we have two conditions, laptop and longhand, so the numerator df are 2 - 1 = 1. The second value is the denominator df, which are generally given by number of participants minus numerator df minus 1. Here we have 142 participants and therefore 142 - 1 - 1 = 140. In general, the larger the denominator df (i.e., the more participants we have) the better we can detect incompatibility with the null hypothesis (i.e., the easier it is to get small \\(p\\)-values). The \\(F\\)-value is a value expressing the observed incompatibility of the data with the null hypothesis. If \\(F \\leq 1\\), the data are compatible with the null hypothesis. If \\(F &gt; 1\\) the data are to some degree incompatible with the null hypothesis, with larger values indicating more incompatibility. The \\(p\\)-value is calculated from df and \\(F\\)-value. Consequently, the results are usually reported in the following way: \\(F(1, 140) = 0.52\\), \\(p = .471\\). The next column that is important is ges which stands for generalised eta-squared, using the mathematical notation with Greek letters, \\(\\eta^2_G\\). \\(\\eta^2_G\\) is a standardised effect size that tells us something about the absolute magnitude of the observed effect (Olejnik and Algina 2003; Bakeman 2005). More specifically, \\(\\eta^2_G\\) is supposed to be a measure of the proportion of variance in the DV that can be accounted for by a specific factor or IV in the model. For example, in the present case the condition effect is supposed to explain 0.4% of the variance in performance. In general, we should avoid standardised effect sizes such as \\(\\eta^2_G\\) and instead report simple effect sizes. A simple effect size is expressed in units of our measured DV. For example, throughout this chapter we have mentioned that the observed difference in memory performance between both note taking conditions is 2.0 on the scale from 0 to 100. Here, the difference of 2.0 is a simple effect size. We will have to say more about effect sizes later, but as some journal editors or publishing guidelines require standardised effect sizes (which is statistically not a reasonable recommendation in my eyes) the default output contains it. Finally, the default output contains the MSE column, which stands for mean squared errors. This column is mainly included for historical reasons. Traditionally, ANOVA models could relatively easily be calculated by hand or by calculator based on different variance terms (hence the name, analysis of variance). One of this term is the mean squared error from which, in combination with the residual squared error, the \\(F\\)-value can be calculated. In my undergrad studies I still learned to calculate ANOVA by hand, but this seems rather unnecessary nowadays. Hence, we will simply ignore this column. Interested reader can find a detailed explanation about the meaning of MSE for example in Howell (2013) or Baguley (2012). One thing we note in the results table is that it does not contain any information about the intercept. However, as discussed above, the intercept is included in the model. The reason for omitting the intercept from the default output is that it is generally not of primary interest. In experimental research usually the main interest is in the effect of our independent variables, the effect of the experimental manipulation. The statistical model that separates the intercept (i.e., overall mean) from the condition effect allows to zoom in on the relevant part. In line with this, the default output of aov_car does the same. Later chapters will show how we can also get information about the intercept. Estimating a statistical model with aov_car() provides us with the inferential statistical results, the null hypothesis tests for the IV-effects shown above. To get these, we just need to call the object containing the results at the R prompt (e.g., calling res1 in the present case). However, we can use the results object also for others parts of the statistical analyses, for data visualisation and follow-up analyses. 3.4.4 Data Visualisation For data visualisation we can use the afex function afex_plot() which is built on top of the ggplot2 package. afex_plot() requires an estimated model object (e.g., as returned from aov_car()) and specifying which factors of the model we want to plot. In the present case, we only have one factor, condition, so we can only choose this one. Importantly, all factors passed to afex_plot() need to be passed as character strings (i.e., enclosed with \"...\"). afex_plot(res1, &quot;condition&quot;) Figure 3.3: afex_plot() figure for data from Urry et al. (2021) This simple call to afex_plot() produces already a rather good looking results figure combining the individual-level data points (in the background in grey) with the condition means (in black). Individual data points in the background that have the same or very similar values are displaced on the x-axis so they do not lie on top of each other. This is achieved through package ggbeeswarm (which needs to be installed once: install.packages(\"ggbeeswarm\")). The plot also per default shows 95% confidence intervals of the means, which we will explain in detail in a later chapter. As afex_plot() returns a ggplot2 plot object, we can manipulate the plot to make it nicer. p1 &lt;- afex_plot(res1, &quot;condition&quot;) p1 + labs(x = &quot;note taking condition&quot;, y = &quot;memory performance (0 - 100)&quot;) + coord_cartesian(ylim = c(0, 100)) + geom_line(aes(group = 1)) For example, in the code snippet above we first save the plot object as p1 and then call a number of ggplot2 function on this plot object to alter the plot appearance (in ggplot2 graphical elements are added to a plot using +). Function labs() is used to change the axis labels, coord_cartesian() changes the extent of the y-axis (i.e., the plot now show the full possible range of memory performance score), and geom_line(aes(group = 1) adds a line connecting the two means. This figure could now be used in a results report or manuscript as is. 3.4.5 Follow-Up Analysis Follow-up analysis refers to an inspection of the predicted condition means and their relationships. In the case of a single independent variable with two levels (e.g., laptop versus longhand) their is not much to investigate in this regard. We can nevertheless show the general procedure. For follow-up analyses we generally begin with function emmeans() from package emmeans (Lenth 2021). Function emmeans() then returns the estimated marginal means, which is a slightly complicated way of saying condition means, plus additional statistical information. Similarly to afex_plot(), emmeans() requires an estimated model object as well as the specification of a factor in the model for which we want to get the condition means: emmeans(res1, &quot;condition&quot;) #&gt; condition emmean SE df lower.CL upper.CL #&gt; laptop 68.2 1.99 140 64.3 72.1 #&gt; longhand 66.2 1.91 140 62.4 70.0 #&gt; #&gt; Confidence level used: 0.95 For now we only focus on the estimates means in column emmean and ignore the additional inferential statistical information in columns SE to upper.CL. We can see that the reported means match the means given in the text at the very beginning of the chapter, 3.1. The power of emmeans is not only to provide the condition means, but it also allows us to perform calculation on the condition means. For example, in the case of a factor with two levels we can easily calculate the difference between the condition means as our simple effect size. For this, we can save the object returned by emmeans() and then call the pairs() function on this object which gives us all pairwise comparisons of conditions means of which there is only one in the present case (we would get the same results by combining both calls into one: pairs(emmeans(res1, \"condition\"))): em1 &lt;- emmeans(res1, &quot;condition&quot;) pairs(em1) #&gt; contrast estimate SE df t.ratio p.value #&gt; laptop - longhand 1.99 2.76 140 0.722 0.4715 The output shows a mean difference of 1.99 which slightly differs from the 2.0 reported above, which is slightly concerning. However, the results reported above are rounded to one decimal only. If we do so for the present results, we also get an estimated difference of 2.0 (we will not explain this code in detail here): em1 %&gt;% pairs() %&gt;% as.data.frame() %&gt;% format(digits = 1, nsmall = 1) #&gt; contrast estimate SE df t.ratio p.value #&gt; 1 laptop - longhand 2.0 2.8 140.0 0.7 0.5 3.5 Summary The goal of this chapter was to introduce the standard statistical approach for analysing experimental data with one independent variable with two levels  an experiment with two conditions. Practically every time when we run such an experiment, we observe that there is some mean difference in the dependent variable between the two conditions. For our example data by Urry et al. (2021) there was a memory difference of 2.0 points between the two note taking conditions (laptop versus longhand) on the response scale from 0 to 100. The important statistical question we then have is whether there is any evidence suggesting that the observed difference in our sample generalises to the population. The sample are the participants in our experiment and the population refers to all possible participants that could have been sampled. For Urry et al. (2021) this population could be loosely described as students taking notes or maybe more precisely undergraduate students at research intensive (R1) US universities. The question we would like to get a statistical answer to is: Should we believe that there generally is a memory difference between note taking with a laptop versus longhand? To answer this question we need inferential statistics. The inferential statistical approach we are using is called null hypothesis significance testing or NHST. However, NHST does not directly address the question whether there is evidence for a difference in the population. Instead, NHST tests the compatibility of the data with the null hypothesis  the assumption that there is no difference between the condition in the population. The most important result from NHST is the \\(p\\)-value. The \\(p\\)-value is a measure of the compatibility of the data with the null hypothesis; it is the probability of obtaining a results as extreme as observed assuming the null hypothesis is true. If the \\(p\\)-value is smaller than .05 we reject the null hypothesis that there is no difference. In this case we decide that there is evidence for a difference (although this does not follow with logical necessity). To apply NHST to the data we set up a statistical model that observed partitions the data into three parts (Equation (3.1)): the intercept representing the overall mean, the effect of the independent variable (i.e., the difference of the condition means from the intercept), and the residuals representing the idiosyncratic aspects not explained by the other parts of the model. This partitioning allows us to zoom in on the part of the data that we are interested in, the effect of our independent variable, the experimental manipulation. To estimate a statistical model to the data we used function aov_car() from the afex package. aov_car() allows us to specify the statistical model using a formula of the form dv ~ iv + Error(pid) (where pid refers to the variable in the data with the participant identifier) mimicking the mathematical specification of the statistical model. The default output returns an ANOVA table which provides a null hypothesis significance test for our iv, the independent variable. The returned table is called an ANOVA table because statistical models that only contain factors are called analysis of variance or ANOVA. In the present case, the statistical model only has a single factor, note taking condition, with two levels, laptop versus longhand. In the returned ANOVA table, we do not only have the \\(p\\)-value for our experimental factor, but additional inferential statistical information such as the degrees of freedom, df, and the \\(F\\)-value. We can also use the object returned from aov_car() for plotting using function afex_plot(). This function produces a plot combining the individual-level data points with the condition means. This provides a comprehensive display of the data of the experiment. As the function returns a ggplot2 object, this plot can be be easily modified to create a figure that can be used in a results report. We can also use the object returned from aov_car for follow-up analyses using emmeans. With emmeans we can easily obtain the condition means (or estimated marginal means) on the dependent variable. Based on these condition means we can calculate the observed effect size (i.e., the mean difference). Applying the statistical model to the data from Urry et al. (2021) showed a non significant difference, \\(F(1, 140) = 0.52\\), \\(p = .471\\). This suggests that there is no difference in memory performance after watching a talk and taking notes with either a laptop or in longhand format. References "],["case-study-1-more-results-from-note-taking-studies.html", "Chapter 4 Case Study 1: More Results from Note Taking Studies 4.1 Conceptual Memory Data from Urry et al. (2021) 4.2 Why are Experiments Replicated? 4.3 Conceptual Memory Data from Mueller and Oppenheimer (2014) 4.4 Summary", " Chapter 4 Case Study 1: More Results from Note Taking Studies In this chapter we will apply what we have learned in the previous chapter - how to analyse experimental data with one experimental manipulation and two conditions. For this, we will again take a look at the data from Urry et al. (2021). Additionally, we will analyse data from Mueller and Oppenheimer (2014). This study was the first published study investigating the question of note taking with a laptop or in longhand format and was the basis on which Urry et al. (2021) planned their study. For the data of Mueller and Oppenheimer (2014) we will perform a full analysis starting with reading in the data. So in addition to performing the statistical hypothesis test, we will calculate some descriptive statistics. We start the analysis in this chapter in the same way as in the previous chapter, by loading the three packages we generally use, afex, emmeans, and tidyverse, and set a nicer ggplot2 theme. Before doing so it is probably a good idea to restart R (unless, of course, you are just starting R). In RStudio this can be conveniently done through the menu by clicking on Session and then Restart R. In other R environments you might need to restart the program. The benefit of restarting R is that it should create a blank R session in which no packages are loaded and no objects exist in the workspace. Only such a blank sessions ensures that, once we have obtained a set of results, we can recreate them later using the same code. That is, a blank R session avoids any potential problems due to analyses performed in a previous session that are still lingering. Restarting R should generally be done when starting a new analysis or after one is completely done with an analysis. In the latter case, it makes sense to restart R and the rerun all code one has saved in ones script to ensure that all results replicate based on only the code in the script (and do not require some additional code not saved). library(&quot;afex&quot;) library(&quot;emmeans&quot;) library(&quot;tidyverse&quot;) theme_set(theme_bw(base_size = 15) + theme(legend.position=&quot;bottom&quot;, panel.grid.major.x = element_blank())) 4.1 Conceptual Memory Data from Urry et al. (2021) As a quick reminder, Urry et al. (2021) showed their participants short lectures (TED talks) on video during which participants were allowed to take notes. One group of participants, the laptop condition, could take notes on a laptop, whereas the participants in the longhand condition could take notes with pen and paper. After the lecture participants were quizzed on two aspects of the content of the lecture, factual questions and conceptual questions. In the previous chapter we have analysed the overall memory score which was the average of the performance for the factual questions and the conceptual questions. Here, we are only concerned with the memory performance for conceptual questions. We begin our analysis by loading in the data (which is part of afex can be loaded with data()) and getting an overview of the variables using str(): data(&quot;laptop_urry&quot;) str(laptop_urry) #&gt; &#39;data.frame&#39;: 142 obs. of 6 variables: #&gt; $ pid : Factor w/ 142 levels &quot;1&quot;,&quot;2&quot;,&quot;4&quot;,&quot;5&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ condition : Factor w/ 2 levels &quot;laptop&quot;,&quot;longhand&quot;: 1 2 2 1 2 2 1 2 2 1 ... #&gt; $ talk : Factor w/ 5 levels &quot;algorithms&quot;,&quot;ideas&quot;,..: 4 4 2 5 1 3 5 2 5 4 ... #&gt; $ overall : num 65.8 75.8 50 89 75.6 ... #&gt; $ factual : num 61.7 68.3 33.3 85.7 69.2 ... #&gt; $ conceptual: num 70 83.3 66.7 92.3 82.1 ... As before, we have the participants identifier variable in pid and the note taking condition in variable condition. We can also guess that the conceptual memory scores are in the aptly name variable conceptual (if we were unsure about this, we could also check the documentation of the data at ?laptop_urry). Usually, once the data is sufficiently prepared (i.e., we have performed some sanity checks and identified DV and IV), the first step in an analysis should be plotting the data. This could be done using ggplot2 directly. However, in cases such as the present one where it is very clear which statistical model we are going to estimate it is often a bit less effort to plot the data with afex_plot(). Thus, we start by estimating the statistical model for the conceptual memory performance of the data from Urry et al. (2021) and save the estimated model object as mc_urry. For this, we again use aov_car() on the laptopt_urry data and specify the model using the formula interface. The DV we are considering here is conceptual, our IV is condition, and the participant identifier is pid. Consequently, the formula is conceptual ~ condition + Error(pid). Then, before looking at the inferential statistical results, we use this model object to plot the data using afex_plot. mc_urry &lt;- aov_car(conceptual ~ condition + Error(pid), laptop_urry) #&gt; Contrasts set to contr.sum for the following variables: condition afex_plot(mc_urry, &quot;condition&quot;) Figure 4.1: Conceptual memory scores from Urry et al. (2021) across note taking conditions The goal behind beginning with plotting the data is that it allows to see whether the data looks alright. That is, we check whether there are any features that stand out such as clear outliers or an unusual pattern in the data. If this were the case, we would try to figure out if we can find a reason for this issue or how we deal with it. But, as the data looks alright, we continue and consider the results of the significance test: mc_urry #&gt; Anova Table (Type 3 tests) #&gt; #&gt; Response: conceptual #&gt; Effect df MSE F ges p.value #&gt; 1 condition 1, 140 441.76 1.00 .007 .319 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 The ANOVA table reveals that the significance test for the effect of condition is not significant with \\(p = .319\\). Thus, in line with the finding that there is no evidence for a difference in overall memory performance, there also is no evidence for a difference in memory for conceptual information. We can also again use emmeans to see the condition means (or estimated marginal means). In line with Figure 4.1 (as afex_plot internally also uses emmeans it shows exactly the same means in graphical form), the memory score in the laptop condition is descriptively around 3.5 points higher than the score in the longhand condition. emmeans(mc_urry, &quot;condition&quot;) #&gt; condition emmean SE df lower.CL upper.CL #&gt; laptop 73.5 2.55 140 68.5 78.6 #&gt; longhand 70.0 2.44 140 65.2 74.8 #&gt; #&gt; Confidence level used: 0.95 Before moving to the next data set, let us consider how we could report this analysis in a research report. We could for example write: As shown in Figure 4.1, participants conceptual memory scores (on a scale from 0 to 100) are descriptively slightly larger in the laptop condition compared to the longhand condition. We analysed these scores with an ANOVA with one factor, note taking condition, with two levels (laptop vs. longhand). The effect of note taking condition was not significant, \\(F(1, 140) = 1.00\\), \\(p = .319\\). This indicates that the data does not provide evidence for a difference in memory for conceptual information based on how notes are taken during lectures. 4.2 Why are Experiments Replicated? The experiment by Urry et al. (2021) was not the first experiment investigating the effect of note taking during lectures on memory. In contrast, their study was a replication of Mueller and Oppenheimer (2014). A replication is the act of rerunning an existing study to see if one can obtain (or replicate) the results of the previous study. As we have discussed before, inferences from NHST are never conclusive as they are probabilistic and require multiple inferential steps. Replications are one of the most important tools in science for overcoming at least the probabilistic uncertainties associated with the inferences we draw from experimental data. For example consider that several independent but otherwise as similar as possible experiments  that is, replications of the same experiment  all obtain a significant result (i.e., indicate that the data are incompatible with the null hypothesis). Such a pattern would dramatically increase our confidence that the null hypothesis is likely false. In addition to the gain in confidence for specific results, there are good practical reasons for replicating an existing experiment. For example, when beginning to work on a new topic it is generally a good idea to replicate the experiment on which one wants to build on. If one already has problems replicating what exists that shows that the topic is maybe not as simple as portrayed in the literature. Another excellent reason for performing a replication is if one simply does not believe an existing result. Remember, one of the key components of the scientific method is scepticism (at least according to Wikipedia). And if a results is difficult to believe, the reasonable sceptical position to take is to require more evidence. A replication is one way (if not the best way) to produce such evidence. Not believing existing experiments also does not imply that one questions the integrity of the researchers who did the experiment. There are many completely harmless reasons why a study might not replicate. For example, researchers might have just obtained a significant results by chance (which happens in 5% of cases, as discussed in the next chapters). Sadly, replicating existing experiments and publishing the results, is still not the norm in psychology and related disciplines. Quite to the contrary, the situation is so dire that many fields are currently considered to be in a replication crisis. For example, a large scale effort to replicate 100 studies in psychology (Open Science Collaboration 2015) showed that less than 50% could be replicated successfully. Similarly sobering results have since been observed across the social sciences (C. F. Camerer et al. 2018, 2016; Klein et al. 2018). Much has been written about this problem and this is not the right place to rehash all arguments. The best summary of the situation is the book by Chris Chambers (Chambers 2017). The important thing is to realise that science is a cumulative endeavour. Every new experiment builds on existing research. If the existing research has never been replicated, our confidence in this research has to be somewhat low. This questions the foundations of any new work that builds up on this non-replicated research. To move forward we researchers need to replicate work that is important for our research, value replications done by others (especially if it is of our work), and let findings that do not replicate fade into obscurity. 4.3 Conceptual Memory Data from Mueller and Oppenheimer (2014) As Urry et al. (2021) is a direct replication of Mueller and Oppenheimer (2014), the design is the same and uses the same materials (i.e., the same TED talks and same questions). Participants watched short lecture videos (projected onto a screen) and could take notes either on a laptop or with longhand format. 30 minutes after the lecture they were asked factual and conceptual questions about the lectures. Their answers were coded by the first author. As in the previous analysis, we will transform the answers to a memory index from 0 (= no memory) to 100 (= perfect memory). In line with the analysis of Urry et al. (2021) above, we are only interested in the conceptual memory here. The experiment by Urry et al. (2021) was a direct replication of Experiment 1 of Mueller and Oppenheimer (2014). Here we focus on Experiment 2 by Mueller and Oppenheimer (2014), which is also a direct replication of their Experiment 1 and only included an additional experimental manipulation which we will ignore here. The reason for focussing on their Experiment 2 instead of Experiment 1 is that the data of Experiment 2 come out a bit more interesting (feel free to rerun the analysis reported here for their Experiment 1 to see what I mean). However, to not provide an incomplete picture for the research question of whether the mode of taking note during lectures affects memory, we will consider the overall evidence (i.e., all 3 experiments of Mueller and Oppenheimer, the experiment of Urry et al., and further data) at the end of this chapter. Luckily for us, the data from Mueller and Oppenheimer (2014), including the data from their Experiment 2, is available online on the Open Science Framework (OSF). The OSF is one of the most visible developments resulting from the replication crisis. It is a free website that allows researchers to share their data and other materials associated with their research. Before the replication crisis and the OSF it was very rare to get access to the data underlying published studies. Nowadays many researchers depose their (anonymised) data for published studies on the OSF and include the links to the data in their papers. This allows other researcher, such as us, to reanalyse existing data and ensure that the reported results can be reproduced.15 4.3.1 Preparing the Data To get into the habit of downloading data from OSF and reanalysing them, this is what we are going to do now. The file we need is called Study 2 abbreviated data.csv and can be found at the following OSF link: https://osf.io/t43ua/ Please go ahead and download it now and put it in a folder so you can access it. I have already done so and copied it into folder data. We then use the tidyverse function read_csv(), which always returns a tibble (the tidyverse version of a data.frame), to read in the data, as object mo2014. Then we use the glimpse() function (also a tiydverse function) to get an overview of the data (it is very similar to str() but less verbose for tibbles). mo2014 &lt;- read_csv(&quot;data/Study 2 abbreviated data.csv&quot;) glimpse(mo2014) #&gt; Rows: 153 #&gt; Columns: 22 #&gt; $ participantid &lt;dbl&gt; 103, 122, 142, 152, 172, 183, 193, 213, 228, ~ #&gt; $ notetype &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, ~ #&gt; $ whichtalk &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~ #&gt; $ Wcount &lt;dbl&gt; 148, 273, 124, 149, 182, 104, 178, 170, 289, ~ #&gt; $ threeG &lt;dbl&gt; 0.08219178, 0.05535055, 0.18032787, 0.0544217~ #&gt; $ factualindex &lt;dbl&gt; 2.499, 2.833, 2.333, 4.499, 4.999, 1.833, 3.4~ #&gt; $ conceptualindex &lt;dbl&gt; 2.0, 1.5, 2.0, 2.0, 2.0, 1.5, 2.0, 1.5, 2.0, ~ #&gt; $ factualraw &lt;dbl&gt; 6, 7, 5, 11, 12, 4, 7, 11, 12, 11, 7, 6, 8, 5~ #&gt; $ conceptualraw &lt;dbl&gt; 4, 3, 4, 4, 4, 3, 4, 3, 4, 5, 3, 1, 4, 1, 2, ~ #&gt; $ perfectfactindexscore &lt;dbl&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ~ #&gt; $ perfectconceptindexscore &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ~ #&gt; $ perfectfactscore &lt;dbl&gt; 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 1~ #&gt; $ perfectconceptscore &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ~ #&gt; $ `filter_$` &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~ #&gt; $ ZFindexA &lt;dbl&gt; -0.17488644, 0.07568357, -0.29942123, 1.32553~ #&gt; $ ZCindexA &lt;dbl&gt; 0.52136737, 0.03238307, 0.52136737, 0.5213673~ #&gt; $ ZFrawA &lt;dbl&gt; 0.32848714, 0.68152314, -0.02454886, 2.093667~ #&gt; $ ZCrawA &lt;dbl&gt; 0.9199338, 0.2942131, 0.9199338, 0.9199338, 0~ #&gt; $ ZFindexW &lt;dbl&gt; -0.76333431, -0.47725123, -0.90551931, 0.9497~ #&gt; $ ZCindexW &lt;dbl&gt; 0.79471336, 0.06811829, 0.79471336, 0.7947133~ #&gt; $ ZFrawW &lt;dbl&gt; -0.6437995, -0.2476152, -1.0399839, 1.3371221~ #&gt; $ ZCrawW &lt;dbl&gt; 0.9647838, 0.1731663, 0.9647838, 0.9647838, 0~ We can see data from 153 participants on 22 columns. Many of the columns have names that are not immediately clear. This is not uncommon. An important task when getting any new data set is trying to figure out what the variables mean. One usually also has to do this for the data for the own experiments. For example, software for running experiments often collects more variables than needed for analysis. Consequently, the first analysis step is usually to figure out what is needed and what not. Before doing so however, we note that 153 participants is not the final number of participants reported by Mueller and Oppenheimer (2014). Instead, they removed two participants before the analysis resulting. Studying their OSF repository in detail (in particular the published SPPS script with output Output and Syntax - Study 2.doc) shows that participants with number 194 and 237 needs to be removed (the same information can be found in variable filter_$ in the current data set). This file also shows that the two relevant notetype conditions are 1 = longhand and 2 = laptop and we will remove notetype == 3 before analysis. Before moving on, we filter our data and remove these observations. For this we use filter() from the tidyverse in combination with the pipe operator %&gt;% (i.e., we pipe our tibble to filter() and only retain those rows that we want). Importantly, we overwrite mo2014 with the filtered tibble as we do not need the filtered out observations any more (to use them, we woul dhave to read the data in again). We then see how many participants remain using nrow() (which returns the number of rows in the data). mo2014 &lt;- mo2014 %&gt;% filter(notetype != 3, participantid != 194, participantid != 237) nrow(mo2014) #&gt; [1] 99 The reported 99 participants matches the 99 participants reported on OSF for the two conditions, laptop versus longhand. Sadly, the OSF does not include a codebook describing all variables for this particular data set (only for an earlier version of the data set with variables that only overlaps to some degree with the present one). However, from looking at data and the information on OSF a few things are clear: participantid is the participant identifier variable, notetype is the condition identifier coding the experimental condition, and whichtalk identifies the TED talk participants saw (the mapping of talks to numbers is also given in the SPPS output). In a first step, we can transform the relevant indicator variables, participant and experimental condition variable, into factors for further analysis. Transforming a variable into a factor guarantees that none of the analyses incorrectly treats one of the factors (i.e., categorical variables) as a numerical variable (e.g., taking the mean of the numbers in the participant identifier column is not a reasonable statistical operation). However, instead of overwriting the existing variables, we create new variable with the same name as in our analysis of Urry et al. (2021), pid and condition. We also assign human understandable labels instead of using 1 and 2 for the condition codes. This will make it easier to understand the pattern of results. To do so we use factor() inside mutate() from the tidyverse in combination with the pipe operator %&gt;%. mo2014 &lt;- mo2014 %&gt;% filter(notetype != 3) %&gt;% mutate( pid = factor(participantid), condition = factor(notetype, levels = c(2, 1), labels = c(&quot;laptop&quot;, &quot;longhand&quot;)) ) glimpse(mo2014) #&gt; Rows: 99 #&gt; Columns: 24 #&gt; $ participantid &lt;dbl&gt; 103, 122, 142, 152, 172, 183, 193, 213, 228, ~ #&gt; $ notetype &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, ~ #&gt; $ whichtalk &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~ #&gt; $ Wcount &lt;dbl&gt; 148, 273, 124, 149, 182, 104, 178, 170, 289, ~ #&gt; $ threeG &lt;dbl&gt; 0.08219178, 0.05535055, 0.18032787, 0.0544217~ #&gt; $ factualindex &lt;dbl&gt; 2.499, 2.833, 2.333, 4.499, 4.999, 1.833, 3.4~ #&gt; $ conceptualindex &lt;dbl&gt; 2.0, 1.5, 2.0, 2.0, 2.0, 1.5, 2.0, 1.5, 2.0, ~ #&gt; $ factualraw &lt;dbl&gt; 6.0, 7.0, 5.0, 11.0, 12.0, 4.0, 7.0, 11.0, 12~ #&gt; $ conceptualraw &lt;dbl&gt; 4, 3, 4, 4, 4, 3, 4, 3, 4, 5, 3, 1, 4, 1, 2, ~ #&gt; $ perfectfactindexscore &lt;dbl&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ~ #&gt; $ perfectconceptindexscore &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ~ #&gt; $ perfectfactscore &lt;dbl&gt; 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 1~ #&gt; $ perfectconceptscore &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ~ #&gt; $ `filter_$` &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~ #&gt; $ ZFindexA &lt;dbl&gt; -0.17488644, 0.07568357, -0.29942123, 1.32553~ #&gt; $ ZCindexA &lt;dbl&gt; 0.52136737, 0.03238307, 0.52136737, 0.5213673~ #&gt; $ ZFrawA &lt;dbl&gt; 0.32848714, 0.68152314, -0.02454886, 2.093667~ #&gt; $ ZCrawA &lt;dbl&gt; 0.9199338, 0.2942131, 0.9199338, 0.9199338, 0~ #&gt; $ ZFindexW &lt;dbl&gt; -0.76333431, -0.47725123, -0.90551931, 0.9497~ #&gt; $ ZCindexW &lt;dbl&gt; 0.79471336, 0.06811829, 0.79471336, 0.7947133~ #&gt; $ ZFrawW &lt;dbl&gt; -0.6437995, -0.2476152, -1.0399839, 1.3371221~ #&gt; $ ZCrawW &lt;dbl&gt; 0.96478381, 0.17316633, 0.96478381, 0.9647838~ #&gt; $ pid &lt;fct&gt; 103, 122, 142, 152, 172, 183, 193, 213, 228, ~ #&gt; $ condition &lt;fct&gt; longhand, longhand, longhand, longhand, longh~ Looking at the data again reveals that the newly created variables are added to the end of the tibble. The next step is to calculate our dependent variable, the memory scores from 0 to 100 as used in the analysis of Urry et al. (2021). We can see that for the two question types, factual and conceptual, there are multiple measures. Each has an index score and a raw score as well as perfect variants for both types of scores. perfect here presumably means the maximal possible value that could be obtained for this score for this observation (i.e., row). The data also contains a number of \\(z\\)-transformed variants of the scores (variables Z), but we will ignore them here (the original paper used the z-scores, but as these are more difficult to interpret and the results are qualitatively the same, we ignore them here). We focus on the index score which gives participant a maximal of 1 point per question (this information is given in the paper/on OSF). Let us take a look at the first six observations for the relevant variables. mo2014 %&gt;% select(pid, condition, factualindex, conceptualindex, perfectfactindexscore, perfectconceptindexscore) %&gt;% head() #&gt; # A tibble: 6 x 6 #&gt; pid condition factualindex conceptualindex perfectfactinde~ perfectconcepti~ #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 103 longhand 2.50 2 7 3 #&gt; 2 122 longhand 2.83 1.5 7 3 #&gt; 3 142 longhand 2.33 2 7 3 #&gt; 4 152 longhand 4.50 2 7 3 #&gt; 5 172 longhand 5.00 2 7 3 #&gt; 6 183 longhand 1.83 1.5 7 3 We can see that the number of questions per question type and talk differs (as indicated by the difference in perfect indexscore values across rows), but the total number of items appears to always be ten (perfectfactindexscore + perfectconceptindexscore = 10 in each row). This also aligns with the list of items found on OSF which show that there are ten questions per talk with the number of factual and conceptual questions differing across talks. From this information we could calculate our memory scores. However, before moving on it makes sense to run a quick sanity check to see that indeed the number of questions per row is ten. To do this, we create a new variable with the sum of the two perfect index scores, using mutate() (which adds a variable to the existing data), and then see whether this sum is always equal to 10, using summarise() (which in this case reduces the data to one row). If the number of question per observation/row sums to ten, this should return TRUE. mo2014 %&gt;% mutate(sum_p_index = perfectfactindexscore + perfectconceptindexscore) %&gt;% summarise(check = all(sum_p_index == 10)) #&gt; # A tibble: 1 x 1 #&gt; check #&gt; &lt;lgl&gt; #&gt; 1 TRUE Fortunately, the check passes so we feel that our assumption about the meaning of the variables are supported so we could go ahead and calculate our memory score. When preparing data for analysis, or running an analysis, it is important to regularly include such sanity checks in ones analysis. Any analysis involves assumptions about the underlying data  for example, what a variable means, which values a variable can possibly take on, which observations are included in the data. Based on this assumption we calculate other variables from our data and perform our analysis. However, humans are fallible and data analysis experience shows that the assumptions are sometimes (regularly) false. Sometimes one has misunderstood (or misremembers) the meaning of a variable, there might have been some data entry error, or the data still includes some observations that should have been excluded (e.g., test runs from the researchers instead of participants). For example, in a study by Lewandowsky, Gignac, and Oberauer (2013) the age of one participants was recorded as 32,757 years and this error was only uncovered after the publication of the manuscript. Luckily for them the error did not affect the conclusion drawn from the data, but they had to publish a correction (Lewandowsky, Gignac, and Oberauer 2015). Publishing a correction is nothing dramatic (I have a few paper with published corrections because of errors discovered only after publication), but of course we would prefer not having to do so. And if the errors affect the conclusion substantially, sometimes a correction is not enough and a paper has to be retracted. Regular sanity or assumptions checks in ones analysis are on way to minimise the chance of errors in the final analysis. Based on the positive outcome of the sanity check we are now convinced we have understood the variables in the data and can now calculate our memory score from 0 to 100. For this, we divide each index score by the perfectindexscore and then multiply the results by 100. To simplify the coming analysis, we create a new tibble, mo2014a, that only retains the variables we really need for our analysis using select(). We then take another look at the first six rows of the data using head(). This shows that the data is now ready for our reanalysis. mo2014a &lt;- mo2014 %&gt;% mutate( factual = factualindex / perfectfactindexscore * 100, conceptual = conceptualindex / perfectconceptindexscore * 100 ) %&gt;% select(pid, condition, factual, conceptual) head(mo2014a) #&gt; # A tibble: 6 x 4 #&gt; pid condition factual conceptual #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 103 longhand 35.7 66.7 #&gt; 2 122 longhand 40.5 50 #&gt; 3 142 longhand 33.3 66.7 #&gt; 4 152 longhand 64.3 66.7 #&gt; 5 172 longhand 71.4 66.7 #&gt; 6 183 longhand 26.2 50 4.3.2 Descriptive Statistics Before performing an inferential statistical analysis of the data, we obtain some descriptive statistics. This will provide us with an overview over the data. In addition, the descriptive analysis is another way to check our data and minimise the chances of errors or problems. As a first thing, we want to calculate the number of participants per condition. For this, we again use some tidyverse functions and will explain the steps in more detail. We generally start our tidyverse analyses with the data, here our tibble mo2014a, followed by the pipe %&gt;%. The pipe pipes the tibble to the next function. When obtaining descriptives statistics we often want to get them conditional on a factor/categorical variable in our data. For example, now we want to calculate the number of observations per condition. This can be done by piping the tibble to the group_by() function and condition on the variable of interest, condition. The results of this is a grouped tibble which ensures that all following operations on this tibble are performed grouped (i.e., conditioned on) this grouping variable. We can now pipe this grouped tibble to the count() function to get the number of observations per note taking condition. mo2014a %&gt;% group_by(condition) %&gt;% count() #&gt; # A tibble: 2 x 2 #&gt; # Groups: condition [2] #&gt; condition n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 laptop 51 #&gt; 2 longhand 48 As another sanity check, we can compare this number to the values reported on the OSF for this data (the \\(N\\) by condition is not reported in the original paper). As the numbers match, this further increases our confidence in our data preparation. As the next descriptive statistic, we calculate the condition means for our DV of interest, conceptual memory scores. We also calculate the standard deviation to get an idea of the spread of the data. We again use piping and the tidyverse to get the result. But this time the final function in our pipe is summarise() which allows to calculate summary statistics. mo2014a %&gt;% group_by(condition) %&gt;% summarise( mean = mean(conceptual), sd = sd(conceptual) ) #&gt; # A tibble: 2 x 3 #&gt; condition mean sd #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 laptop 35 23.5 #&gt; 2 longhand 46.5 28.9 This shows that the conceptual memory is more than 10 points larger in the longhand compared to the laptop condition. We also see a difference in around 5 points in the SD. As a side note, we could have added another calculation into the summarise() call. For example, n = n() would have also calculated the number of participants per condition, as the previous code did. One important part of a descriptive analysis should always be a plot of the data. A plot of all data points is usually the best way to see if there is something wrong with the data. Above, we have used afex_plot() after having estimated a model with aov_car(), but we can also invoke ggplot2 directly. For this, we also pipe the data to ggplot() and then build the figure layer by layer. The important part is the mapping of variables in the data to aesthetics in the aes() function. We call this directly in the ggplot() call and mimic the other figures we have seen so far, mapping condition on the \\(x\\)-axis and the DV, conceptual memory, to the \\(y\\)-axis. As Figure 3.1, we begin with a violin plot (geom_violin()) with different quantiles. The violin plot shows the shape of the distribution. We combine this with the individual data points, whcih we show using geom_beeswarm() from the ggbeeswarm package (here we call the function without loading the package beforehand by using package::function()). We then add the mean (with standard error, which will be explained later) using stat_summary() in red. In this plot we see that the data already spans the full range in the \\(y\\)-axis, so we do not need to use coord_cartesian(ylim = c(0, 100)). mo2014a %&gt;% ggplot(aes(x = condition, y = conceptual)) + geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) + ggbeeswarm::geom_beeswarm() + stat_summary(colour = &quot;red&quot;) #&gt; No summary function supplied, defaulting to `mean_se()` Figure 4.2: Conceptual memory scores from Mueller and Oppenheimer (2014). This plot combines individual data points in black with means in red. Before looking at the figure, we see that we got a status message in the console, No summary function supplied, defaulting to `mean_se()`. This message is always shown when using stat_summary() without additional argument and can be safely ignored (i.e., it just indicates that the red point shows the mean and the red error bars show the standard error). The plot itself does not show anything unusual. The plot just reinforces the previous descriptive results: The mean memory score, but also the three displayed quantiles, are larger in the longhand than in the laptop condition. Taken together, the descriptive analysis suggests that there is nothing preventing us from running the inferential analysis. 4.3.3 Inferential Analysis The inferential analysis of the conceptual scores uses exactly the same call as our previous analysis, only with a new data set, mo2014a. mc_mo &lt;- aov_car(conceptual ~ condition + Error(pid), mo2014a) #&gt; Contrasts set to contr.sum for the following variables: condition mc_mo #&gt; Anova Table (Type 3 tests) #&gt; #&gt; Response: conceptual #&gt; Effect df MSE F ges p.value #&gt; 1 condition 1, 97 688.89 4.77 * .047 .031 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 Looking at the ANOVA table of the results shows that the \\(p\\)-value is smaller than .05; the analysis reveals a significant effect of condition. This indicates that this data provides evidence against the null hypothesis of no difference between the note taking conditions. Consequently, we would be justified in saying that the data provides evidence for a difference. To make it easy to detect a significant result, afex, like most statistical software tools, indicates a significant effect with \\(p &lt; .05\\) with one * next to the \\(F\\)-value (in case of \\(p &lt; .01\\) the indication is **, in case of \\(p &lt; .001\\) it is ***, and in case the effect is not significant, but \\(p &lt; .1\\), it is +). afex_plot(mc_mo, &quot;condition&quot;) Figure 4.3: afex_plot() figure for the conceptual memory scores from Mueller and Oppenheimer (2014, Experiment 2) that show a significant difference between the two note taking conditions. 4.4 Summary One reality of research is that a significant results is generally what researchers are looking for. If a results is significant we are happy, our experiment has worked and we can publish it. If it is not significant, we generally have problems publishing our results. References "],["references.html", "References", " References "]]
