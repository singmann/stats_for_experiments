[["index.html", "Introduction to Statistics for Experimental Psychology with R Overview 0.1 Acknowledgments 0.2 License and Attribution", " Introduction to Statistics for Experimental Psychology with R Henrik Singmann 2021-09-02 Overview At this point in time, the book is being written so only few chapters are already available. Chapter 1 provides an introduction to the role of statistics in the research process. It also answers the important question: Why do we need statistics? Chapter 2 provides an overview over important concepts and correct terminology we need to describe research designs (e.g., what is a variable? what is the difference between dependent and independent variables?). It also introduces the distinction between experimental and observational studies. Chapter 5 introduces the basic statistical approach. 0.1 Acknowledgments This project would not exist without the help and feedback provided by others: David Kellen, Lukasz Walasek, Stuart Rosen, Anna Krason 0.2 License and Attribution This book is licensed under the Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) license. This license allows you to share and adapt the work as long as you give appropriate attribution and do not use the materials for commercial purposes. Parts of this book uses other materials released under a compatible CC license. Where it does so, the source is clearly indicated. Please ensure to attribute the original source in case you re-use such materials. "],["role-of-statistics-in-the-research-process.html", "Chapter 1 Role of Statistics in the Research Process 1.1 The Research Process 1.2 Example: The Psychology of Loss Aversion 1.3 Epistemic Gaps: The Difference Between What we Want to Know and What we can Know 1.4 Summary 1.5 Conclusion", " Chapter 1 Role of Statistics in the Research Process In this book we are concerned with experimental psychology, in particular the statistical analysis of experiments in psychology and related disciplines such as language science, behavioural science, cognitive science, or neuroscience. The order expressed in the previous sentence  science first and statistical analysis second  is one of the overarching principles with which we will think about statistics. Whereas the goal here is to introduce the concepts and techniques required to perform a statistical analysis of (mostly experimental) data, the perspective taken is that a statistical analysis can only be performed or understood within the scientific context it takes place in. One consequence of this perspective is that the start point of any statistical analysis needs to be a specific and clear research question. In the case in which both data collection and analysis is guided by such a research question, a statistical analysis is generally an indispensable part of the research process. As we will describe in more detail in this and the coming chapters, statistics is the tool that allows us to draw inferences that go beyond the data we have observed. This ability to generalise is what allows us to connect experimental results with the research questions and underlying theories. In sum, the statistical techniques introduced in this book can provide meaningful and scientifically helpful answers when the data is collected and analysed with a clear research question in mind. 1.1 The Research Process One way to describe the research process in psychology and related sciences is in terms of four interrelated steps: the research question, the operationalisation of the research question and the data collection, the statistical analysis of the data, and finally the communication of the results. Let us explain these steps in more detail. Any research should begin with a research question. For the disciplines considered here this is often a theory or hypothesis about human behaviour or the human mind. For example, a widely accepted idea in decision making and behavioural science is that people exhibit loss aversion  the displeasure resulting from losing £10 is stronger than the pleasure derived from winning £10. We will look at this example research question, whether there is evidence for the idea of loss aversion, in more detail in this chapter. What the example already shows is something that is common for research questions  it contains a general statement that involves not directly observable quantities (such as pleasure or displeasure). Not all research questions are as general as the question of whether there is loss aversion. In fact, many research questions are a lot more specific or more applied. For example, in many applied domains an important question is whether a specific intervention  such as a new therapeutical procedure or a workplace training  is better than no intervention or an already existing one. Sometimes, the researcher just wants to find an answer to a specific issue that had come up in a previous study. As a concrete example, consider Hinze and Wiley (2011) that addresses a specific question concerning the testing effect. The testing effect describes the now well established phenomenon that testing of newly learned materials (e.g., self-tests or quizzes) leads to better memory than additional studying (e.g., re-reading) (Roediger and Karpicke 2006). What Hinze and Wiley (2011) noticed was that at that time most research on the testing effect only used limited ways of implementing testing, such as multiple choice tests or open-ended questions. To see whether the phenomenon was more general, their research question was whether the testing effect also occurred for another type of testing, fill-in-the blank tests (i.e., sentences from the learned materials were shown with some words missing that needed to be filled in). The results showed that whereas the fill-in-the blank tests were more effective than mere re-reading (i.e., it showed a testing effect), it was not as effective as testing types that require more involved processing of the materials (e.g., open ended questions). The next step in the research process is the transformation of the research question into an empirical and statistical hypothesis. We will call this step operationalisation. That is, instead of talking about abstract ideas or research questions, we need to concretely decide which study to run. We need to find tasks or other measures (e.g., questionnaires) that allow us to collect data that addresses our research question. An important part of the operationalisation is the specification of relevant variables. We can understand variables as dimensions, features, or characteristics on which individuals or situations can differ. For example, relevant variables for loss aversion are the magnitude of a potential loss or gain and the intensity of resulting pleasure or displeasure (e.g., as measured from a questionnaire). For the testing effect, relevant variables are the type of additional learning (e.g. re-reading versus multiple choice testing) and the final memory performance. As we can see from the example of the testing effect, sometimes it appears difficult to separate the research question from its operationalisation. In this case, the research question is tied to a specific aspect of the operationalisation (i.e., how testing is implemented). However, even with a concrete research question there are always other aspects of the study that require further operationalisation (e.g., what is our measure of learning?). As the example of loss aversion shows, some research questions are very general. Consequently, there are a multitude of possible studies that can be performed to investigate one research question. However, for any specific study a researcher needs to decide on one specific study design. What exactly are the tasks and measures we are using to investigate the question we are interested in? The traditional view of the research process  known as the hypothetico-deductive method  is that during the operationalisation step, researchers derive an empirical prediction that tests their theory. In other words, the theory coupled with the operationalisation predicts a specific outcome (the empirical prediction), that needs to occur if the theory is true. For example, as we will discuss in detail below, loss aversion predicts that people should be unwilling to gamble money if the chance of losing a specific amount of money is equal to the chance of winning the same amount of money (because losing that amount hurts more than winning that amount). According to the traditional view, if this predicted outcome were not to occur, we would learn that the theory is false. Furthermore, if the predicted outcome were to occur, this would not entail that the theory is necessarily true, but it would provide support for the theory. However, as we will see below (Section 1.3.1), in reality we generally learn less than that prescribed by the idealised form of the hypothetico-deductive method. Research practice often diverges from this idealised view of the research process (e.g., Haig 2014). Not in all cases is there one clear prediction or specific theory that is tested. Researchers might compare multiple theories with diverging predictions, only have a vague hypothesis instead of a fully fledged theory from which more than a single prediction follows, or they might simply be curious about what happens in a specific situation. For example, in Hinze and Wiley (2011), the research question was if the fill-in-the blank test would show the same testing effect as other testing procedures. None of the possible results, even the complete absence of a testing effect for this type of testing, would have provided evidence against testing effects in general. The goal of the research was not to confirm or disconfirm the testing effect. Instead, the goal was to test the generality of the testing effect. Consequently, it does not seem appropriate to say operationalisation always involves making specific empirical predictions. However, even in the absence of a specific empirical prediction, the operationalisation must result in an empirical hypothesis relating two or more of the variables that are part of the research design to the research question.1 The final step of the operationalisation is the data collection. Once the research question has been operationalised and the corresponding data is collected, it is time for the statistical analysis. Generally, the statistical analysis answers one specific question: Does the data provide evidence for the empirical hypothesis derived from the research question? The remainder of this book will show in detail how to perform statistical analyses for common study designs and how to interpret the results in light of the research question and operationalisation. Once the data is sufficiently analysed, we have reached the final step of the research process, we need to communicate the results (this step is also known as dissemination). There are different forms of dissemination depending on your goal and audience (e.g., scientific journal article, dissertation report, conference presentation, or a press release). Whereas the different forms differ in the amount of detail and background that is provided, they all need to provide a truthful and comprehensive account of the whole research process: What is the research question? How was it investigated (i.e., describe the operationalisation)? What are the results? What does this mean for the research question? Often, the difficult problem to solve during this step is to provide a comprehensive and truthful account but in a succinct manner. One important tool for doing so is through graphical means  pictures of the results. Consequently, in this book we will discuss both how to present statistical results in a text and how to create appropriate graphs. Whereas this abstract overview leaves out some important things that are also part of research  such as where research questions come from  it shows three important things. The primacy of the research question. The research question determines the operationalisation and thus which data is collected. The research question also determines the statistical analysis, but indirectly; the research question determines the empirical hypothesis which is then tested in the analysis. The statistical analysis is not directly connected to the research question. The statistical analysis is performed on the operationalisation of the research question, but not on the research question itself. What this means is that the statistical analysis itself cannot directly inform us about the research question. In other words, we cannot statistically test the research question. Instead, statistics can only tell us something about a specific operationalisation. Whether or not this allows strong inferences about the research question depends on the operationalisation. And as shown in the following example, an important part of the scientific discourse is to argue whether certain operationalisations allow one to address specific research questions. However, this is generally not a statistical question. Statistics is not the end goal of the research. Instead, the end goal is usually a written communication of the research. In most cases, the statistical analysis is an indispensable part of this communication in that it can provide evidence for or against a specific empirical hypothesis. However, to understand the full meaning and implications of a particular statistical result, it is important to know its context  the research question and its operationalisation. It is the task of the research to communicate this context when communicating the research. Without the context, the impact and meaning of a statistical result is severely limited. 1.2 Example: The Psychology of Loss Aversion To get a better understanding of the research process and the problems that can arise in it, let us consider in detail a concrete example of a research question and how it can be investigated empirically. Specifically, let us return to the example of loss aversion. Loss aversion is one of the assumptions underlying prospect theory (Kahneman and Tversky 1979), a mathematically formalised theory combining cognitive psychology with economic theory.2 The concise description of loss aversion is that losses loom larger than gains (Kahneman and Tversky 1979, 279). As described above, loss aversion means that the negative feeling associated with a loss of a certain amount of money is larger than the positive feeling associated with a gain of the same amount. For example, loss aversion predicts that the displeasure or pain from losing £10 is larger than the pleasure or joy from winning £10. We can see that loss aversion is a theoretical statement involving latent - that is, unobservable - quantities such as negative or positive feelings (i.e., displeasure versus pleasure). We can ask people how they feel, but we cannot easily observe feelings without asking. In psychology, we generally call unobservable theoretical concepts constructs. So how can we test whether people indeed show loss aversion if we cannot directly observe the constructs that form the core of it? One possibility for testing the hypothesis that individuals show loss aversion is hinted at above. We could either give people a certain amount, say £10, or take it away from them, and then ask them how they feel. This procedure runs into at least two problems. First, it is clearly ethically unacceptable to perform an experiment that consists of taking £10 away from our participants. Second, even if we were to overcome the ethical problems (e.g., by first giving participants an endowment and only taking money away from that endowment) there would still be the problem of how to measure the feelings associated with the two events. One way to avoid the ethical problem of taking away money from participants is to ask them to imagine how they would feel if they lost or gained a certain amount of money. Even though it is not certain whether this imagined feeling corresponds to the actual feeling the participants would have when actually losing or gaining this amount, this procedure is commonly used. For example, McGraw et al. (2010) asked their participants to imagine that they would play a single game with a 50% chance of losing $200 and a 50% chance of winning $200 (e.g., flipping a coin and if it comes up heads you win $200; otherwise you lose $200). They then asked participants to imagine how they would feel after either of the two outcomes. They identified two different ways to ask this question, both of which are shown in Figure 1.1 below (McGraw et al. 2010). The first possibility, a bipolar scale, is shown in the upper part. On this response scale participants in the loss condition respond on the left side of the scale and participants in the gain condition respond on the right side of the scale. To compare the ratings, they measured participants responses as the absolute distance of the rating from the neutral point No Effect (i.e., Small Positive Effect would be treated as the same intensity as Small Negative Effect). The second possibility is shown in the lower part of Figure 1.1 and shows a unipolar intensity scale. On this scale, participants in both conditions provide their response on the same scale and only rate the intensity of their displeasure or pleasure. Figure 1.1: Example of two different scales for measuring feelings after a potential gain and loss. The upper part shows a bipolar scale in which losses receive a rating on the left side (of No Effect) and gains receive a rating on the right side. The lower part shows a unipolar scale in which the intensity for both losses and gains are given on the same scale. Image adapted from McGraw et al. (2010). Before reading on, take a moment and ask yourself if you believe the two scales shown in Figure 1.1 make a difference to whether or not participants show loss aversion. Or put differently, can you think of a reason why it matters how we ask for participants feelings after a loss or gain? And if there were a difference, on which scale would you expect it to be more likely that loss aversion occurs? To investigate the question of whether there is loss aversion for both scales, McGraw et al. (2010) asked half of their participants to use the bipolar scale and the other half to use the unipolar scale to rate their feelings after the imagined loss and gain of $200. With this data in hand, they then compared the feeling ratings for imagined losses and wins for each of the two conditions. Their results showed that it indeed matters which scale is used. For the bipolar scale, there was no evidence for loss aversion. The feeling ratings for gains and losses were approximately equal at around 3.4 (where 1 = No Effect and 5 = Very Large Effect). However, when using the unipolar scale, they found evidence for loss aversion. In the loss condition, participants reported a stronger feeling on average (at around 3.6) compared to the gain condition (at around 3.1). McGraw et al. (2010) explain the results in terms of relative versus absolute feeling judgements. For the bipolar scale, people first judge the valence of the feeling  that is, whether it is good or bad  to determine the side on which they have to provide their response. Once this is done, they only then judge the intensity of the feeling. However, as the intensity judgement follows the valence judgement, they make this intensity judgement only by comparing against other feelings of the same valence. More specifically, McGraw et al. (2010) assume that in the loss condition, the loss is only compared against other negative events. Similarly, in the gain condition, the gain is only compared with other positive events. In other words, for the bipolar scale, participants only make a relative judgement of intensity which cannot be compared for loss and gains. Consequently, they argue that the results from bipolar scale are not helpful in answering the question of whether or not there is loss aversion.3 For the unipolar scale, McGraw et al. (2010) argue that the judgement of feeling intensity is not preceded by a valence judgement. Therefore, people use an absolute judgement of feeling, comparing it with both negative and positive feelings. Consequently, this data, which shows a pattern in line with loss aversion, is helpful for the question of whether or not there is loss aversion. Overall they conclude that their study provides evidence for loss aversion, because it appears when using the unipolar scale. What the results of McGraw et al. (2010) show is that seemingly minor differences in the operationalisation of a research question can have a tremendous effect on the results. In line with this, it is perhaps not too surprising that this operationalisation for investigating loss aversion  asking participants about their feelings  is not very common. Below we will discuss alternative operationalisations. And whereas McGraw et al. (2010) provide an explanation for the results, it is an explanation that might have been difficult to come up with before having seen the data. This inability to be able to predict what the effect of the response scale on the results are, has potential consequences that are further reaching. If we take their results seriously and generalise them to other domains, we might conclude that whenever we are interested in participants feelings it matters whether we use bipolar or unipolar scales. One could even go another step further and say that whenever we use subjective rating scales, the type of the scale could have an unintended effect on the results. Because of this, it is often a good idea to try to find an operationalisation of research questions that do not only involve subjective rating scales, but other types of responses such as choices, response times, or more complex behaviour. 1.2.1 Evidence for Loss Aversion: Lotteries A different operationalisation for testing loss aversion is to compare choices across different risky choices, lotteries, or gambles (these terms can be understood interchangeably here), a common experimental paradigm in decision making and behavioural economics. A lottery in this context consists of different options, each of which is associated with one or multiple outcomes, from which the participant has to choose one. The simplest type of lotteries are ones consisting of only one option. In this case, participants can only decide whether to accept or reject the lottery. For example, evidence for loss aversion can be found in the lotteries used by Battalio, Kagel, and Jiranyakul (1990). One of their lotteries (their Question 15) was: Would you play the following gamble: A 50% chance of losing $20 or a 50% chance of winning $20. Participants then had to decide whether to accept or reject the lottery.4 43% of participants accepted this lottery. So how does this result provide evidence for loss aversion? The evidence comes from the fact that participants are more likely to reject the lottery than to accept it (i.e., the acceptance rate is below 50%). What is important about this lottery is that it is a symmetric 50-50 lottery. That is, the magnitude of the potential loss was equal to the magnitude of the potential gain and both possible outcomes appear with equal probability of 50%.5 Remember that loss aversion means disliking a loss more than liking a gain of the same magnitude. A symmetric lottery is exactly such a situation. The fact that participants are more likely to decline to participate in a symmetric lottery is therefore very much in line with loss aversion. One problem with the results of Battalio, Kagel, and Jiranyakul (1990) is that they only collected data from 35 participants which did not provide statistically compelling evidence for loss aversion. Whereas the data shows a descriptive pattern that is in line with loss aversion (i.e., below 50% acceptance of symmetric lotteries) this is not supported by a statistical analysis. More specifically, the statistical analysis does not provide support for the empirical prediction that the observed acceptance rate is below 50%.[a] It does descriptively look like this, but the evidence in the data is not enough to surpass the statistical criterion we use for judging the evidence. A fuller description of how we set a statistical criterion will come later. More compelling evidence for loss aversion comes from a study by Brooks and Zank (2005). In their task, participants were asked to make a decision about more complex lotteries in which participants had to decide between two options. For example, one of the lotteries was the following: Which option do you prefer? A: A 25% chance of +£11, a 50% chance of £0, or a 25% chance of -£11. B: A 25% chance of +£10, a 50% chance of £0, or a 25% chance of -£10. We see that for each of the two options, there are two symmetric outcomes of the same magnitude. For both options A and B, there is a 25% chance of a loss and a 25% chance of a gain of the same magnitude (and with 50% probability participants neither lose nor gain anything). The difference between A and B is that for A, the magnitude of the potential loss and gain is larger by £1. The notion of loss aversion makes an interesting prediction in this case. Participants should increasingly dislike a symmetric lottery the larger the potential outcomes are. For our lottery this means that if we dislike losing £10 more than liking gaining £10, this difference between dislike and liking should be larger if the potential outcomes are £11. In terms of empirical prediction this means that participants should be more willing to choose option B than option A in the lottery above. In the study of Brooks and Zank (2005), each of 49 participants worked on around 50 lotteries that were all similar in structure to the lottery above. That is, for each of the two options there was a loss and gain of the same magnitude with the same probability (the third potential outcome was always smaller in magnitude than the loss/gain). As in the example lottery above, the difference between both options was always that the loss/gain magnitude for one option was £1 larger than for the other option. In line with the empirical prediction of loss aversion, participants chose the option with smaller magnitude of loss/gain in 63% of cases. And given the larger sample size in this study, the statistical analysis also supported the prediction that this 63% was larger than 50%. So does this all show that there is such a thing as loss aversion? Results such as those from Brooks and Zank (2005) certainly appear to support this theoretical idea (see also Camerer 2005). And this also makes intuitive sense. Most people (me included) feel that symmetric lotteries are not really attractive and become increasingly unattractive with increasing magnitude (who really thinks flipping a coin for the chance of losing or winning say £100,000 sounds like a good idea?). However, as in the previous example, the evidence for loss aversion discussed here hinges on a particular operationalisation of the theoretical idea. From these results, we have not really learned that the magnitude of a loss or gain is the psychologically relevant factor in the minds of people. The only thing we have learned is that people dislike certain lotteries or options in lotteries. Can we find an alternative for this data pattern that does not involve loss aversion? 1.2.2 Alternative Explanation: Loss Aversion or Loss Seeking? One clever alternative explanation for why it looks like there is loss aversion was provided by Walasek and Stewart (2015). In their study, participants were also presented with 50-50 lotteries. An example of one of their lotteries is shown in Figure 1.2 below. As shown in the figure, each lottery involved mixed outcomes, that is both gains and losses (i.e., -$18 and +$20 in the example). Furthermore, each lottery consisted of only one option so participants only had to choose whether to accept and play a lottery or not. Finally, both possible outcomes always had a 50% probability of occurring. To make this logic clearer to participants, they were told that accepting the lottery shown in Figure 1.2 was equal to flipping a coin that has -$18 on one side and +$20 on the other side. Depending on which outcome came out on top, their money would change accordingly. If participants rejected a lottery, they neither lost nor won any money. Figure 1.2: Screenshot of lottery task used to investigate loss aversion. This screenshot is from Walasek and Stewart (2015, Figure 1). Before describing the study and its results in detail, let me already lay out the gist of the argument of Walasek and Stewart (2015). Among the lotteries participants saw in their study were some symmetric lotteries in which the potential loss was equal to the potential gain. For example, a -$12/+$12 lottery. If participants accepted most of these lotteries they would on average neither win nor lose money (as potential loss = potential gain). The clever manipulation of Walasek and Stewart (2015) was that across different conditions (i.e., groups of participants) they manipulated how attractive these symmetric lotteries were, relative to the other lotteries a participant saw. In one condition, there were many lotteries for which the potential loss was smaller than the potential gain, such as a -$12/+$20 lottery. That is, if a participant in this condition accepted most lotteries they were likely to win money. In this condition the symmetric lotteries were therefore relatively unattractive. In another condition, there were many lotteries for which the potential loss was larger than the potential gain, such as a -$20/+$12 lottery. That is, if a participant in this condition accepted most lotteries they were likely to lose money. In this condition the symmetric lotteries were therefore relatively attractive. According to the original idea of loss aversion, the only thing that matters for a lottery is the absolute magnitude of its potential gains and losses so such a manipulation of relative attractiveness of symmetric lotteries should not have any effect whatsoever. However, the results of Walasek and Stewart (2015) showed that the manipulation mattered. In the condition in which the symmetric lotteries were relatively unattractive participants only accepted them in 21% of cases, but in the condition in which the lotteries were relatively attractive, participants accepted them in 71% of cases. A statistical analysis supported the empirical prediction that the acceptance rates for symmetric lotteries differed across conditions. In the next section we present the full details of the study of Walasek and Stewart (2015), before coming back to what this means for loss aversion and how it fits within the general theme of this chapter. If decision making research or behavioural economics is not your main interest, these details may not be super relevant to you, so it is okay if you do not understand all of it perfectly. However, because we use this study as an example in the next chapters it would be great to at least give it a cursory read. 1.2.2.1 Full Details of Walasek and Stewart (2015) As in many experiments that fall within the cognitive domain, the study of Walasek and Stewart (2015) consisted of a series of similar trials in which participants had to do the same task (i.e., accept or reject the shown lottery). What differed across trials were the values of the two possible outcomes. For example, in one of the conditions of the experiment losses ranged from -$6 to -$20 in increments of -$2 (resulting in 8 different possible losses) and gains ranged from $12 to $40 in increments of $4 (resulting in 8 different possible gains). Across all trials for one participant in this condition, all possible losses were combined with all possible gains so that in total participants had to decide for \\(8 \\times 8 = 64\\) lotteries whether they accepted or rejected it. Within the 64 trials was a subset of trials that allowed the researchers to directly address the question of whether there is evidence for loss aversion.6 Whereas most of the lotteries shown to participants were asymmetric  that is, the potential loss differed numerically from the potential gain (as in the example of Figure 1.2)  a small subset of lotteries were symmetric. For these lotteries, the amounts for the potential loss were equal to the amount for the potential gain. More specifically, the symmetric lotteries were -$12/+$12, -$16/+$16, and -$20/+$20. For the condition described above in which losses ranged up to -$20 and gains ranged up to +$40, the 191 participants accepted the symmetric lotteries only 21% of the time. 21% is descriptively below 50% indicating that participants indeed disliked these lotteries more than they liked them (i.e., they were overall more likely to reject than to accept the symmetric lotteries). Furthermore, a statistical analysis supported the empirical hypothesis (i.e., provided evidence that this pattern of results generalises beyond the current data). As described above, this result makes sense in light of loss aversion. When losing a certain amount of money is worse than winning the same amount of money, one should reject a symmetric lottery in which one is equally likely to lose or to win a certain amount of money. The clever manipulation of Walasek and Stewart (2015) was that they included three further conditions in which they changed the range of possible outcomes. In addition to the -$20/+$40 condition discussed above, the 202 participants in the $-20/+$20 condition saw lotteries with losses ranging to -$20 and gains also ranging to +$20 only. Another group of 190 participants, the -$40/+$40 condition, saw lotteries with losses ranging to -$40 and gains also ranging to +$40. Finally, Walasek and Stewart (2015) also included a -$40/+$20 condition with 198 participants in which the losses ranged to -$40, but the gains only to +$20 (i.e., the complement to the -$20/+$40 condition). Importantly, in all conditions the number of possible outcomes for losses and gains was 8 (so the step size was either \\(\\pm\\)$2 or \\(\\pm\\)$4). Table 1.1 below shows the possible outcome for each condition Table 1.1: Possible outcomes for the lotteries in Experiment 1 of Walasek and Stewart (2015). In each condition, each participant saw 64 lotteries resulting from combining all possible gains with all possible losses in that condition. Condition -$20/+$40: Gains $12 $16 $20 $24 $28 $32 $36 $40 -$20/+$40: Losses -$6 -$8 -$10 -$12 -$14 -$16 -$18 -$20 -$20/+$20: Gains $6 $8 $10 $12 $14 $16 $18 $20 -$20/+$20: Losses -$6 -$8 -$10 -$12 -$14 -$16 -$18 -$20 -$40/+$40: Gains $12 $16 $20 $24 $28 $32 $36 $40 -$40/+$40: Losses -$12 -$16 -$20 -$24 -$28 -$32 -$36 -$40 -$40/+$20: Gains $6 $8 $10 $12 $14 $16 $18 $20 -$40/+$20: Losses -$12 -$16 -$20 -$24 -$28 -$32 -$36 -$40 As a consequence of this design, what changed across conditions was whether the symmetric lotteries were relatively good or relatively bad. To understand this, we need to look at the remaining asymmetric lotteries. In the -$20/+$40 condition discussed so far, there were more lotteries in which the possible gain was larger than the possible loss (e.g., a -$18/+$20 lottery) than lotteries for which the possible loss was larger than the gain (e.g., a -$20/+$18 lottery). Consequently, the symmetric lotteries were relatively bad (i.e., compared to the many lotteries in which the possible gain is larger than the possible loss). In the -$20/+$20 and -$40/+$40 conditions, the asymmetric lotteries were balanced. In half of the asymmetric lotteries the possible gain was larger than the possible loss, whereas for the other half the possible loss was larger than the possible gain. Consequently, the symmetric lotteries were neither relatively good nor relatively bad. Finally, in the -$40/+$20 condition the pattern was flipped with respect to the -$20/+$40 condition. There were only a few lotteries in which the possible gain was larger than the possible loss compared to the many lotteries for which the possible loss was larger than the gain. Consequently, the symmetric lotteries were relatively good. So does it matter whether the symmetric lotteries are relatively good or not? Indeed it does. As a reminder, people were unlikely to accept the symmetric lotteries in the -$20/+$40 condition in which the symmetric lotteries were relatively bad. Participants in this condition only accepted 21% of the symmetric lotteries. In the -$20/+$20 condition in which the symmetric lotteries were neither relatively good nor relatively bad, participants accepted 50% of the symmetric lotteries. Similarly, in the -$40/+$40 condition participants accepted 42% of the symmetric lotteries. Finally, in the -$40/+$20 condition in which the symmetric lotteries were relatively good, participants accepted 71% of the symmetric lotteries. We can see that 71% is descriptively above 50%, indicating that participants here liked the symmetric lotteries more than they disliked them (i.e., they were overall more likely to accept than reject the lotteries). Furthermore, a statistical analysis supported the empirical hypothesis (i.e., provided evidence that this results pattern generalises beyond the current data). 1.2.2.2 What do the Results Mean for Loss Aversion? What the results of Walasek and Stewart (2015) show, is that the choice pattern for lotteries are not always in line with the idea of loss aversion. Only in a context in which a symmetric lottery is relatively bad do we see evidence in line with the idea of loss aversion. If we are in a context in which a symmetric lottery is relatively good, we see the opposite pattern that one could term loss seeking. What does this mean for loss aversion? The original idea of Kahneman and Tversky (1979) that what matters is the magnitude of a loss or gain surely is not in line with the results of Walasek and Stewart (2015). Instead, Walasek and Stewart (2015) argue that what is relevant to determine the psychological impact of a gain or loss is the relative magnitude or rank of a possible outcome: Compared to other gains or losses I regularly experience, is this a large gain or large loss? Before moving on and linking this example of the psychology of loss aversion to the general goal of this book, let us answer one last question. If what matters is the rank (as suggested by Walasek and Stewart (2015)) and not the magnitude of an outcome (as proposed by Kahneman and Tversky (1979)), why do we see evidence for an effect of magnitude in other studies investigating loss aversion that did not manipulate the context of gains and losses (e.g., Brooks and Zank 2005)? An answer to this question is provided by Stewart, Chater, and Brown (2006). They argue (and also provide empirical evidence) that in our daily lives we experience more small losses (e.g., buying something at the bakery) and more larger gains (e.g., the monthly salary). As a consequence, for a gain and loss with the same magnitude, the relative rank of the gain compared to all other gains is lower than the corresponding relative rank of the loss compared to all other losses. From this difference, we should generally observe a pattern consistent with loss aversion but for a different theoretical reason than proposed by Kahneman and Tversky (1979). 1.3 Epistemic Gaps: The Difference Between What we Want to Know and What we can Know The goal of this chapter is to provide an introduction to the role of statistics in the research process. Why do we need statistics and what can it tell us about the research questions we are interested in? However, so far we have not talked much about statistics. Instead we have introduced an abstract concept of the research process and exemplified it with the example of loss aversion. What this example has tried to show is that even though we can find statistical evidence that supports our empirical hypothesis, this does not mean we have found an answer to our research question. The question of whether or not there is loss aversion is not a statistical question. The answer to the research question depends on whether we can rule out possible alternative explanations, for example, that it is the rank of a potential loss or gain that is important, rather than its magnitude. We nevertheless need statistics to help us determine whether our data provides support for the particular empirical hypothesis that operationalises our research question. Lets put this more bluntly. If you do not yet know about statistics in detail, the application of statistical methods can appear like a magical machine that provides us with an answer to the research question we have. You throw your data in, turn the handle on the statistics machine, and get an answer to your research question out. Sadly, this image of statistics (not to mention needing a handle!) is false. The reason is that there are at least two epistemic gaps in the research process that prevent us from getting a straight answer to our research question. In this section we will introduce these gaps to ensure you can get a realistic image of the role of statistics in the research process. Before explaining what an epistemic gap is, we need to take a step back and think about science in general. There are at least two different domains that constitute a scientific discipline. Firstly, we have the substantive content, what the science is about. For example, psychology is concerned with the human mind and behaviour. So one domain of psychology consists of theories about mind and behaviour (e.g., people exhibit loss aversion). But, just having theories is not enough. The problem is that there are many intuitively plausible but ultimately untrue theories (e.g., the idea that there are distinct learning styles; see Pashler et al. 2008). So the second domain constituting a science is research; systematic investigations of our research questions that provide us with evidence. This evidence allows us to decide which theories to believe and which theories to discard. This perspective on science allows us to draw some conclusions about what this means for us as scientists. Firstly, it is important not to adopt theories too easily and early. As scientists we need to be natural sceptics. Instead of believing in a theory because it sounds compelling, we need to ask for the evidence first. We then need to evaluate the evidence and use this as the basis for the degree of our belief. For example, if there are a few independent studies that support a certain theoretical position (as in the case of loss aversion), it seems appropriate to consider a certain theoretical position as a possibility but also to consider alternative accounts (for example, the rank based account of Walasek and Stewart 2015). If the evidence is weaker, say only studies from the proponents of a theory, we should be even more cautious in the degree of belief we assign to a theory. And only if the scientific evidence is overwhelming should we be willing to treat a theory as approximately true. For psychology, there are not many theories for which the majority of researchers would agree that the latter criterion is reached (besides maybe operant and classical conditioning). As a consequence, in many cases, the best thing to do is to be honest and admit that the evidence is not sufficient to hold a strong theoretical position. For us scientists, this perspective also means that we need to be able to evaluate the evidence from the research in our area of interest. Doing so requires substantive knowledge about our research domain, but also the statistical knowledge that we will introduce in this book. Finally, it means that we need to be open to revising our beliefs in light of new evidence (as in the case of loss aversion and the results of Walasek and Stewart 2015). As the last two paragraphs show, science is ultimately about evidence. What evidence do we have to support the theories in our field? As scientists we hope that statistics is a tool that helps us answer the question about evidence. And to some degree it does, but not as much as we would hope. We need to be aware of the epistemic gaps. Let us explain now what this means. Epistemology is a branch of philosophy that is concerned with knowledge (e.g., what is knowledge, how do we know that we know) and justification (e.g., what is the reason I believe something) (for a philosophical overview see: Steup and Neta 2020). Epistemic is the corresponding adjective. Epistemology therefore, is the field that deals with philosophical questions at the core of science, for example, which theories should I believe based on the available evidence. An epistemic gap describes the difference between what we want to know and what we can actually know.7 Ideally, we want to know whether a theory or hypothesis is true. As we have already seen in the example above, in many cases even carefully designed experiments cannot unambiguously answer this question (e.g., before Walasek and Stewart (2015) the available evidence appeared to support loss aversion, but now we are not so sure about it). In the following, we will take a more abstract look at this question and discuss two general problems that further complicate this issue. What we can know is often quite different from what we would like to know. A competent application of statistics requires that one is aware of this problem and avoids over-interpreting the results from ones research. 1.3.1 Epistemic Gap 1: Underdetermination of Theory by Data Above we have provided an example of a theoretical (or basic) research question (Is there evidence for loss aversion?). We have seen that answering the research question requires careful thinking about the operationalisation; how exactly should we set up a study to test this question? And once we have decided on one operationalisation, we have seen that we can find alternative explanations that can explain the results without making the assumptions of the original research question. In other words, even though the operationalisation was carefully chosen, it could not unambiguously answer our research question. The fact that we could not compellingly answer the research questions despite employing a carefully chosen operationalisation is not a problem that is unique to our example. In contrast, an important insight from philosophy of science is that this is a problem of any empirical study. This issue is also known as underdetermination of theory by data or the Duhem-Quine thesis and always occurs if there is a difference between the research question and the corresponding operationalisation. And as we have seen in the examples, there essentially always is. There are different aspects of underdetermination. The first has to do with the specification of the research question and its operationalisation. Research questions usually involve unobservable constructs  such as emotions (e.g., fear), memory, attention, comprehension, or learning  or vague phrases, such as works better or improves. An empirical investigation of such questions however requires a precise specification and operationalisation. By going from the research question to the concrete operationalisation, there is no guarantee that the operationalisation captures the intended meaning of the research question. For example, consider a test of a new therapeutic intervention compared with a standard one. Imagine we found that the new intervention decreased the self-reported discomfort of symptoms on a patient questionnaire more strongly than the old treatment, but did not reduce the number of sick days due to the disorder. Should we interpret this to mean that the new treatment works better than the old one? In some sense it does, but in another it does not. The problem is that the nuances that result from operationalising a research question concretely do not always align with the broad way in which we like to think about research questions. At this point you might think this does not yet sound like such a big problem. We just need to define our research questions precisely enough and then we would be able to learn something about our research question. Sadly, this is easier said than done. The first problem is that it is often impossible to precisely define our research question, because we have not yet found a way to precisely define the constructs that are involved in it (this is known as the problem of coordination, (Kellen et al. 2021)). And in most cases, precisely defining these abstract constructs is not possible anyway. For example, if you have the hypothesis that a specific emotion, say fear, is related to some behavioural pattern, say aggression, you run into the problem that there is not a generally agreed upon definition of either of these constructs. There probably exist questionnaires for measuring fearfulness and aggressive tendencies, but these questionnaires do not represent the corresponding constructs or a definition of them. If you were to ask a sample of participants to fill out these questionnaires and found that the scores of the participants in these two questionnaires are related, it would not allow you to conclude that fearfulness and aggression are related. The only conclusion that would be allowed is that fearfulness as measured with one questionnaire is related to aggression as measured with another questionnaire. Of course, as scientists we would like to make the general conclusion that the constructs are related, but such an inference does not logically follow. The general problem that we run into is the Duhem-Quine thesis: any empirical hypothesis that is tested in a study has two parts: The theoretical prediction as well as a set of auxiliary assumptions that links the theoretical prediction with the data. To stay with our example, our theoretical prediction could be that fear and aggression are related. The auxiliary assumptions are all those additional assumptions that are needed to test this question empirically as decided on as part of the operationalisation: that the questionnaire is a valid measure of the constructs (which is a big assumption), that the data collection took place without any unforeseen problems, that we have tested enough participants to find an effect, that we use appropriate statistical procedures, etc. As can be seen, the list of auxiliary assumptions is somewhat limitless and difficult to enumerate fully. It also contains quite mundane assumptions such that we have to assume that the research actually took place and is not just made up by the researcher (for an exception, see the case of Diederik Stapel). The core of the Duhem-Quine thesis is that any empirical result cannot pertain solely to the theoretical prediction of interest, but the union (or conjunction) of the theoretical prediction of interest with the auxiliary assumptions. If the results are in line with the empirical hypothesis, that only supports the theoretical prediction if all auxiliary assumptions are true. Likewise, if the results are not in line with the empirical hypothesis, this only provides evidence against the theoretical prediction if all auxiliary assumptions are true. However, testing whether all auxiliary assumptions are true cannot be done in the same study that tests the empirical hypothesis we set out to test (because we can always come up with more and more auxiliary assumptions not specifically tested). Consequently, any individual result on its own cannot provide conclusive evidence for or against a particular theoretical prediction. There can always be an alternative explanation that differs from the theory or hypothesis one has.8 That is what is meant by the underdetermination of theory by data. Whereas this issue might seem like a purely philosophical discussion, it is far from it. Most actual scientific discussions in the literature are about the auxiliary assumptions that are part of the operationalisation of a research question. For example, the argument for loss aversion as proposed by Kahneman and Tversky (1979) hinges on the auxiliary assumption that participants interpret the possible outcomes of the lotteries in terms of their magnitude or absolute value. As shown by Walasek and Stewart (2015), this auxiliary assumption does not appear to hold at least in some cases and participants instead interpret the relative value of the possible outcomes of the lotteries. It will be easy to find similar examples for the research area you are interested in. To sum up, the problem of underdetermination and the first epistemic gap is that any particular result never uniquely supports or challenges one theoretical position or hypothesis. For any result that appears to support a theory there is another theory that makes the same prediction because an auxiliary hypothesis could be false and thus require a different theory. Likewise, for any result that seems to disagree with a theory, the theory can always be protected by claiming one of the auxiliary assumptions is incorrect. And this is also exactly what happens in real scientific discourse. As an example, when John Bargh, a prominent social psychologist from Yale, was confronted with results that disagreed with one of his most prominent findings (Doyen et al. 2012) he attacked (in a now deleted blog post that still can be found here) the incompetent or ill-informed researchers and claimed their study had many important differences from our procedure, all of which worked to eliminate the effect. As this section has shown, questioning the methods (i.e., the auxiliary assumptions) is a legitimate defence that protects ones theory. Of course, one can question the auxiliary assumptions of the original results that appeared to support the theory in the same way. In the case of Bargh, it appears that this is exactly what happened. Most other psychologists have stopped believing his original finding (e.g., Harris, Rohrer, and Pashler 2021). 1.3.2 Epistemic Gap 2: Signal versus Noise The first epistemic gap is that there is no strong logical link between the theories underlying our research questions and the operationalisation of the research question. Thus, in terms of the steps in our research process it concerns the relationship between step 1, the research question, and step 2, the operationalisation and data collection. The next epistemic gap concerns the relationship of steps 2 and step 3, the statistical analysis. As described above (1.1), the important task during the operationalisation is to transform the research questions into an empirical hypothesis. Ideally, this hypothesis comes in the form of an empirical prediction, describing which possible outcome would support our theoretical hypothesis (i.e., the outcome predicted by our theory). As part of this we should also clearly designate a possible outcome that, if it were to occur, would speak against our theoretical hypothesis. Once we have decided on this, we collect the data and then run the statistical analysis. The goal is that statistical analysis provides us with evidence with respect to the empirical hypothesis. Does the data support the empirical hypothesis or not? Before providing an overview of how this is done, let us go back to the example above, the study of Walasek and Stewart (2015). In contrast to the original formulation of loss aversion (Kahneman and Tversky 1979), which is based on the magnitude or absolute value of a gain or loss, the theoretical prediction of Walasek and Stewart (2015) was that what drives peoples behaviour is the relative value of a gain or loss. To test this, they presented participants with lotteries in different conditions in which the range of gains and losses differed. In one condition there were small losses and large gains and in another condition there were large losses and small gains (we ignore the other two conditions here). The hypothesis that follows from this design is that the relative attractiveness of the symmetric lotteries (in which magnitude of possible loss = magnitude of possible gain) differs in both conditions. In the small losses/large gains condition the symmetric lottery is relatively unattractive and in the large losses/small gains condition it is relatively attractive. The resulting empirical prediction is that participants should be less willing to accept the symmetric lotteries in the small losses/large gains condition than in the large losses/small gains condition. In line with this prediction, participants in the small losses/large gains condition accepted only 21% of the symmetric lotteries whereas participants in the large losses/small gains condition accepted 72% of the symmetric lotteries. From just looking at the bare numbers, the results appear to support the empirical prediction. Participants are roughly 50 percentage points less likely to accept the symmetric lotteries in the small loss/large gains condition than in the large loss/small gains condition. However, how can we be sure this particular difference is not a chance occurrence? Maybe we just got unlucky and the participants in the small loss/large gains condition are for some reason generally less likely to accept any lotteries than the participants in the large loss/small gains condition. Maybe the former participants all had a horrible night of sleep and are in a really bad mood at the time of testing and therefore reject all gambles whereas this was not the case for the participants in the latter condition. If this were the case, the observed difference would not actually tell us anything about our research questions. The problem described in the previous paragraph is at the heart of the statistical approach described in this book. The core problem is that the responses we get from participants in experiments are inherently noisy. Human participants can do things for any number of reasons. Some of these reasons are related to our research question and its operationalisation and others are not. For example, if participants read the lotteries carefully and think before they provide an answer, it is likely that the values of the possible outcome play a role in their answer. In this case, their responses are relevant for our research question. But what if participants are distracted by a message on their phone and do not read the problem fully? Or what if they intend to accept a lottery and accidentally reject it (i.e., press the wrong button)? We can also imagine that it matters if participants are relatively rich or relatively poor. For someone with a million in the bank, it might not really matter if they lose or win $16 so they might be inherently more likely to gamble on such a lottery than someone for whom this is more than the hourly wage. In all these cases, the values of the lotteries have a minor effect and thus the responses are more or less irrelevant for our research question. For the question of whether or not the results support our empirical hypothesis, we therefore would like to distinguish between those responses that are relevant for our research question  we can call this the signal  and those responses that are generated more or less randomly and are irrelevant for our research question  we can call this the noise. If we had a procedure that could distinguish signal from noise we could then simply see whether the signal supports our empirical prediction. If it did, the data would provide support for the hypothesis and if not, the data would not support the empirical hypothesis. Sadly, such a procedure does not and cannot exist (as we would then know why people do what they do, which is the reason we do research in the first place). In the absence of a procedure that can definitely separate the contribution of signal and noise, the statistical approach introduced here compares an estimate of the signal with an estimate of the noise. Let us assume for a moment the estimated signal supports the empirical prediction in our example (i.e., we predict that participants are less likely to accept a lottery in the small loss/large gains condition and this is what the data shows). We then compare this estimated signal against the estimated noise. If the estimated signal is large given the estimated level of the noise, we assume that the data supports the empirical prediction. If the estimated signal is not large given the estimated noise, we assume the data does not support our empirical prediction. So how can we estimate the signal and the noise? Estimating the signal is straight forward. We just use the observed difference between the conditions as our estimate of the signal. So for the example from Walasek and Stewart (2015), this would be the observed difference in accepting the symmetric lotteries between the two conditions (roughly 50 percentage points). Estimating the noise is a bit more complicated and will be described in detail in later chapters. For now it is enough to understand that it is affected by two components: (1) The variability in responses within each condition and (2) the overall sample size (i.e., number of participants). If the variability within each condition becomes smaller (i.e., measurement becomes more precise) and the sample size stays the same, the levels of noise decrease. Likewise, if the sample size increases with a constant level of variability, the level of noise decreases. Another important question is what counts as large when comparing the estimated signal to the estimated noise. In the following chapters we will introduce a decision threshold for the signal to noise ratio to make this judgement.9 If the signal to noise ratio is above the threshold, we act as if there were a signal and change our beliefs. If it is not, we cannot make a decision. This decision threshold is chosen such that across many such decisions we control the rate of making false positive decisions (a false positive occurs when we say there is a signal when there is none). In particular, the decision threshold is chosen such that across many situations in which there is no signal, we only incorrectly assume that there is a signal in 5% of decisions. Taken together, the statistical procedures we use attempt to answer the question whether there is a signal that supports the empirical hypothesis given that human data is inherently random and noisy. The problem in this is that the estimated signal  the observed difference between the conditions  is also affected by the noise. We never know if the observed difference is due to the signal we are interested in or just based on noise. To overcome this problem we compare the observed signal with the observed level of noise. If the observed level is large relative to the observed noise, we decide the data supports the idea that there is a genuine signal present . In other words, we never really know if the current data genuinely supports our prediction or not; we just act as if it does. There always is a chance that the effect is due to the noise. Because we only have this one data set we are analysing, we cannot be 100% certain our estimate of the signal and the noise is fully accurate. However, as we will describe in detail later, across decisions that use this statistical decision procedure, it controls our rate of making false positive decisions. As in the case of the first epistemic gap, the second epistemic gap also shows that we cannot unambiguously learn what we wanted to know  whether the data supports the empirical hypothesis or not. If the signal is large relative to the noise, we have evidence that it does, but this evidence is never fully conclusive. The evidence might be strong, and we will later see how we can identify that, but there always should be some remaining doubt in the back of our head. Maybe we just got unlucky and the participants in our study responded in a way that made it look like there was a signal, but there wasnt. With just one data set in hand, this cannot be ruled out. 1.4 Summary In this chapter we have provided a conceptual overview of the research process in psychology and related disciplines. In this concept, the research always begins with a research question. What is it that I want to know? Often this research question stems from a particular theory that we want to test, but it can also be a purely applied hypothesis. The next important step is the operationalisation of this question followed by the data collection. That means we need to find appropriate tasks or measures and develop a study design with which we can test the empirical hypothesis following from our research question. As we have seen in the discussion of the first epistemic gap, the consequence of the separation of research question and operationalisation is that, strictly speaking, our study only lets us learn more about the tasks and measures we are using. Because of the problem of underdetermination of theory by data, even an apparently positive result does not allow us to infer that it supports our theoretical hypothesis. The core problem is that the empirical hypothesis is a combination of a theoretical hypothesis and auxiliary assumptions and we cannot rule out that one of the auxiliary assumptions is false. With the collected data in hand, the next step is to perform the statistical analysis. Here, we hope to find evidence that informs us about our empirical hypothesis. The procedure we will use in this book attempts to distinguish between the signal in the data, the part of the data relevant to our empirical hypothesis, and the noise, the randomness that is inherent in using human participants.10 However, the second epistemic gap entails that even with such a statistical procedure, we cannot find fully conclusive evidence. The problem is that we cannot estimate the true amount of noise in the data. Research participants have a myriad of potential reasons for why they show a certain behaviour and these reasons need not be related to our research question. With more precise measures and more participants, we can control this level of noise to some degree, but ultimately cannot be sure whether we did not just get unlucky and what we see is due to noise and not because of our hypothesis. The final step in the research process is the communication of our results. This step essentially combines all previous steps. We need to communicate the research question, the operationalisation, the data collection process, and the results from the statistical analysis. Whereas the communication of the results is ultimately the goal of any research project, it is also the step during which we have to be mindful of the limits of our research. The biggest danger is that we forget the epistemic gaps that are inherent in any empirical research and oversell our results. We of course want that our research allows us to answer our (potentially big and broad) research questions, but we should be honest with ourselves and our audience and stick to the reality that we are primarily learning something about our operationalisation. When we get a statistical result that appears to support our empirical prediction, we want to treat it as if it is true, but we should be clear that there always is a chance that our result might be a fluke. 1.4.1 Some Further Examples and Passing Thoughts Let us end this chapter with another concrete example from the literature that highlights some of the issues discussed here and ties it together with some thoughts on how to communicate research. To motivate ones research question, it is a good idea to start with the big picture. What are the real world issues and theoretical problems we want to address? Whereas this is a good idea, we should not mistake our operationalisation of the research question with this big picture. Surely, if we use a particular task  such as symmetric lotteries for investigating loss aversion  we learn something about the underlying research question and theory? We surely do, the question that is difficult to answer is exactly what we learn. As a final example to illustrate this problem, let us consider the research on risk preferences in decision making. The idea of risk preferences is that some people might be more willing to take risks (e.g., when gambling or when choosing an investment) than others. There exist a number of different tasks to investigate risk preferences experimentally, such as the balloon analogue risk task [BART; Lejuez et al. (2002)] or the Columbia card task (Figner et al. 2009), as well as a number of different questionnaires. A large study with around 1500 participants who each performed eight different tasks and filled out twelve different questionnaires designed to measure risk preferences (Pedroni et al. 2017; Frey et al. 2017) could show that participants behaviour across tasks and questionnaires was surprisingly unrelated. Whereas participants who scored high on one questionnaire also scored high on other questionnaires (i.e., the different questionnaires shared a common risk trait), the scores on the questionnaires were largely unrelated to the behaviours in the different tasks. Furthermore, behaviours across the different risk tasks were unrelated to each other (i.e., a participant who was specifically risky in one task was not particularly risky in another task). In other words, even though the tasks and questionnaires all appear to measure risk preferences, their failure to find a consistent pattern across participants suggests they fail to do so in a coherent manner. One might wonder if the fact that the questionnaires were related to each other represents some sort of silver lining. I would not share this interpretation and instead attribute this to common-method variance. The important result is that the questionnaires were also unrelated to the behaviour in the tasks. This tells me that at this point in time, we do not really understand what risk preferences are, how to measure them, or if they exist at all in the way they are conceptualised. To sum this up, it is important to keep in mind that the thing we learn something about in our research is primarily our operationalisation. If we want to make a case that we also learn something about the underlying research question, we have to make a good case for this and spell out which alternative explanations we rule out and which auxiliary assumptions we can take for granted. This usually requires considering other results than our own study. In short, do not confuse the task or measures with the theory or research question. When communicating statistical results, we also need to avoid overselling the results. As a general principle, we should report the results in a humble manner. To this end, we should avoid language that suggests a level of confidence that we cannot provide. This means, statistical results never prove or confirm our empirical hypothesis. Instead, they may support it or suggest certain interpretations. 1.5 Conclusion As scientists, we aim to answer interesting and important questions about the domain we are studying. The problem is that research cannot address the research questions directly. Instead, our research only addresses the operationalisation of the research question. Drawing inferences about the research question requires accepting a number of usually untested auxiliary assumptions. And even if the data appears to support our hypothesis and we are willing to accept the auxiliary assumptions, there is always the risk that we just got unlucky and interpret noise as if it were a signal. As a consequence, any individual study can only provide little evidence, especially for large and general research questions. Even worse, sometimes we only learn from our research that the chosen operationalisation is unable to answer our research question. In sum, definitive answers to our research questions need more than one study. Whereas this paints a less optimistic picture on what we can learn from our research than one would hope, it is important to stay realistic and humble. Many ideas, especially if they are our own, appear intuitive and compelling and we feel they must be right. But as scientists we need to stay sceptical and avoid the urge to believe in theories before we have overwhelming evidence that conclusively rules out possible alternative explanations (even those we havent yet thought about). The one thing that distinguishes science from non-scientific belief systems is that science is in principle based on solid evidence. And the overall strength of the evidence provided by our research depends on the whole research process of which statistics is one part. References "],["chapter-1-quiz.html", "Chapter 1: Quiz", " Chapter 1: Quiz Note: The pull-down menu for selecting an answer turns green after selecting the correct answer. Exercise 1.1 The start point for a statistical analysis should be? A data set with many variables that allows testing many different hypotheses A clearly specified research question The believe that my hypothesis is probably true Answer: 1 2 3 Exercise 1.2 Which of the following is NOT one of the steps in the research process? Research Question Operationalisation and data collection Statistical analysis Responding to emails Communication of the results Answer: 1 2 3 4 5 Exercise 1.3 What is NOT part of the operationalisation? Hypothetico-deductive method Specifying which tasks or questionnaires will be used Transformation of a research question into empirical and statistical hypothesis Deciding for which variables data will be collected Answer: 1 2 3 4 Exercise 1.4 What is the main reason why we need statistics? To prove/confirm a theory To test whether the data we collected provide evidence for the empirical hypothesis To publish papers in high-impact journals Answer: 1 2 3 Exercise 1.5 What is an epistemic gap? The difference between what we want to know and what we can know A specific outcome predicted by a theory A mathematically formalized cognitive theory Answer: 1 2 3 Exercise 1.6 Underdetermination of theory by data suggests that: Every specific hypothesis or theory is false or will eventually shown to be false We need hypotheses to answer research questions No single data sets unambiguously supports a specific hypothesis or theory Answer: 1 2 3 Exercise 1.7 When can a researcher assume that the data support the empirical prediction? When the data descriptively supports the empirical prediction When there are no epistemic gaps When the estimated signal is large given the estimated level of noise When the estimated level of noise is smaller than a pre-specified threshold Answer: 1 2 3 4 Exercise 1.8 What can we conclude if a statistical analysis provides strong support for our empirical hypothesis? Our research question is true or probably true Our empirical hypothesis is true or probably true It is unlikely that an alternative explanation could explain the results None of the above Answer: 1 2 3 4 "],["research-designs.html", "Chapter 2 Data and Research Designs 2.1 Empirical Evidence and Data 2.2 Data Types 2.3 Measurement 2.4 Independent and Dependent Variables 2.5 Experimental versus Observational Variables 2.6 Summary", " Chapter 2 Data and Research Designs In the previous chapter (Chapter 1) we have provided an overview of the research process in psychology and related disciplines. The goal of this overview was to show that statistics is just one part of the full research endeavour and usually not the end goal. We also highlighted that an answer to our research questions requires not only the results from the statistical analysis, but the context in which this result was generated. More specifically, we argued that the most important part of the research process is usually the operationalisation of the research question: What are our measures? What is the task participants have to do? What is our study design? The goal of this chapter is to provide us with the necessary conceptual knowledge and the right terminology to answer these questions for our research. 2.1 Empirical Evidence and Data In this book we are concerned with empirical research. What this means is that we are generally not interested in research questions for which an answer or proof can be found purely through thinking hard, such as in mathematics or philosophy. Instead, we are only interested in research question for which the evidence comes in the form of observations or experiences, empirical evidence for short. As we have discussed in the previous chapter, that does not mean that our theories cannot include unobservable quantities such as mental states (e.g., fear, enjoyment, attention). However, if our theories include such unobservable quantities, these must be causally responsible for something that is observable (e.g., behaviour). This way, we can still test the theories (e.g., if our theory predicts that fear should lead to aggression, but we can induce fear without it leading to aggression, we learn that our theory must be wrong). The fact that we are interested in empirical research means that the ultimate arbiter of whether or not we should believe in a theory is empirical evidence. It does not matter how elegant or intuitive a theory is. If the observed behaviour of people disagrees with a theory, it is wrong. It also means that theories that are so vague that there is no possible empirical evidence that would disprove them are not part of the empirical sciences (i.e., they are not empirical theories). This criterion is also known as falsifiability and was introduced by the philosopher Karl Popper in the 1930s. For example, there is a never ending discussion of whether Freudian psychoanalysis is in principle falsifiable or not. Whereas Karl Popper was very strong in his belief that it is not (which would render psychoanalysis non-scientific), proponents of Freudian psychoanalysis naturally see this rather differently.11 Empirical evidence comes in at least two different forms, either as anecdotes or as data. Whereas an anecdote typically refers to a single person, data usually contains information about multiple persons. However, anecdotes and data differ on more dimensions than just the number of observations, as summarised in the following aphorism: The plural of anecdote is not data. Anecdotes are unsystematic observation, typically in the form of stories (e.g., the friend of a friend), that somehow address our research questions. The problem with anecdotes is that they are generally difficult to verify and to investigate further. This makes it impossible to rule out possible alternative explanations for the relationship between the anecdote and the research question. And as we have seen in the previous chapter, one of the main criteria for deciding whether an observation provides evidence for a theoretical claim is whether we can rule out plausible alternative explanations. In sum, anecdotes surely matter when coming up with good hypotheses or ideas what to study, but for mature sciences anecdotes should only play a minor evidentiary role in deciding which claims to believe. Data are systematic observations that are collected for a specific purpose, such as answering a research question or bookkeeping. Data generally consists of observations on multiple variables. In the previous chapter we have defined variables as dimensions, features, or characteristics on which individuals or situations can differ. A more technical definition is that each variable corresponds to a specific set of possible outcomes (or states of affair/events), where each possible outcome corresponds to one value of the variable. Furthermore, we can define an observation as the smallest unit of data. More technically, one observation results from collecting at least one value of one variable or values on different variables from one unit of observation (in psychology, the unit of observation is usually the participant). As an example of data, consider again the study by Walasek and Stewart (2015) discussed in the previous chapter (1). The task of participants was to accept or reject 50-50 lotteries (for an example, see Figure 1.2) and each participant had to do this for 64 trials. Table 2.1 below shows six observations each from two different participants from this study. The way the data is shown here is exactly the format that Lukasz Walasek used to analyse the data (i.e., no variables added or removed). Observations are shown in rows and variables are shown as columns. This tabular representation of the data with observations in rows and variables in columns is common and will be used throughout the book. Table 2.1: Selected observations of the data from Walasek &amp; Stewart (2015, Exp. 1a). The [] indicates that this is only part of the whole data set and some observations are not shown. subno loss gain response condition resp 8 6 6 accept 20.2 1 8 6 8 accept 20.2 1 8 6 10 accept 20.2 1 8 6 12 accept 20.2 1 8 6 14 accept 20.2 1 8 6 16 accept 20.2 1 [] 369 6 12 reject 40.2 0 369 6 16 reject 40.2 0 369 6 20 reject 40.2 0 369 6 24 accept 40.2 1 369 6 28 accept 40.2 1 369 6 32 accept 40.2 1 In total we can see six different variables in this data set. Let us discuss these in turn. The first variable, subno (we generally use a monospace font to refer to variable names as they appear in a data set), is the participant identifier or subject number (because actual individuals take part in research and not passive subjects, the term participant is now preferred to subject). This variable should be part of any data set to uniquely identify to which participant (or more generally, unit of observation) a specific observation belongs. Here, we see that it only takes numbers. It is not uncommon to only use numbers for the participant identifier variable, but it can also be a combination of numbers and letters, or a (ideally anonymous) name. The second and third variables, loss and gain, specify the possible outcomes of the lotteries for each trial. For example, the second observation/row shows a lottery in which the potential loss was $6 and the potential gain was $8. Based on these two columns, we can see that the observations are ordered by the combination of loss and gain. This means the order of observations in the data does not reflect the actual order of trials in which participants saw them (as this order was random). All values of the two variables are numbers with the lowest possible loss/gain being 6 and the largest possible loss/gain being 40, depending on the condition a participant is in. Column four shows the response of the participant to the lottery, either accept or reject. The sixth variable, resp, is a numeric version of the response. Here, an accept decision is represented by a 1 and a reject decision by a 0. So these two variables carry the same information, but in different formats that have different benefits. The response variable makes it easy to understand what participants response was (i.e., it is clear which value corresponds to which possible response, accept or reject, in the study). The resp variable makes it easy to perform calculation on the results given it uses a numerical code to represent the same information. For example, because reject is mapped onto 0 and accept is mapped onto 1, we could take the mean over all observations to get the overall acceptance rate across all gambles (the mean of the whole data is 0.38 which corresponds to an acceptance rate of 38%).12 However, if we only had the resp variable, we would additionally need the information what actual response 0 and 1 correspond to. Finally, variable five informs us in which condition each participant is in. Remember from the previous chapter that the experiment varied the range of gains and losses across participants resulting in four conditions in total: a condition with loss and gains ranging up to -$20/+$40, a -$20/+$20 condition, a -$40/+$40 condition, and a -$40/+$20. This information is here provided in form of a decimal number with the values before the decimal point referring to the range of gains and the values after the decimal to the range of losses without the trailing 0 (i.e., in the opposite order to how we have referred to the conditions so far). Thus, participant with subno 8 is in the -$20/+$20 condition and participant 369 in the -$20/+$40 condition (if we were showing a participant in the -$40 loss condition, there lowest possible loss outcome would be 12 and not 6 as for the two participants shown here). The example from Walasek and Stewart (2015) hopefully clarifies the abstract definition of a variable that was provided above. For each variable, we have a set of possible outcomes that is defined by our research design. For example, for the participant identifier subno, this set encompasses all possible participants that can take part in the study. For loss and gain, this set contains all potential losses and potential gains that occur in the lotteries. For response there are the two possible outcomes, to accept or to reject a lottery. We then define values for each possible outcome. In the case of subno we assign a different number to every participant from which we collect data. For loss and gain, the values correspond to the magnitude of the potential loss and gain in US dollars (the currency that was used in the experiment). For response, we do not use a numeric code but use a word to represent the two outcomes. There are a few things of note in the example data shown in Table 2.1. Every observation in this data is complete; for each observation we have values on each variable and no missing data. Whereas this is common in experimental research, it is not always the case for other types of research. Whereas missing data is not something we will discuss in detail in this book, it is important to be aware that it can happen and to think about what to do in this case (sadly, there is no general solution). For example, in the type of experimental research used as an example here, missing data can happen if the computer used for data collection crashes. As this usually happens rarely and in an unsystematic manner, we generally simply discard such incomplete data and collect data from another participants. A different issue exist if data is missing systematically. This can happen in research on sensitive issues. For example, it is easy to imagine that in a study on sexual health participants that have a sexual transmittable disease (STD) are unwilling to report this fact and could therefore just not answer questions about their STDs. Discarding these cases of missing data is problematic as doing so will bias the results in the sense of resulting in lower rates of STD than actually present. We have multiple observations per participant, 64 to be precise. These 64 observations are given in different rows. We call this data format  in which the data from each participant potentially spans multiple rows, one row per observation (i.e., 64 rows per participant in the present case)  the long format. This long format contrasts with a wide format that is also commonly found in the social sciences. In the wide format, the data of one participant only spans a single row. In case a participant has multiple observations, these are given in different columns. For the procedures introduced in this book, we generally want the data to be in the long format. (If each participant provides only one observation, there is no difference between the long and the wide format.) The variables differ in whether they contain numbers (all variables but response) or no numbers (only response). 2.2 Data Types Let us discuss the last point from above in more detail. A common intuition is to think about numbers when thinking about data. As we have seen in the example data, this is not necessary. Data does not have to be numbers. The response variable shows that we can use other values, such as words or phrases, to represent values of variables. However, the conception of data primarily as numbers is also not completely false. For example, the statistical analyses introduced in this book need to represent all variables in terms of numbers. Fortunately for us, the tools we will be using will generally convert data not using numbers into numeric data when necessary. This means we will use the type of data representation that makes it easiest to understand what the data stands for. 2.2.1 Numerical Versus Categorical Variables An important issue that arises when thinking about data as numbers is that numbers can mean different things. One possibility is that numbers represent numerical information; that is, they represent a measurement, magnitude, or count of something. However, we can also use number in a broader sense in which they only serve as a label (e.g., numbers on football jerseys or telephone numbers). In this case, numbers only represent categorical information; each observation falls into one of a set of mutually exclusive categories. The meaning of the numbers has important consequences of how we use them. If numbers do not represent numerical information most mathematical operations do not make sense. For example, it does not make much sense to calculate the average of two telephone numbers. Let us exemplify this with some variables from the example data. Let us begin with the loss/gain variable pair (we can consider them together, because the type of information is the same, the only difference is whether the number refers to a potential loss or a potential gain). For these variables, the meaning of numbers corresponds to the common understanding of numbers as a magnitude of something. In particular, the magnitude of a potential loss and potential gain. We could understand these variables as measuring the magnitude of the potential loss and potential gain of a lottery. The larger the number the larger potential loss and gain. In fact, the number exactly represents the potential loss and potential gain (i.e., the measurement of the potential loss and gain is perfectly accurate). We can treat these variables as numeric variables in a statistical analysis, because (a) the numbers of the variables represent numeric information, and because (b) performing mathematical operations, such as addition or calculating the average of the numbers, is meaningful for this variable. For example, we could calculate the average potential loss/gain for a participant and it would be useful information (i.e., we could interpret the average in a meaningful way, for example by comparing with the average loss/gain in a different condition). As a second example, let us consider the subno variable. Here, the numbers do not really measure the magnitude of something. Participant number 16 is not twice participant 8. From just looking at the variable, we also do not know what it means. As described above, one just needs to assign numbers somehow to each participant. For example, one could assign number 1 to the first participant that participates in the experiment, number 2 to the second one that participates, and so forth. Alternatively, one could also assign number 1 to the first participant invited, number 2 to the second one invited, and so forth. Another possibility is to specify the maximum number of participants one can collect, say 500 participants, and then just assign a unique random number from 1 to 500 to every participant that participates (e.g., by drawing them from a pool of all numbers without replacement). Importantly, we do not need to know which of these procedures was used. The only reason we have the subno variables is so we know to which participant a particular observation belongs to. The numbers in subno only serve the purpose as a label identifying the participant. Instead of numbers, we could also use non-numeric labels, such as random strings of letters, as the participant variable. Consequently, it does not make much sense to perform any mathematical operation on the subno variable. For example, the average participant number does not provide any useful information. For the purposes of this book, this distinction between these two data types is central: Should we treat a variable as a numerical variable or a categorical variable? The statistical methods introduced in the following chapters can only deal with these two types of variables (and categorical variables can generally also only serve the role of an explanatory variable and not as an outcome variable). So how can we identify whether a variable is numerical or categorical? Usually, it is easy to identify categorical variables among the variables that have numbers. Whenever the numbers represent a label, a variable is usually a categorical variable. For example, in addition to the subno variable, the numbers in the condition variable only serve as a label to identify the condition. We cannot interpret the numbers of the condition variable shown in Table 2.1 as actually representing a numerical value of either 20.2 or 40.2. Instead, each of the four possible values of the variable, 20.2, 20.4, 40.2 and 40.4, refers to one of their four conditions of the experiment, with loss and gains ranging up to either -$20/+$20, -$40/+$20, -$20/+$40, and -$40/+$40 (note again that the value after the decimal point is the range of the potential loss and the value before the decimal point the range of the potential gains). And non-numeric variables in which the values are labels, such as for the response variable, are also clearly categorical variables. More difficult is the decision for the resp variable. Clearly, the two possible values of the response variable, the accept or reject decision, are response categories or labels. However, when transforming it into a variable with numbers 1 and 0, we can perform meaningful mathematical operations on it. As discussed above, the mean of the variable can be interpreted as the average accept proportion. More generally, any binary categorical variable (i.e., a categorical variable of two categories) can be seen as a special case where treating it as a numerical variable can in certain situations be meaningful. However, whether or not it is meaningful depends on the situation. In general it is best to explicitly treat a variable as categorical unless one is sure treating it as numerical is meaningful. To sum this up, for the statistical purposes of this book we distinguish numerical variables and categorical variables. Numerical variables hold numerical information such as magnitudes of something or the degree with which something holds. For categorical variables the values of the variable serve as labels designating membership in one of a number of mutually exclusive categories. When categorical variables are part of an experimental design, we will later also call them factors. 2.2.2 Assumptions of Numerical Variables What the case of the resp variable (i.e., the numerical representation of a binary categorical variable) shows is that the decision of whether something is a numerical or categorical variable can depend on the situation. To help with this decision, it is helpful to know what exactly is entailed by treating a variable as numerical. For the statistical methods used here, when we treat a variable as numerical we assume it represents continuous numerical information. What this means is that we assume that: A certain difference or interval has the same meaning anywhere on the scale. For example, a difference of 1 unit of the variable means the same whether we add it to 10 or 20. We can see that this holds for the loss/gain variable pair but we will later discuss examples where this is a questionable assumption. A corollary to this assumption is that calculating the mean for our variable must be meaningful in itself. If we cannot interpret the mean, a variable cannot be treated as numeric. Our variable can in principle take on any real-valued (i.e., decimal) number. That is, even though we might have only used discrete values for our variables,13 such as for the loss/gain variable pair only a subset of the whole numbers between 6 and 40 (see Table 1.1), our statistical method assumes the in-between values are possible and in principle meaningful. As we can see, for the loss/gain variable pair, the two assumptions are fully satisfied. However, the loss/gain variable pair is not actually an outcome that was measured in an experiment. Therefore, in a statistical analysis it will not play the role of a variable for which it is most important that the assumptions are fulfilled. Instead, this variable pair was part of the design of an experiment. Consequently, let us consider a few other example variables to see how well they fulfil the two assumptions for a numerical variable. For example, the numerical outcome variable in this data set is resp. Clearly, resp does not fulfil the assumptions as it only has two discrete outcomes, the values 0 and 1. However, we can calculate and interpret the mean (as the average proportion accepted). We could also assume that a specific difference, say a 0.1 (or 10%) difference, means the same whether it happens at an acceptance rate of 50% or an acceptance rate of 85%.14 So whereas the assumptions are violated they are also partially fulfilled. What this entails is that whether we can interpret the results from an analysis depends on the exact context and circumstances. For example, if our statistical analysis would lead to results or predictions beyond the probability range of 0 to 1, this would be clearly problematic as the results would not be meaningful. In other words, we would have learned very little meaningful about our data from such a statistical analysis. A very popular variable in psychology and related sciences is subjective rating scales (also known as Likert scales). For example, we have discussed the study of McGraw et al. (2010) where participants in one condition where asked to rate the intensity of their emotional reaction to a potential loss or potential gain on a response scale ranging from 1 = no Effect to 5 = Very Large Effect (see Figure 1.1, unipolar intensity scale). Does this variable represent a numerical variable? Clearly, a value of 5 represents an emotional reaction that is larger than a value of 1. What this means is that the variable does represent a magnitude, but does it fulfil the assumptions spelled out above? We can also take the average of the scale and interpret it in a meaningful way. Specifically, the average emotional intensity for participants in the loss condition, 3.6, was larger than the average emotional intensity for participants in the gain condition, 3.1. However, it is questionable whether a difference of 1 means the same everywhere across the scale and therefore if assumption 2 is satisified. More specifically, is the difference between No Effect and Small Effect (i.e., the difference between 1 and 2) the same as the difference between Moderate Effect and Substantial Effect (i.e., the difference between 3 and 4)? Numerically it is, but whether this also holds psychologically is a question that is difficult to answer. Like most researchers McGraw et al. (2010) have treated this variable as a numerical variable so have made this assumption (which is also implicit in the process of calculating the average). The validity of their conclusions rests to some degree on whether or not we believe making this assumption makes sense. Let us generalise the conclusion of the previous paragraph and answer the question what it means that the two points above represent the assumptions for treating a variable as a numerical variable. Can we only treat a variable as a numeric variable in a statistical model if it perfectly meets the assumptions? In an ideal statistical world the answer would be yes, but the reality of data analysis always differs from the ideal. Many of the variables that we regularly encounter in our research (e.g., rating scales) violate the two assumptions to some degree and we still need to include them as numerical variables in our model (because treating them as categorical does not help us in answering our research questions). Whenever the assumptions are to some degree violated this can be interpreted as another instance of an epistemic gap (or as an instance of the first epistemic gap, Section 1.3.1). The fact that the assumptions are violated opens the possibility for an alternative explanation of the results that differs from our hypothesis. In other words, if the assumptions are perfectly met the evidence provided by our statistical analysis is stronger than when the assumptions are only partially met. The problem is that once we have numbers and treat them as a numerical variable, the computer treats all the numbers the same way (i.e., assuming they are a continuous numerical variable), the numbers dont remember where they came from (Lord 1953)15. Only we  the researchers  know where the numbers came from and need to take this into account when interpreting statistics. We can also interpret this insight in terms of the concepts introduced in the previous chapter. The numbers are part of the operationalisation; we establish a procedure that maps real world entities (above we have called these possible outcomes or states of affairs) onto values of the variables (which are in many cases numbers). The numbers that emerge from this procedure are related to our research question, but they are not identical with our research question. Any inference from the statistical results based on these numbers requires many auxiliary assumptions, one which is that we assume that numerical variables are continuous. And as we can never be sure if all the auxiliary assumptions are true, we have to be careful and humble with the conclusions we draw from our research. 2.3 Measurement So far we have categorized different variables as they appear in a data sets and how we can integrate them into a statistical analysis. Here, we take a step back and consider in a principled manner how the variables are created. The question we are considering is what does the measurement process by which we assign values to events allow us to infer from the variable. 2.3.1 Measurement Scales The discussion above is about variables consisting of numbers and the meaning of the numbers. One way to interpret this discussion is in terms of the first epistemic gap introduced in the previous chapter, the difference between our research question and the operationalisation of the research question (Section 1.3.1). If we apply this distinction to the issue of this section, the meaning of variables, we can understand this as the distinction between the magnitude of a latent construct, which is called an attribute in this context, a variable is supposed to represent (e.g., strength of an emotional intensity or personal risk preference) and the measurement of that attribute through the operationalisation (i.e., the application of a procedure that assigns a number for that attribute to an observation). An important theoretical contribution to this distinction within the context of psychology comes from Stevens (1946). He assumed that we can distinguish four different types of measurement operationalisation, which he called measurement scales, with respect to which type of relationship between attributes they reveal. These four different measurement scales are, nominal, ordinal, interval, and ratio scale. If a scale is constructed by assigning labels to different attribute values, it is called a nominal scale and can be understood as equivalent to what we have termed a categorical variable. For a nominal scale, the values of the attributes do not exhibit any quantitative relationship among each other. In addition to the examples discussed above, many demographic variables can be understood to be on a nominal scale such as gender (e.g., male, female, non-binary, or other) or handedness (right-handed, left-handed, or ambidextrous). If a scale is constructed through rank orderings of attributes it is called an ordinal scale. As a consequence, we can order attributes along a dimension but cannot make any further quantitative distinctions. A common example of an ordinal scale is the final result of a sports competition with first place, second place, and so on. The important aspect of an ordinal scale is that differences between values on the ordinal scale do not need to correspond to equivalent differences in the attribute that is measured with the ordinal scale. If we stay within the sports competition example, the difference between the first and the second place in terms of performance does not need to be the same as the difference between the second and the third place. For example, in the 2020 Olympics 100 m women sprints final the difference between the first place (Elaine Thompson-Herah) and the second place (Shelly-Ann Fraser-Pryce) was 0.13 seconds, whereas the difference between the second and third place (Shericka Jackson) was only 0.02 seconds. In this case the same difference on the ordinal rank scale (i.e., one rank difference), does not correspond to the same difference in the underlying attribute (i.e., performance, time needed for sprinting 100 m). Some demographic characteristics can also be understood to be on an ordinal scale, such as education levels (e.g., some primary or secondary school education, compulsory education up to age 16, college, or higher education or professional &amp; vocational equivalents). An interval scale results from an operationalisation that also maintains that differences, or intervals, between the values of the attributes on the scale need to have the same meaning across the scale. Thus, an interval scale can also be understood as fulfilling the requirements of a numerical variable.16 The typical example of an interval scale is temperature measured in either degrees Celsius (°C) or Fahrenheit (°F). Within each temperature scale, a 1 degree difference has the same meaning independent of the current temperature. Furthermore, both scales can be transformed into each other. For interval scales it also makes sense to calculate the mean (e.g., the mean temperature), but calculating ratios does not make sense. For example, saying the 40 °C is double the temperature of 20 °C is not a really meaningful statement (e.g., because 20 °C = 68 °F and 40 °C = 104 °F and \\(2 \\times 68 \\neq 104\\)). The final scale type, ratio scale, results from an operationalisation that in addition to maintaining the meaning of differences across the scale also contains a true zero point of the attribute. To stay within the typical example of temperature scale, whereas the zero point of degrees Celsius and Fahrenheit is arbitrary and they represent only interval scales, the zero point of the Kelvin scale, 0 K, is the lowest possible temperature making the Kelvin scale a ratio scale. Many physical scales are on a ratio scale such as length or time. For example, it makes sense to say that a sprinter who took 20 seconds for the 100 m sprint took twice the time as a sprinter who only took 10 seconds because 0 seconds is the true zero point of no time. Whereas Stevens four measurement scales are widely popular in psychology and related disciplines thanks to their prominence in most introductions and textbooks, their actual scientific contribution needs to be considered critically (following Michell 1997, 2002, 1999). What Stevens proposes attempts to bridge the epistemic gap between the attribute and its measurement. According to this position, if we have established an interval or ratio scale, we have learned that the underlying attribute exhibits an interval or ratio structure. Unfortunately, because of the problem of underdetermination discussed before, learning about the actual structure of a theoretical construct or attribute is not that easy. Only because the numbers look like a numerical variable, this does not mean that the underlying attribute behaves like a numerical variable. Therefore, I do not recommend using the four different measurement scales to discuss psychological measures. For example, in this book we will use the theoretically more neutral terms categorical and numerical variables. As discusses in the previous chapter, this way our description of the research remains on the level that it is performed, the level of the operationalisation and statistical analysis. Discussing the problem of measurement scales in detail is beyond the scope of the present chapter, but the crux of the matter is the issue we have already highlighted in the previous chapter. The operationalisation used to measure a certain latent construct does not represent the latent construct. Stevens defines measurement as the application of a procedure that assigns a number representing an attribute to an observation. However, this definition of measurement does not allows one to infer the structure of an attribute from its operationalisation. Instead, a proper quantitative measurement that is analogous to the measurement of physical quantities (e.g., length) needs to fulfils a number of assumptions, such as expressed in the theory of conjoint measurement, that very few psychological or behavioural measures have been shown to fulfil. Put more bluntly, Stevens attempt to bridge the epistemic gap is logically incorrect, to establish what he wants to establish we need stronger theories that define the theoretical constructs more rigorously (Michell 1999). Even though Stevens idea that we can measure latent constructs through interval or even ratio scales is incorrect, his distinction is nevertheless helpful as it again allows us to understand the limits of what we can learn from data. What we can take away from the present discussion is that the common measurement approach in psychology and related fields is generally only able to establish ordinal relationships. For example, subjective ratings scales, but also choices among lotteries, only represent ordinal relationships in terms of the underlying latent attributes. Nevertheless, we generally treat data from such variables as numerical variables in statistical analyses. This again reinforces the point made before that we cannot interpret the results from statistical analyses as directly answering our research questions. Our statistical analysis generally makes assumptions about the nature of the underlying attribute or construct that we cannot verify. However, this does not mean that all hope is lost. As Stevens (1946) already mentioned when introducing ordinal scales, treating them as numeric is not always pointless (p. 679): In numerous instances it leads to fruitful results. We just have to be mindful that measurement in psychology is generally not the same as measurement in physics when interpreting the results. We primarily learn something about our operationalisations and not directly about the theoretical constructs that we use when formulation our research questions. 2.3.2 Reliability and Validity The main message of the previous section is that measurement of mind and behaviour is not as straightforward as measuring physical attributes such as length. Nevertheless, we should aim to use measures (i.e., operationalisations that provide measurements) that are of high quality. Two concepts that are important for judging the quality of measurement are reliability and validity, which we will introduce now. Reliability refers to the consistency of a measure. One intuitive way to understand reliability is as the consistency of a measure across different measurement occasions under the same conditions. Reliability is also inversely related to noise in the measurement process. A measure has a high reliability if repeated applications to the same conditions lead to very similar outcomes (i.e., the level of noise in the measurement process is low). A measure has low reliability if repeated applications to the same conditions lead to widely different outcomes (i.e., the level of noise in the measurement process is high). For example, consider a regular bathroom scale. We expect such a scale to have a very high reliability; we should get pretty much the exact same results if we step on it several times in a row, as long as we do not change our weight in between (e.g., by drinking something). Validity refers to how strongly a measure measures what it is supposed to measure. Within the concepts introduced within this book, validity thus refers to the ability of a measure to bridge the epistemic gap between the operationalisation of a construct and the actual (i.e., true) value of the construct. Thus, validity can be seen as one way to conceptualise the question of how strongly a measurement operationalisation corresponds to the measurement of a physical quantity. Given the difficulties in defining or even establishing the constructs researchers are interested in, establishing whether or not a measure has a high or low validity is generally difficult. One way to visualise both reliability and validity is given in Figure 2.1 below. Here, each panel represents one operationalisation or measure and each shot on the target represents one measurement with that measure. We can see that reliable measures have low levels of noise (i.e., low level of dispersion of the shots) around one mean value. In this figure, validity is visualised as a bias with respect to the centre of the target. Valid measures are centred on the target whereas invalid measures are centred around an off-target value. What this figure highlights is that reliability and validity are in principle distinct qualities. Only because a measure is reliable, it does not mean it is valid. And likewise, a valid measure does not have to be reliable. Figure 2.1: Visualisation of reliability and validity as shots on a target. Figure is taken from Statistical Thinking for the 21st Century by Russell A. Poldrack (Figure 2.1). Whereas the visualisation in Figure 2.1 should provide a good first intuition about reliability and validity, the conceptualisation of validity as a bias is not the only way to think about it. To stay within the metaphor provided by the figure, an invalid measurement could also be one that aims for the floor instead of the target. Or even something completely missing the task such as shooting darts when the goal is to shoot bullets. The problem with validity is that sometimes we dont know how to better define or measure what we are measuring so determining validity is not always easy. The previous paragraph already points to an important distinction between reliability and validity. Usually there is a way to quantify reliability, but quantifying validity is only possible for specific interpretations of validity. The most common ways to quantify the reliability of a measure are: Split-half reliability or internal consistency refer to the reliability estimate that results from splitting a measure into sub-measures and comparing the scores across sub-measures (e.g., by calculating a score for all odd items and comparing them with the score of all even items). Generally, the split-half reliability or internal consistency can be improved by making a measure longer (i.e., adding additional items). Therefore, longer measures are often more reliable than shorter measures (which also intuitively makes sense, the shorter a measure the more likely noise plays an important role). Test-retest reliability refers to the consistency of applying the same measure at different time points. When using a measure such as a questionnaire, the difficulty in calculating the test-retest reliability is the possibility of memory consistency effects or temporal instability. Memory consistency effects refer to the observation that participants often prefer to be self-consistent with their previous answers if they remember them thus potentially artificially increasing the reliability. Temporal instability on the other hand can result in artificially lower reliabilities if the measured construct is temporally unstable (such as mood). Inter-rater reliability refers to the agreement of different raters for the same event or situation. Inter-rater reliability can usually only be calculated for measures that are not self-assessments or subjective scores. Prominent examples of situations in which one can calculate inter-rater reliabilities are medical diagnoses (i.e., by comparing the diagnosis across multiple doctors) or essay marks (when marked by multiple independent markers). To quantify validity we need to have an external criterion that also measures the construct of interest. When we have such a criterion, we can compare the value on the criterion with the value on our measure which gives us an estimate of the criterion validity of the measure. For example, in the previous chapter we had discussed that there exist different tasks or questionnaires for measuring an individuals risk preferences. Consider we also had access to a persons financial history showing to what degree they invest their money into relatively high risk (e.g., stock options), medium risk (e.g., individual stocks), or lower risk (e.g., index funds) financial instruments. A risk preference measure with high criterion validity would be one for which participants that have a high score on the measure invest more of their actual money in high-risk investments. As we have discussed in the previous chapter, it is unclear if such a measure of risk preferences exist. Other types of validity are non quantifiable. Construct validity is usually considered the most important type of validity as it refers to all empirical and theoretical support that provides evidence that a measure measures what it is supposed to measure. As construct validity essentially asks how well a measure bridges the first epistemic gap, it is more an abstract concept in which one talks about measures than an actual criterion used to describe measures (i.e., I am not aware of a psychological measure that has construct validity). Another interesting type of validity is face validity. This refers to the degree with which a test appears to measure what it is supposed to measure. Whereas face validity is not objectively important for the validity of a test (i.e., what matters it not if a test looks like it measures what it is supposed to measure, but if it does so), it can be important for the commitment of participants. For example, if participants are intrinsically interested in contributing their time and effort to a research question then a measure with high face validity (i.e., that matches their interest) shoulod reduce participant drop out compared to a measure with lower face validity. Considerations of both reliability and validity should be incorporated into the overall assessment of the results when judging the empirical evidence provided by a given study. For example, if the dependent variable of a study only consists of a single item or response, this generally implies low internal consistency compared to a dependent variable based on more items or responses. In this case, the level of noise should be relatively high, which should influence how strongly we weigh the evidence from such a study. 2.4 Independent and Dependent Variables In addition to distinguishing the type of information variables can contain, we can also distinguish the different roles variables can play in the research process. Remember, when discussing the operationalisation step of the research process we specified that we need to identify relevant variables that we hope can address the research question as well as specify an empirical hypothesis involving at least two variables. As we will see here, we can assign two different roles to these at least two different variables. Usually exactly one variable is the variable for which we are interested in the results, in psychology we call this variable the dependent variable. Synonyms for dependent variable that are common in the statistical literature are response variable, outcome variable, or criterion and we can see that their meaning points in the same direction as dependent variable. The dependent variable is the main outcome of our study, the variable which we are primarily interested in measuring. The other variable(s) is called independent variable(s) in psychology, because we believe the values of the dependent variable depend on the values of the independent variable(s). A popular synonym for independent variables in the statistical literature is covariates, as the dependent variable is assumed to covary with the independent variables.17 Above we have also used the term explanatory variable to describe independent variables. Loosely speaking we can describe the distinction such that a study is about the effect of the independent variable on the dependent variable.18 Let us show this distinction in the loss aversion study of Walasek and Stewart (2015) the data of which is shown in Table 2.1 above. Their research question was that what matters for peoples preference for symmetric 50-50 lotteries is not the absolute value of the potential loss and gain but the relative rank of the lotteries compared to other lotteries. The operationalisation of this research question involved the manipulation of the range of lotteries in one variable, condition, and measuring participants responses to symmetric lotteries in the response\\resp variable pair. Here the distinction between both variable types is relatively straight forward. We are interested in the effect of condition on response: What is the impact of different ranges of lotteries (i.e., manipulated across conditions) on participants responses. This makes condition the independent variable and response variable the dependent variable. In general, the distinction between independent and dependent variable is easy to understand in an experiment, such as the study by Walasek and Stewart (2015). In an experiment, independent variables are manipulated, such as the case for condition. Manipulated means we assign participants to the different conditions (compared to measuring in which condition a participant is). Not all studies are or can be experiments in which the independent variable is manipulated. For example, a common research question is the effect of a demographic variable on an outcome. However, demographic variables cannot really be manipulated or assigned to participants. For example, we might be interested in studying the effect of parental wealth on childrens educational attainment. Whereas manipulating parental wealth is in principle possible, a more common approach is to measure this variable as well as childrens educational attainment. Nevertheless, we can still make the distinction between the independent variable, parental wealth, and dependent variable, educational attainment. Not all variables in an experiment neatly fall within the distinction of independent and dependent variables. For example, let us go back to the six variables shown in Table 2.1 that make up the full data collected in the study by Walasek and Stewart (2015). As discussed above, the response\\resp variable pair is the dependent variable and condition the independent variable. This leaves us with three further variables that need to be classified. Have a go and try to classify the other three variables before continue reading the paragraph. Let us begin with the loss/gain variable pair that determines the possible outcomes of a lottery shown to participants. Clearly these are also manipulated. More specifically, the condition determines exactly which lotteries and therefore which values of the loss/gain variable pair a participant works on. Thus, the loss/gain variable pair are also independent variables that jointly determine the independent variable condition (i.e., if we did not have the condition variable in this data set we could determine it from the loss/gain variable pair). This means that determining the independent variables in this study depends on the perspective one takes. If we only focus on the main research question and symmetric lotteries then condition is the independent variable. However, if we also look at the lotteries individually, then condition and the loss/gain variable pair are the independent variables. Table 2.1 contains one more variable, subno, the participant identifier variable. It might seem a bit surprising to think about this variable in terms of independent and dependent variable, but we should be able to classify this variable somehow. Clearly, subno is not a dependent variable. We are not interested in the values or results of the subno variable. However, it also seems not clearly an independent variable. We do not have any specific expectations or ideas of how different subjects affect the response variable. To help us with the classification let us consider again why we have the subno variable in the data in the first place; because we collect data from multiple participants and need to identify to which participant an observation belongs. The follow-up question to this is why do we collect data from multiple participants? As discussed in the previous chapter (Section 1.3.2), participants are a source of noise in our experiment as different participants can do what they do for a multitude of reasons. If we only had the data from one participant, we could not distinguish between the idiosyncratic noise of that participant and the signal we are interested in. By collecting data from multiple participants, we try to control for the noise by averaging over it with the hope that what remains is the signal. The overarching idea is that noise has an unsystematic effect on the results; some participants may be more likely to show a particular behaviour whereas other participants may be less likely to show that behaviour, but on average the noise cancels out. In sum, we collect data from multiple participants to control for noise that is inevitable when dealing with real people. Thus, we could say subno is a control variable. We could even be more specific and say it is a control variable and an independent variable, because we use it to control noise at the level of the design. In other situations we might measure a variable for control purposes in which case we could call it a control variable and a dependent variable. To sum this section up: Jointly, the dependent and independent variables are the key components of the operationalisation of a research question. They are the central concepts that make up the study design and link the practical reality of the research (i.e., how the research actually takes place and what is measured) with the research question. It is not wrong to say that a study is defined primarily by its dependent and independent variables. When designing ones own study, making it clear what the dependent and independent variables are is maybe the most important decision after having decided on a research question. Likewise, when reading a scientific article describing a study, understanding clearly what the independent and dependent variables are is central to understanding the study. Therefore, whenever thinking and talking about any research, make sure to be clear what the dependent and independent variables are. In experimental research this often boils down to asking: What was the task of the participants? Other variables that are part of a study usually serve some control purpose and can be denoted control variables. 2.5 Experimental versus Observational Variables As we have seen when discussing the distinction between independent and dependent variables, we can distinguish different types of independent variables or research designs, namely experimental and non-experimental independent variables. Here we will adopt the common terminology and use observational variable to describe non-experimental independent variables. If a study solely consists of experimental variables (we drop the independent part from now on because experimental or observational variables are always independent variables), we can call it an experimental study or experiment for short. If a study solely consists of observational variables, we can call it an observational study. If a study contains both experimental and observational variables, there is no agreed upon name and depending on which variable is more relevant to the research question, researchers tend to use either experimental or observational study. However, as experimental variables provide a number of evidential benefits that will be discussed here, there is a tendency to call a study an experiment even if it also contains observational variables. Depending on the actual situation and the inferences drawn this can be seen as a stretch. An experimental variable is one that is in control of and can be manipulated by the researcher. What this means is that the values of the variables can be assigned to participants by the researcher. For example, in the study of Walasek and Stewart (2015) the researchers assigned each participant to be in one of the four conditions corresponding to a different range of potential losses and gains. Likewise, in the previous chapter we briefly introduced a study of Hinze and Wiley (2011) on the generality of the testing effect. After an initial reading of a piece of text, participants were assigned to one of three experimental conditions: a control condition in which they could re-read the materials, a testing condition using open-ended questions, and a testing condition using a fill-in-the blank text. In both of these cases, the researchers decided which condition a participant was part of. The important part of an experiment variable is not only that participants can be assigned to different conditions, but how they are assigned. More specifically, for an experimental variable the assignment needs to be performed randomly; we say participants are randomised to the available conditions. One way to understand random assignment is that before the experiment takes place, the probability to be in any of the experimental conditions needs to be the same for every participant.19 For example, random assignment means that for every participant in the study of Walasek and Stewart (2015), the probability to be in any of the four conditions is 0.25 (i.e., 1/4). For every participant in the study of Hinze and Wiley (2011), the probability to be in any of the three conditions is approximately 0.33 (i.e., 1/3). You can imagine randomisation as an actual physical process that produces a random outcome, such as the toss of a coin or throw of a dice. For example, for the study of Hinze and Wiley (2011) we could imagine that for every participant that takes part in the experiment, the researcher (or research assistant) throws a regular six-sided dice. If the dice lands on 1 or 2 the participant is assigned to the re-reading condition, if the dice lands on 3 or 4 the participant is assigned to the open-ended question condition, and if the dice lands on 5 or 6 the participant is assigned to the fill-in-the-blank condition. Alternatively, if we pre-specify the sample size and want to ensure that every group is of approximately the same size, we could use a different approach. We could prepare as many sheets of paper as the number of participants we want to collect. On each sheet we write one condition so that among all sheets, each condition appears equally often. Then, we shuffle all sheets in a bowl to randomise their order. When performing the experiment, we take one sheet out of the bowl (without putting it back) for every participant and assign the participant to the condition written on the sheet. Nowadays randomisation is mostly done through a computer using so-called random number generators. An observational variable is a variable that is not in control of the researcher and we cannot randomly assign participants to condition. In other words, whenever randomisation is impossible an independent variable is an observational variable. Above we talked about demographic characteristics, in particular parental wealth, as an independent variable. As already described there, demographic variables generally cannot be randomly assigned but are a mostly immutable part of a person. Consequently, demographic characteristics (e.g., age, gender) are generally observational variables. The same is true for many other psychological characteristics of a person such as personality traits (e.g., extraversion) or abilities (e.g., intelligence quotient). In the vast majority of cases such variables are observational variables.20 At this point you might wonder why an experiment necessarily entails randomisation of the independent variable. What is the benefit of an experimental over an observational variable? The reason for this is that only randomisation allows drawing causal inferences from a study. Only with an experimental  that is randomised  independent variable can we say that the independent variable is the cause of the dependent variable. Without randomisation (i.e., when dealing with an observational independent variable) such an inference is not permitted. Remember that above we said one way to think about the distinction between dependent and independent variables is that in a study we want to learn about the effect of the independent variable on the dependent variable. We did not specify what we mean with effect, but we said this way to think about dependent and independent variable only holds loosely. With effect we meant a causal relationship, the independent variable is the cause of the dependent variable. In the absence of such a cause-effect relationship, it seems wrong to speak of an effect of the independent variable. And we can now see the reason why we said this only holds loosely, it only holds if the independent variable is an experimental variable, but not if it is an observational variable. 2.5.1 Epistemic Gap 3: Causal Inference and Confounding Variables The reason only an experimental variable allows a causal inference is due to another epistemic gap, the possible influence of confounding variables when dealing with observational data. A causal inference means we learn that the independent variable and nothing else is responsible for the effect observed on the dependent variable. A causal inference is only possible if plausible alternative explanations for the observed effect on the dependent variable that do not involve the independent variable can be ruled out. In the context of a causal inference, such an alternative explanation is known as a confounding variable or confounder for short. If we randomly assign participants to conditions we can theoretically rule out confounders as an alternative explanation. However, in the case of an observational variable we cannot; there may be other reasons, the confounders, that are related to the observational variable that are responsible for the observed effect. Similar to the first epistemic gap, the inference that the independent variable is the cause of the effect in the dependent variable is underdetermined for an observational variable, but not underdetermined for an experimental variable.21 Let us exemplify this problem with a new example. Imagine you want to investigate the effectiveness of a novel drug against a control treatment (i.e., old drug) for the treatment of a viral infection in a hospital setting. The independent variable is the treatment (control treatment versus novel drug) and the dependent variable is viral load (i.e., whether or not the virus can still be detected in the system). Let us imagine that the results show that the new drug is more effective than the control treatment. That is, participants show a lower viral load (i.e., are less sick) after the novel treatment than the control treatment. The question we are trying to answer now is whether it matters for the inference we can draw from this study if the assignment to treatment condition is random or not. Let us begin by considering a non-random assignment of the independent variable. For example, one way to implement non-random random assignment could be to use the novel drug in one hospital and the control treatment in another hospital. If we were to run such a study, would this allow us to conclude that a difference in the dependent variable is due to the differences in treatment? This inference would only be allowed if the two hospitals were identical. If there were any systematic differences between the hospitals, say patients in the hospital with the control treatment are on average older than patients in the hospital with the new treatment (e.g., because they are in different areas with different population characteristics), then this difference could be responsible for the difference in the dependent variable. The systematic difference in age between the hospitals plays the role of a confounder. Age is responsible for the choice of hospital patients go to, because patients near the hospital in which the control treatment is administered are on average older than patients near the hospital in which the new treatment is administered. Age is also responsible for the difference on the dependent variable, because older patients are less likely to recover and thus have a higher viral load at the end of the study than young patients. In a situation in which a confounder is present, we cannot infer that the treatment is the cause of the observed effect. We again have the logical structure indicative of an underdetermination: The two conditions differ in terms of two characteristics, treatment status and age, so either of them (or both) can be responsible for the differences in the dependent variable. We cannot be sure which one of the two possible causes it is. Let us now consider a situation in which participants are randomly assigned to the two treatment conditions. Here, we again have data from two hospitals but participants in each hospital are randomly assigned to the treatment conditions. For example, for each new patient that shows the relevant symptoms the doctor administering the treatment takes a pre-randomised envelope that contains either the old drug or the novel drug.22 Because the assignments to conditions is random we would expect that confounders such as age are balanced across the two condition. Every participant that comes to any of the two hospitals has the same chance to either get the control treatment or the novel drug, independent of their age or other characteristics. Consequently, as long as randomness did not introduce an accidental confounding (see Section 1.3.2) we can attribute the effect on the dependent variable to the independent variable. In summary, the difference between an experimental and an observational variable is the degree with which you can rule out possible alternative explanations. For an experimental variable for which participants are randomly assigned to conditions we know that in theory all confounding variables should be balanced. So as long as the randomisation did proceed as planned, one can be certain that the only alternative explanation for the effect of the independent variable on the dependent variable is random chance. We always might get unlucky and a confounding variable just happens to be unbalanced in our data set. However, with larger sample sizes and larger effects the chance alternative explanation becomes increasingly unlikely. So for experimental variables the only epistemic gap when wanting to judge whether the independent variable is responsible for the effect of the independent variable on the dependent variable is the effect of random chance or noise (see Section 1.3.2). For example, in the main study that lead to the approval of the Biontech Covid-19 vaccine (Polack et al. 2020), over 40,000 participants were randomly assigned (i.e., more than 20,000 participants per condition) to either receive the real vaccine or a placebo (i.e., a saline injection without active ingredients). Among the participants that received the vaccine only 8 participants developed Covid-19 and among the participants that received the placebo 162 participants developed Covid-19. Whereas from these results we cannot definitely rule out that there is some confounding variable that explains the difference in contracting Covid-19 it seems extremely unlikely. Participants for this trial were recruited from six different countries (e.g., USA, Turkey, Brasil) and were diverse in their demographic characteristics (e.g., sex, ethnicity, age, weight), but these characteristics were extreme similar for both conditions (Polack et al. 2020, Table 1). For an observational variable for which participants are not randomly assigned to condition we do not know whether there is a potential confounding variable. One way to address this problem is to measure known confounding variables and show that they are not responsible for the difference in the dependent variable. But even when we are able to control or measure a large number of possible confounding variables, we can never be certain that there is not another unobserved confounding variable that is responsible for the effect. So for observational variables we always have to deal with the two epistemic gaps when wanting to judge whether the independent variable is responsible for the effect of the independent variable on the dependent variable, the problem of possible confounders plus random chance or noise. To end this section, let us come back to the example of our trial testing a new drug in a hospital setting. After the lengthy discussion on observational versus experimental variables you can hopefully see that the idea of only administering the new drug in one hospital and the control treatment in another hospital is a bad idea. Without proper randomisation of participants to treatments the inference that the drug is responsible for the effect on the viral load seems very weak thanks to the possible influence of confounders. You might even go so far to wonder who would ever run such a study without proper randomisation or believe the corresponding results. Sadly, a study that pretty much did exactly what we have sketched above  administering the novel drug only in one hospital and the control treatment in another hospital, with patients systematically differing between hospitals  played a very unfortunate role during the Covid-19 pandemic. In particular, the first study to suggest that Hydroxychloroquine was effective against Covid-19, the study by Gautret et al. (2020), had exactly this problem.23 Whereas critics where quick to point out this and other problems with the study (Bik 2020; Rosendaal 2020; Sayare 2020), the damage was done. The then current US president Donald Trump praised Hydroxychloroquine as a wonder cure of Covid-19. It required much scientific effort and follow-up studies, using resources that could have potentially been used more productively elsewhere, to show that it is not (for a full timeline of events see Sattui et al. 2020). The problem in this case was that whereas medical and statistical experts could immediately see the problems with the study, the general public could not. And once a false claim that appears to be scientific (i.e., in the media reported along the lines of researchers have shown that ) is established in the public discourse, it is often difficult to combat it. In general it seems that discussing the empirical evidence provided by a particular scientific study is either beyond the expertise available to the mass media or they are unwilling to invest the time commitment to do so. 2.5.2 Is Causal Inference from Observational Data Possible at All? What the previous section argues is that causal inference is generally only possible from experimental independent variables. With observational variables there can always be a confounder that is responsible for the effect instead of the independent variable. However, many interesting research questions cannot be investigated with experiments but only through observational variables. As we have discussed above, demographic variables or other immutable features of individuals, such as personality traits, are observational variables by definition. Likewise, many variables relating to lifestyle choices, such as dietary or exercise habits, might in principle be amenable to experimental manipulations, but in reality it seems difficult to impossible or completely unethical to run corresponding experiments. Does this mean we cannot draw causal inferences for such research questions? I believe the honest and realistic answer is that in the vast majority of cases we cannot. In my eyes a fair assessment of the situation is that, causal inference from observational data is literally the most difficult problem in the empirical sciences. Importantly, causal inference from observational data is not primarily a statistical problem. We have introduced the problem that confounders pose as an epistemic gap. And as for the other epistemic gaps, overcoming this epistemic gap requires diverse and conceptually strong evidence. There are statistical methods that can assist in providing such evidence, but they cannot provide the type of compelling evidence that is needed. The problem is that even if the observational data strongly suggests something, there always is the possibility that a confounder was missed or not adequately taken into account. As an example of the problem, let us consider the case of vitamin supplements, specifically vitamin C and E supplements (Lawlor et al. 2004; Woodside et al. 2005; Mozaffarian, Rosenberg, and Uauy 2018). Early evidence from large observational studies in the 90s with tens of thousands of participants suggested that taking vitamin C and E supplements reduces the chance of getting cancer and cardiovascular diseases to a considerable degree. Based on these positive results, large scale experiments (i.e., also with tens of thousands of participants) followed in which participants were randomly assigned to either take vitamin supplements or a placebo (i.e., sugar pill without vitamins) and monitored for several years. By and large, these experiments could not replicate the positive effects found in the observational studies. Unless an individual is susceptible to a vitamin deficiency, vitamin supplements do not appear to have a measurable health benefit. The probable reason for the difference between the observational studies and the experiments is likely due to an insufficient adjustment of socioeconomic status as a confounder. As often found, participants that came from a better socioeconomic background where more healthy (i.e., less likely to develop cancer and cardiovascular diseases) but they were also more likely to take vitamin pills (because they believed them to be helpful). Whereas the observational studies measured and tried to account for differences in socioeconomic status between participants who already take vitamin supplements and who do not, they only did so partially [e.g., they did not account for differences in the socioeconomic status of the parents which led to developmental differences that also affected the probability of developing cancer and cardiovascular diseases as well as the probability of taking vitamin pills; Lawlor et al. (2004)]. What this example shows is that even in a situation in which the confounder is in principle known (i.e., socioeconomic status) and the observational data sets were large (&gt; 10K participants), causal inference from observational data was not possible. Even after attempting to account for confounding, the observational data suggested a relationship that turned out to be spurious. The apparent problem was that accurately measuring the influence of the confounder was not possible, before knowing that the observed relationship is in fact spurious. Only an experiment was able to reveal that there was no effect of vitamin supplements. This example suggests that for many research questions and data sets common in psychology and related disciplines causal inference from observational data is equally difficult or even impossible. Especially as data sets are often considerably smaller and less is known about the causal relationship existing in a domain (i.e., which variables could act as confounders). As a consequence of the problem with observational data, the current book primarily focuses on experimental data sets and, where non-experimental variables are considered, their limitations will be discussed. Whereas focussing on experimental data restricts the type of research questions that can be investigated, it at least eliminates one of the three epistemic gaps introduced here. This also means that applying the methods introduced here to observational data sets will require additional care when trying to draw justified conclusions and is not recommended. To repeat what we have said above, the question of whether an effect found in observational data reflects a causal relationship is not a statistical question. So the statistical tools introduced here cannot provide an answer to the question of whether a relationship in observational data is causal. For researchers interested in analysing observational data, some good introductory literature that attempt to approach the problem of confounders in a principled manner are Rohrer (2018), Gelman, Hill, and Vehtari (2021), McElreath (2020), Hernán and Robins (2021), and Shadish, Cook, and Campbell (2002). Note that, given the additional epistemic gap that needs to be bridged, these methods are more advanced than the methods introduced here (i.e., require technical and mathematical knowledge going beyond what is required here). After all, drawing causal inferences from observational data is literally the most difficult problem in the empirical sciences. 2.5.3 Internal versus External Validity Above we have talked about validity in the context of measurement. In this context, the question of validity is the question if a measure measures what it is supposed to measure (e.g., when a risk attitudes questionnaire really measures risk attitudes it has a high validity). However, the term validity is also used in the context of experimental versus observational studies. In this context, the two relevant types of validity are internal validity and external validity which do not refer to a specific measure, but are used to describe complete studies or research designs. We provide a brief introduction to these two terms here, for more see Shadish, Cook, and Campbell (2002).24 Internal validity refers to the internal structure of a study and reflects the degree with which the study provides evidence for the causal relationship between independent and dependent variable. This means that generally speaking, internal validity is high if a study is an experiment (i.e., independent variable is randomised) and internal validity is low if a study in an observational study.25 Within the terminology of the epistemic gaps introduced in this book, internal validity is related to the third epistemic gap. Only when we can be sure there are no possible confounders is the internal validity high, and we can only be sure of this in case we have an experimentally manipulated independent variable. External validity refers to the degree with which the results of a study generalise to different settings, such as different situations, people, stimuli, and times. Within the terminology of the epistemic gaps introduced in this book, external validity is related to the first epistemic gap, the underdetermination of theory by data. The degree with which we can be sure that our results really address our research question and are not confined to the specifics of our operationalisation we can be sure that the results generalise to other situations. In other words, if we only learn that the causal link holds in the very specific circumstances that are tested within our study, but do not actually hold in the general terms in which our research question is formalised, external validity is low. For example, the study by Hinze and Wiley (2011) introduced in Chapter 1 directly addressed the external validity by seeing whether the testing effects also holds for a different operationalisation of testing. 2.6 Summary In this chapter we have introduced a number of important concepts that allow us to describe studies and research designs. We have begun by highlighting that as empirical scientists the ultimate arbiter for whether or not to believe in a theoretical position or hypothesis is empirical evidence. This evidence should come from systematically collected data sets and not anecdotes. Data sets that can be used to address our research questions consist of independent variable(s) and usually one dependent variable. The distinction between both is that we assume the dependent variable depends on the independent variable. If the independent variable is an experimental variable for which participants are randomised onto conditions we can even infer that the independent variable is causally responsible for the effect of the dependent variable. If the independent variable is solely an observational variable, we generally cannot make such a causal judgement. The reason for why observational variables do not allow causal inferences lies in the third epistemic gap introduced here. For an observational variable, there can always be a different confounding variable that is responsible for both the effect on the independent variable and the dependent variable. Together the three epistemic gaps put clear limits on what we can learn from empirical data in psychology and related disciplines. The first epistemic gap, underdetermination of theory by data, is the difference between the research question and the operationalisation of the research question. Whereas the operationalisation attempts to address the research question they are usually not the same. The second epistemic gap, signal versus noise, concerns the relationship between the operationalisation and the statistical analysis. Even if the statistical analysis appears to provide support for the empirical hypothesis, we cannot be 100% sure of that. There always is the chance that the observed outcome just occurred by chance  that is noise  and does not represent a genuine signal in the data. Finally, the third epistemic gap, confounding variables, is always present when dealing with observational independent variables. As just summarised, in the absence of randomisation we can never really be sure if the independent variable and not a confounding variable is the reason for the observed effect. Thus, we can reiterate the message with which we ended the previous chapter. If we interpret statistical results, we need to be careful and humble with the conclusions we draw. We have also introduced the different data types we can deal with in a statistical analysis. Independent variables can be both numerical and categorical variables. If an independent variable is categorical, we generally call it an experimental factor, or just factor. Dependent variables can generally only be numerical variables, unless it is a binary numerical variable that we are treating as numerical. We have also argued that most genuine psychological variables we collect, such as responses on rating scales, are only on an ordinal scale and do not satisfy the assumptions of being a numerical variable. However, in our analyses we nevertheless treat them as numerical. This violation of a statistical assumption places further limits on the inferences that are permitted from our study. In line with this, we have argued that measurement in psychology is a generally difficult problem and simply assuming our measures provide more information than they actually provide is another inferential problem we have to deal with. References "],["chapter-2-quiz.html", "Chapter 2: Quiz", " Chapter 2: Quiz Note: The pull-down menu for selecting an answer turns green after selecting the correct answer. Exercise 2.1 What does it mean if statistical assumptions are perfectly met? The results of the statistical analysis are true. The results of the statistical analysis are likely true. The evidence provided by the statistical analysis is stronger than if the assumptions were not met. Answer: 1 2 3 Exercise 2.2 Which one of the following is NOT a good way of checking the quality of a measurement? Reliability Validity Operationalisation Answer: 1 2 3 Exercise 2.3 What does it mean if a measure has low reliability? It means that reliability analysis has not been conducted It means that repeated applications under the same conditions lead to widely different outcomes It means that the level of noise in a measurement is low Answer: 1 2 3 Exercise 2.4 Which one of the following is NOT a measure of reliability? Test-retest reliability Inter-rater reliability Split-half reliability Criterion reliability Answer: 1 2 3 4 Exercise 2.5 Which one of these is NOT a synonym for a dependent variable? Result variable Outcome variable Response variable Criterion Answer: 1 2 3 4 Exercise 2.6 Why is randomisation important in experimental studies? It allows researchers to theoretically rule out confounders as cause for the effect observed in the dependent variable It allows researchers to run statistical analysis It allows researchers to correctly choose dependent and independent variables Answer: 1 2 3 Exercise 2.7 What is true about numbers? 1. Numbers can represent categorical information 2. Numbers can only represent numerical information 3. If we apply a mathematical operation to numbers, we can interpret the results independent of the meaning of the numbers Answer: 1 2 3 Exercise 2.8 Which one of these is NOT a type of a variable in a research design? 1. Dependent variable 2. Epistemic variable 3. Control variable Answer: 1 2 3 Exercise 2.9 What does it mean if the data is presented in a wide format? 1. The data of one participant only spans a single row 2. The data of one participant spans multiple rows 3. There is one row per observation Answer: 1 2 3 Exercise 2.10 What does it mean if a variable represents continuous numerical information? 1. A variable can in principle take on any real-valued (i.e., decimal) number 2. A certain difference or interval has the same meaning anywhere on the scale 3. Both answers are correct Answer: 1 2 3 "],["short-introduction-to-r-and-the-tidyverse.html", "Chapter 3 Short Introduction to R and the tidyverse 3.1 What is R? 3.2 Getting Started with R 3.3 A Base R Example Session 3.4 The tidyverse 3.5 Summary", " Chapter 3 Short Introduction to R and the tidyverse As empirical scientists our believes should be based on empirical evidence that comes in the form of data. Consequently, a central skill for researchers is the ability to process and analyse data. For this we will be using the statistical programming language R. 3.1 What is R? R is a comprehensive tool that enables the skilled user to perform all steps or tasks of a data analysis with it. For example, with R we can: Prepare data for analysis: read data that comes in pretty much any format, data manipulation, and data wrangling Explore data using summary statistics and graphical summaries: exploratory data analysis, descriptive statistics, and data visualisation Perform statistical analysis of the data: inferential statistics Communicate the results: publication-ready results graphics, research reports that combine narrative text and statistical results And much more such as: data simulations and advanced statistical methods, machine learning, interactive data visualisations, websites, books (such as the present one) On top of this incredible list of things that can be done with R, R is free software (sometimes also known as open source software). This means, not only is R completely free to download, install, and use, you are also free to inspect its source code or make changes to it (as long as you do not make your version of R un-free). Given the flexibility of all the things you can do with R, it is not completely surprising that it requires some effort to start with R. Especially if R is your first real experience of learning a programming language. The important thing to know for any new R user is that the beginning is hard for almost everyone (including the present author). So the most important message is to hang in there and keep on trying. In all likelihood, there will be at least some rather frustrating situations in your first weeks of interacting with R, but it will get better. I have taught R to many users with a great variety of backgrounds and experiences and many of them have struggled in some way in the beginning, but for anyone who kept up their hopes and continued to put in the time and effort their struggles were not in vain. You can learn R if you just do not give up and believe in yourself. Be assured I believe in you. Learning R is such an incredibly powerful skill that will surely have a positive effect on whatever comes later in your life, be it a career in Academia (i.e., at a university) or in the real world (as we academics like to call everything that is not a university job). 3.2 Getting Started with R With this somewhat scary introduction out of the way, the next important question is probably how do you get going with R? Providing a comprehensive answer to this question is beyond the scope of the present work. Instead, we will shortly provide pointers to other freely accessible resources that provide introductions for different levels of prior experiences. Before that, I feel it is important to highlight that the basic R software that performs the calculations and can be installed from CRAN, the Comprehensive R Archive Network, is particularly bare bone. Therefore, in addition to R I also recommend you install RStudio. RStudio is the most popular IDE  that is, integrated development environment  for R that makes using it quite a bit more comfortable. Also note that both R and RStudio need to be updated independently of each other. Especially R should be updated at least once per year from CRAN. So if you already have R installed on your computer and do not remember the last time you have updated, now is probably a good time to do so. I personally update R usually within a few weeks of a new version appearing. The following provides an overview of resources to get started with R with a brief explanations of what it offers: R for Psychological Science by Danielle Navarro. This is a series of websites that provide an introduction to the basic concepts of R and my recommended introductory resource. At the time of writing it is not yet complete (i.e., still in progress), but what is there is excellent. It will guide you through all the steps of getting to know R and learn the basics. As an introductory resource I recommend you check out: Part 1, the core toolkit covers R installation, variables and data types, scripts, packages, and basic programming concepts, such as loops, branches, and functions. I recommend you go through all sections of Part 1 as the first thing of getting started with R. Part 2, the working with data covers more complex data types, most importantly Rs central data type, the data.frame, and already provides an introduction to the tidyverse, which will also be used here. I recommend you at least go through the prelude and data types (pay special attention to dataframes). It discussed many of the issue also relevant for the present work. In case you prefer video introductions instead of reading text, Danielle Navarro has also published a series of awesome video lectures to some introductory topics. For getting started with R I recommend: Installing R: Downloading and installing R with specific videos for each operating system Project structure: This provides a great overview and introduction to a topic that is fundamental to working with computers beyond R: working with the file system. It covers naming files, file paths, folders, and related technical stuff that is very important when programming, but not often taught explicitly. MSc Conversion: R Research Methods book by Emily Nordmann is a book with accompanying video lectures. This book is aimed at a similar audience as the present book. However, the books focuses on a few concepts that we do not use here (e.g., RMarkdown, accessing R that runs on a server). The book is part of the psychTeachR book plus video series developed by the University of Glasgow that contains a few more interesting books such as Data Skills: psyTeachR Books which is very introductory. Learning Statistics with R also by Danielle Navarro (as you can see, I am a big fan of Danielles work). This is a completely free introductory book to statistics using R, which can be downloaded from its website (the currently available version is 0.6). Part II (i.e., Chapters 3 and 4) provides a gentle and comprehensive introduction to R for newcomers. From installing R and RStudio, navigating the console and RStudio windows, basic data types, reading in data and the file system, to the most important data types, data.frames, these roughly 70 pages (i.e., pp. 35 - 109) have you covered. Once you had a look at these two chapters, Chapter 8 is a great next step as it introduces R scripts. If you are completely new to statistics, Chapter 5 also provides a great introduction to other important concepts. One downside of this resource is that it comes in the form of a PDF and not a website, so cannot be read comfortably on all devices (but great for printing). Also note that R for Psychological Science is an updates version of this book, so I would probably start with that first. Finally, note that the present book has a somewhat different conceptual focus for introducing statistical tests and methods (i.e., especially compared to Part IV). If you prefer an introduction that has a stronger programming focus, I recommend the free book Hands-On Programming with R by Garrett Grolemund. Chapters 1 to 5 (i.e., pp. 1 - 99) also provide an introduction that starts with installing R in a gentle manner and then introduces the necessary basic concepts including the different data types and data.frames. To sum this long list up, as a minimum to get through this book it is recommended to go through all sections of Part 1 (especially chapters 1, 2, 3, 4, 5, 6, and 11) and at least the data.frames chapter (Chapter 13) of Part 2 of Danielle Navarros R for Psychological Science. If you are not yet super comfortable with the file system on your computer, I also recommend you check out Danielle Navarros video lecture on project structure. 3.3 A Base R Example Session As we have discussed in the previous chapter, the most important format of data representation is a tabular format with each column representing a single variable and typically one row per observation. Such data is represented in base R in a data.frame, the most important data format for our needs. We use the term base R here to refer to using R without any additional packages. Let us quickly recap how a data.frame in base R looks like and let us do some basic operations with it that also sets the stage for using R scripts. 3.3.1 Data Files, Scripts, and Working Directory In this chapter, we are mainly working with the data from Walasek and Stewart (2015) we have introduced in detail in Section 1.2.2. The data consists of two data files each representing a separate experiment we have discussed together so far as they are exact replications of another. In Walasek and Stewart (2015) these are reported as Experiments 1a and 1b. The original data files sent to me by Lukasz Walasek were Excel files (which can be found here: Experiment 1a and Experiment 1b). As base R does not allow to read in Excel files, I have opened the original data files of Walasek and Stewart (2015) in Excel and saved them as .csv files (i.e., comma separated files) which can be downloaded from here: Experiment 1a and Experiment 1b. I recommend you download these files and save them in some folder so you can access them from within R. To follow along with the upcoming R code, save the downloaded data files in a folder named data that is in your current R working directory. I also recommend you create a new empty R script and also save it in the working directory. For example, say you already have already created a folder for this book/class, lets assume this folder is called stats_r_intro-stuff and in some easy to find location (e.g., depending on your operating system the My Documents folder or your home folder ~). Lets assume you want this folder to be your working directory. Then, you create a new folder in this folder, called data (i.e., stats_r_intro-stuff/data), and download the two data files (Experiment 1a and Experiment 1b) into this folder. Next you create an empty R script in this folder for this chapter, say tidyverse-intro.R (i.e., it is now stats_r_intro-stuff/tidyverse-intro.R). You can now easily set your current R session to this folder as your working directory by opening tidyverse-intro.R in RStudio (if it is not yet opened) and the using the menu: Session - Set Working Directory - To Source File Location. This sets your working directory to the folder (i.e., location) in which thr currently active R script is located, which should be your stats_r_intro-stuff folder. Note, if you have been using the current R session already for some time, now is a good time to restart the session (not necessarily RStudio) by using the menu again: Session - Restart R. The workflow laid out in the previous paragraph represents my general recommendation for working with R at this stage. Have a folder that is your designated project folder. In this folder you have your R scripts as well as a sub folder for the data. Then you can open a script in RStudio and set the folder to be your working directory through the menu (i.e., Session - Set Working Directory - To Source File Location). You can then access the data files simply by accessing data/my-data.file.csv. Importantly, dont forget to restart the R session before starting something new through the menu: Session - Restart R. 3.3.2 Read Data and Inspecting it With this set-up steps out of the way, we can now load in the data from Experiment 1a using base Rs read.csv() function: df1a &lt;- read.csv(&quot;data/ws2015_exp1a.csv&quot;) This should read in the data file without any warnings or problems.26 If for some reason you fail to download the file or set the correct working directory, you can also try to read it directly from the internet as in the following code. But please only do that after you have tried downloading and dealing with the actual file. Handling data files and working directories is an important R skill you need to acquire. df1a &lt;- read.csv(&quot;https://github.com/singmann/stats_for_experiments/raw/master/data/ws2015_exp1a.csv&quot;) The first thing we usually want to do with a data.frame is to inspect its structure using the str() function. It lists all the variables, their data types, and the number of observations and variables. str(df1a) #&gt; &#39;data.frame&#39;: 22912 obs. of 6 variables: #&gt; $ subno : int 8 8 8 8 8 8 8 8 8 8 ... #&gt; $ loss : int 6 6 6 6 6 6 6 6 8 8 ... #&gt; $ gain : int 6 8 10 12 14 16 18 20 6 8 ... #&gt; $ response : chr &quot;accept&quot; &quot;accept&quot; &quot;accept&quot; &quot;accept&quot; ... #&gt; $ condition: num 20.2 20.2 20.2 20.2 20.2 20.2 20.2 20.2 20.2 20.2 ... #&gt; $ resp : int 1 1 1 1 1 1 1 1 0 1 ... In the present case we have over 20 thousand observations on six variables. Most of the variables are int which means integers; that is, numeric variables consisting of whole numbers (i.e., discrete values). We also have one num  that is, numeric  variable (i.e., numeric variable with decimal values), condition. In general, int and num variables are treated in the same way, as numeric variables, so there is hardly ever a reason to transform one into the other. Finally, condition is a chr or character variable. 3.3.3 Transforming Categorical Variables into Factors As discussed in Section 2.2, some of the numeric variables as well as condition are actually categorical variables, or Factors in R parlance. We can transform variables into Factors using the apptly named factor() function. We general use factor() instead of other methods (e.g., function as.factor()) because it allows us to specify the ordering of the factor levels and potentially other labels for the factor levels. Let us do so for three variables now, subno, response, and condition. For this we use the $ operator to access variables of a data.frame. After transforming the variables we take another look at the structure of the data.frame using str(). df1a$subno &lt;- factor(df1a$subno) df1a$response &lt;- factor(df1a$response, levels = c(&quot;reject&quot;, &quot;accept&quot;)) df1a$condition &lt;- factor(df1a$condition, levels = c(40.2, 20.2, 40.4, 20.4), labels = c(&quot;-$20/+$40&quot;, &quot;-$20/+$20&quot;, &quot;-$40/+$40&quot;, &quot;-$40/+$20&quot;)) str(df1a) #&gt; &#39;data.frame&#39;: 22912 obs. of 6 variables: #&gt; $ subno : Factor w/ 358 levels &quot;5&quot;,&quot;8&quot;,&quot;13&quot;,&quot;24&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... #&gt; $ loss : int 6 6 6 6 6 6 6 6 8 8 ... #&gt; $ gain : int 6 8 10 12 14 16 18 20 6 8 ... #&gt; $ response : Factor w/ 2 levels &quot;reject&quot;,&quot;accept&quot;: 2 2 2 2 2 2 2 2 1 2 ... #&gt; $ condition: Factor w/ 4 levels &quot;-$20/+$40&quot;,&quot;-$20/+$20&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... #&gt; $ resp : int 1 1 1 1 1 1 1 1 0 1 ... We see that after running these commands, we have as expected three Factors in the data. Let us take a look at each of these calls to factor() to understand why we call it in a different manner in each case. The call for subno only passes the variable (i.e., the df1a$subno vector) and no further arguments to factor(). As a consequence, the factor levels are ordered in an alpha-numerical increasing manner. The call for response specifies the ordering of the factor levels using the levels argument to which we have passed a vector of the levels, c(\"reject\", \"accept\") (i.e., remember, c() is the function for creating vectors of any kind, such as character vectors here). The reason we do this is that otherwise the factor levels would be alphabetically ordered and then accept would be the first level and reject the second level (as a comes before r in the alphabet). This would be inconsistent with resp, where 0 = reject and 1 = accept. The call for condition specifies both the levels through levels as well as new names for the factor levels using labels. The labels use the ordering (i.e., potential loss first, potential gain second) we have used throughout the book and which differs from the ordering of the original condition (i.e., it switches the ordering to the format we are used to now). For both arguments a vector is passed, with elements mapped by position (i.e., the new label for the first level, 40.2, is the first label, -$20/+$40). We again specify the ordering of factor levels here through levels to maintain the ordering we have used throughout with the condition that typically shows loss aversion, -$20/+$40 as first condition. If we had not done so, the first level would have been the -$20/+$20 condition (as 20.2 was the smallest number in the original vector). An interesting part of the three calls to the factor() function is that if you run it again, after you have already ran it, it will break the condition variable. If you try it out, you will see that all values of the condition variable change to NA if you run this call a second time. The reason for this is that the values passed through the levels argument need to be present in the variable. However, because we have replaced the original values with new labels, none of the original levels (i.e., c(40.2, 20.2, 40.4, 20.4)) is part of the variable any more. Consequently, all values are replaced by NA (i.e., not available which means missing data). To get the values back you need to reload the data using the read.csv() command from above and then you can run the factor() call again in a state in which the data was when you ran it for the first time. What this shows is that you cannot assume that running a piece of code twice gives the same output in all cases. The problem here is that the code changes the data itself (i.e., values of condition). However, the code also assumes certain values for condition. Because this assumption only holds the first time you run the code, but not the second time, the second calls breaks the results in somewhat unexpected ways. So what this means is that randomly re-running code can lead to unexpected results (which are called bugs in programming language parlance). Therefore, instead of re-running individually pieces of code it can often help to re-start at the top of a script and re-running everything in order to ensure all data is in the state you think it is (ideally after restarting the R session through Session - Restart R). One more tip when transforming variables into factors. It is often a bit annoying to type out the factor levels by hand, especially when it is more than say two. In this case, a handy trick to know is that you can get R to produce the c() call having all factor levels. You just need to make sure you are using the correct ordering. The heart of the trick is the dput() function which creates a text representation of an R output that can be copied from the console to your script. To use this for factors the basic structure of the call is dput(unique(df$variable)) which returns the c() call for all unique elements of a variable. If you want the elements ordered you can use dput(sort(unique(df$variable))). For example, to create the levels argument for the condition variable I initially executed dput(sort(unique(df1a$condition))) at the console which returns c(20.2, 20.4, 40.2, 40.4) for the original df1a data (i.e., before turning everything into Factors). I then just copy and pasted a bit in this vector to get the ordering right (admittedly, typing might have been faster then copying and pasting, but whatever). Another thing we often want to do when reading in the data is getting an overview of how it actually looks like. One way to do so that I do not recommend for data.frames is just typing the name of the variable into the console (or calling just the name from the script). What this means in R is that the object is printed which, for large data.frames, leads to hundreds or thousands of rows being printed until some printing limit is reached. An alternative is to just look at the first few rows using the head() function (which prints the first 6 rows per default): head(df1a) #&gt; subno loss gain response condition resp #&gt; 1 8 6 6 accept -$20/+$20 1 #&gt; 2 8 6 8 accept -$20/+$20 1 #&gt; 3 8 6 10 accept -$20/+$20 1 #&gt; 4 8 6 12 accept -$20/+$20 1 #&gt; 5 8 6 14 accept -$20/+$20 1 #&gt; 6 8 6 16 accept -$20/+$20 1 Alternatively, you can click on an object in the RStudio Environment pane which opens the data in a viewer pane. The equivalent R call is using View(df1a) which opens the same viewer. However, View() should only be used interactively at the console and not be in an R script as it requires user interaction beyond script and console (i.e., it opens the viewer). In other words, it is nice during development to get an overview of the data, but not for the final analysis script. 3.4 The tidyverse A popular alternative to base R is the tidyverse (Wickham et al. 2019), a selection of packages curated and in large parts developed by the RStudio company. The mastermind behind the tidyverse is Hadley Wickham, the RStudio chief scientist and maybe the one person that can be considered an R superstar.27 Most of the core tidyverse packages that will be introduced below, such as dplyr and ggplot2, are his developments (even though many other have contributed to those packages as well). The easiest way to access the tidyverse is by first installing the tidyverse package from CRAN using install.packages(). Note that it is a good idea to only run install.packages() at the console and do not put it into your R script. The reason for this is that you only want to execute install.packages() once per R installation or after updating R and not every time you run a script. install.packages(&quot;tidyverse&quot;) Then, you can load all tidyverse packages at once. This is something we will do at the top of pretty much all scripts we will be creating. library(&quot;tidyverse&quot;) #&gt; -- Attaching packages --------------------------------------- tidyverse 1.3.1 -- #&gt; v ggplot2 3.3.5 v purrr 0.3.4 #&gt; v tibble 3.1.2 v dplyr 1.0.7 #&gt; v tidyr 1.1.3 v stringr 1.4.0 #&gt; v readr 1.4.0 v forcats 0.5.1 #&gt; -- Conflicts ------------------------------------------ tidyverse_conflicts() -- #&gt; x dplyr::filter() masks stats::filter() #&gt; x dplyr::lag() masks stats::lag() When loading packages it is common that this produces some status or other messages in the console (i.e., some packages do, some dont). For example, tidyverse lists the package versions of the loaded (or attached) packages and lists function conflicts; that is, cases in which a tidyverse function masks a previous loaded function with the same name. We include these messages here to show that this is normal and nothing to worry about, but will generally not later in the book (as this would get repetitive). However, note that the exact version numbers of the packages that are shown here may differ from the version numbers you see (e.g., because some packages might update meanwhile). The core tidyverse packages are (with descriptions taken or adapted from the official websites): tibble: A modern version of the data.frame readr: Reading data in, the RStudio way. Data wrangling with magrittr, tidyr, and dplyr: Coherent set of functions for tidying, transforming, and working with rectangular data. Supersedes many base R functions and makes common problems easy. ggplot2: System for data visualization. This will be disucssed in the next chapter. purr and broom: Advanced modelling with the tidyverse and tidymodels which will not be discussed here. In the following we provide a short introduction to the core components of the tidyverse as they are needed for this book. A more comprehensive introduction is provided in the Wickham and Grolemund book R for Data Science which is available freely at http://r4ds.had.co.nz. To get a good grip on the tidyverse, I highly recommend working through chapters 1 to 21, or better 1 to 25 (the chapters in R for Data Science are a lot shorter than the chapters in the present book). 3.4.1 tibble and readr The back bone of the tidyverse is the tibble, or tbl_df (its class name in R), a modern reimagining of the data.frame, keeping what time has proven to be effective, and throwing out what is not (quote from tibble documentation). The main difference between data.frame and tibble in daily use is that tibbles do not try to print all rows and columns and thus do not overwhelm the console when just entering the name of the data at the console. This feature alone is enough for me to prefer tibbles over data.frames. When working on an analysis one often wants to have a quick look at the data to see what has happened and the easiest way to do that is often to just enter the name of the data into the console. With data.frames this regularly clutters the console and with tibbles it does not. To convert a data.frame into a tibble you can use the as_tibble() function. Let us do this for our existing data.frame of Walasek and Stewart (2015), df1a. tbl1a &lt;- as_tibble(df1a) tbl1a #&gt; # A tibble: 22,912 x 6 #&gt; subno loss gain response condition resp #&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 8 6 6 accept -$20/+$20 1 #&gt; 2 8 6 8 accept -$20/+$20 1 #&gt; 3 8 6 10 accept -$20/+$20 1 #&gt; 4 8 6 12 accept -$20/+$20 1 #&gt; 5 8 6 14 accept -$20/+$20 1 #&gt; 6 8 6 16 accept -$20/+$20 1 #&gt; # ... with 22,906 more rows In the output of the tibble we can see two things. First, the output only shows the first 6 rows. Compare this to what happens when typing df1a in the console which will print many more rows (so you cannot any more see the column names). Second, the data type of each column is shown below the column names. This is another feature that makes tibbles more convenient in everyday use. We directly see that for example subno is a Factor (or &lt;fct&gt;) as it should be. An alternative way to creating a tibble instead of a standard data.frame is by loading the data using the tidyverse specific read functions which are in the readr package. Remember that above we used the base R read function read.csv(), which is one of a handful of base R data read in functions such as read.table() (for reading data separated by spaces) and read.delim() (for reading tab separated data). readr offers a similar set of functions where the main differences are that instead of using a . in the function name they use an _ and instead of a data.frame they return a tibble. So in case you have a csv file, you could use read_csv() instead of read.csv(). Note that in this and in the following code chunks we are going to override the existing tbl1a object to not clutter the work space. tbl1a &lt;- read_csv(&quot;data/ws2015_exp1a.csv&quot;) #&gt; #&gt; -- Column specification -------------------------------------------------------- #&gt; cols( #&gt; subno = col_double(), #&gt; loss = col_double(), #&gt; gain = col_double(), #&gt; response = col_character(), #&gt; condition = col_double(), #&gt; resp = col_double() #&gt; ) tbl1a #&gt; # A tibble: 22,912 x 6 #&gt; subno loss gain response condition resp #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 8 6 6 accept 20.2 1 #&gt; 2 8 6 8 accept 20.2 1 #&gt; 3 8 6 10 accept 20.2 1 #&gt; 4 8 6 12 accept 20.2 1 #&gt; 5 8 6 14 accept 20.2 1 #&gt; 6 8 6 16 accept 20.2 1 #&gt; # ... with 22,906 more rows What we can see from the output is that read_csv() not only returns the tibble, but also a message showing the column specification (i.e., data type for each column). This message is actually a call to the cols() function that could be copied and modified to change the column specification when reading in the data. For example, if we wanted to change subno we could do so directly here: tbl1a &lt;- read_csv(&quot;data/ws2015_exp1a.csv&quot;, col_types = cols( subno = col_factor(), loss = col_double(), gain = col_double(), response = col_character(), condition = col_double(), resp = col_double() )) tbl1a #&gt; # A tibble: 22,912 x 6 #&gt; subno loss gain response condition resp #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 8 6 6 accept 20.2 1 #&gt; 2 8 6 8 accept 20.2 1 #&gt; 3 8 6 10 accept 20.2 1 #&gt; 4 8 6 12 accept 20.2 1 #&gt; 5 8 6 14 accept 20.2 1 #&gt; 6 8 6 16 accept 20.2 1 #&gt; # ... with 22,906 more rows We could do something similar also for response and condition, but one issue is that the function for defining Factors within readr, col_factor(), only allows a level argument and not a labels argument (see ?cols). So to get the same factor structure as above, we will need to use the factor() function in a similar way as above, which we will show below. Another feature of readr compared to the corresponding base R functions is that it is a bit more restrictive in some cases. That is, in case the data is not as expected by a particular read function, readr fails more often than base R. Whereas this can appear annoying while programming, it has the benefit that one learns of problems in the data early compared to late. In other words, readr function are more likely than base R functions to ensure the data looks like you expect it to look like. And if your data is more likely to look like you expect it to look like, your code will be less likely to produce incorrect results. Therefore, in addition to using read_csv() instead of read.csv(), it is often better to use read_table() instead of read.table() for space separated data (or read_table2() if the data format is a bit sloppy), read_tsv() instead of read.delim() for tab separated data, or read_delim() if you want to specify the data delimiter. 3.4.2 The Pipe: %&gt;% One of the coolest features and at the time novel features of the tidyverse is the pipe, which is the name of the %&gt;% operator.28 To understand what it does and why it is so great, it makes sense to begin by taking a step back and thinking a bit about functions and evaluation order in R. One of the most important features of R is that we can use the result (or output) from one operation and use it as the input of another operation. For example, remember that the loss column in the data of Walasek and Stewart (2015) shows the potential loss for each of the lotteries, these losses range from $6 to $40. Now imagine that for some reason we want to know the midpoint between minimum and maximum loss (which is $23 in the present case). One way to calculate those is by taking the mean of the minimum and maximum. To get this in R we would need to do two steps: get the minimum and maximum potential loss from the vector of losses and then taking the mean of the minimum and maximum. We can get the minimum and maximum of a vector using the range() function and the mean is obtained using the mean() function. So to do so in R for our vector of all potential losses, tbl1a$loss, we could do: # step 1: get minimum and maximum and save in temporary variables tmp &lt;- range(tbl1a$loss) tmp # print minimum and maximum, to check everything is okay #&gt; [1] 6 40 #step 2: calculate mean of minimum and maximum: mean(tmp) #&gt; [1] 23 This code above does this two step calculation explicitly and saves the results of the first step in a temporary variable we have called tmp. We could also combine both steps into one by passing the results of the first step to the mean() function: mean(range(tbl1a$loss)) #&gt; [1] 23 We first get the minimum and maximum (i.e., the range) and then pass this to the mean() function. As the mean function is the last operation we want it is the outermost call in this bit. In other words, in R code that does not use the pipe, code is executed from the inside to the outside. This can make it difficult to chain many operations after each other without the need to create temporary variables as we did in the first bit. In contrast to executing code from innermost to outermost, the pipe allows us to chain operations from left to right. We can start with the innermost call and pipe it to the next function using %&gt;%. For example, using the pipe we could do the following: tbl1a$loss %&gt;% range() %&gt;% mean() #&gt; [1] 23 This shows a typical pipe workflow. We start with our data, in the present case the tbl1a$loss vector. This data is piped to the first operation, the range() function. The output of this call is piped to the next operation, the mean() function. At the end, we get exactly the output we should get but with code that is easier to read. One feature of the pipe that makes the code particularly readable is that we can start a new line for each step in a chain. The pipe also works like any regular R operation, so we can easily save the result from the whole chain of operation in a new object for later use. For example: loss_midpoint &lt;- tbl1a$loss %&gt;% range() %&gt;% mean() loss_midpoint #&gt; [1] 23 To sum this up again, in base R the order of operations is from the innermost operation to the outermost operation. In contrast, with the pipe we can chain (or pipe) operations from left to right. The following image exemplifies this using a screenshot of a slide from Andrew Heiss. In the top part we see the operations me needs to execute in order to finally leave the house using base R (i.e., starting with the innermost wakeup() function). The coding style without the pipe makes it not easy to see what happens when and is generally difficult to read. The lower part shows the same operations using the pipe and we can see how much easier it is to see the logic from waking up, getting out of bed, and so forth. Figure 3.1: Comparison of execution order without pipe (upper part) or with pipe (lower part). From: https://evalsp21.classes.andrewheiss.com/projects/01_lab/slides/01_lab.html#116 In the following sections we will see how powerful piping can be. Especially when the pipe is combined with the dplyr functions that will be introduced next, we can do quite a lot of neat things relatively easily. Let us end this section with two more points that are important. First, typing the pipe (i.e., typing the characters %&gt;%) is annoying. If you are in RStudio you do not need to do it. Instead, there is a keyboard short cut for it, Ctrl/Cmd + Shift + M (i.e., on Windows/Linux Ctrl + Shift + M and in Mac Cmd + Shift + M). I highly recommend you get used to using this short cut. It is one of the few short cuts in RStudio I use regularly.29 Second, base R now has its own pipe, |&gt;. It works very similar to the tidyverse pipe (i.e., can in many situations used instead of the tidyverse pipe) and can be used without loading any packages. I am not using it here for the only reason that I have adapted the tidyverse pipe before there was a base R pipe and there does not seem to be many reason to prefer one over the other if the tiydverse is already loaded. So if you see |&gt; instead of %&gt;% somewhere, assume it does pretty much the exact same thing. 3.4.3 dplyr Whereas the back bone of the tidyverse are the tibble and the pipe, the package that most significantly improves ones workflow is dplyr. The core of dplyr are five functions that combine the most common data manipulation operations. Or to quote the official documentation: dplyr is a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges. These five verbs/functions are: mutate() adds new variables or changes variables as a function of existing variables select() picks variables based on their names filter() picks cases based on their values summarise() reduces multiple values down to a summary arrange() changes the ordering of the rows A great introduction and overview of the functionality of these functions is provided on the official getting started page, but we will do so here as well, if only somewhat briefly. One important part of the dplyr functions is that they all are developed to work with the pipe so can be chained. Furthermore, all dplyr functions work with what is called non-standard evaluation. This means we can refer to variable names of the data.frame/tibble we are working with in any of the dplyr functions without enclosing them in quotes. 3.4.3.1 mutate() Let us show this behaviour here by replicating the call above to convert some variables in our data to factors. We start again by reading the data in. We then pipe this freshly read in data to the mutate() function to convert three variables to factors as done above and save it again in the same object, tbl1a. We then get an overview of the object. tbl1a &lt;- read_csv(&quot;data/ws2015_exp1a.csv&quot;) tbl1a &lt;- tbl1a %&gt;% mutate( subno = factor(subno), response = factor(response, levels = c(&quot;reject&quot;, &quot;accept&quot;)), condition = factor(condition, levels = c(40.2, 20.2, 40.4, 20.4), labels = c(&quot;-$20/+$40&quot;, &quot;-$20/+$20&quot;, &quot;-$40/+$40&quot;, &quot;-$40/+$20&quot;)) ) tbl1a #&gt; # A tibble: 22,912 x 6 #&gt; subno loss gain response condition resp #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 8 6 6 accept -$20/+$20 1 #&gt; 2 8 6 8 accept -$20/+$20 1 #&gt; 3 8 6 10 accept -$20/+$20 1 #&gt; 4 8 6 12 accept -$20/+$20 1 #&gt; 5 8 6 14 accept -$20/+$20 1 #&gt; 6 8 6 16 accept -$20/+$20 1 #&gt; # ... with 22,906 more rows Compared to the code above in which we have changed the variables to factors, there a few changes: The block of code in which variables are converted to factors is wrapped into the mutate() call to which we have piped the data, tbl1a. For each call that converts one variable to a factor, we use the = sign and not the assignment operator &lt;-. The reason for this is that all the operations are performed within the mutate() call. We generally only use the assignment operator &lt;- outside of function calls (if you try it out and replace the = with &lt;- you will see which horrible consequences this has). The first two calls for changing a variable are ended by a ,. The reason again is that we are inside one mutate() call so to tell mutate we are not yet done, we end one operation with a ,. Instead of passing the full vector to factor() as above, for example df1a$subno, we can refer to each variable, such as subno, by name (and without quotes) directly (this is what is meant with non-standard evaluation). The reason for this is that we are working within the context of the data that we have piped to the mutate() call. 3.4.3.2 summarise() The second most important function from dplyr is summarise() which as the name suggests creates summaries. What this means is that we are usually reducing the number of rows in a data, often the data is reduced to one row. For example, we could use summarise() to get the average probability with which lotteries were accepted across all conditions and lotteries. For this we just need to take the overall mean of the resp variable. tbl1a %&gt;% summarise(mean_acc = mean(resp)) #&gt; # A tibble: 1 x 1 #&gt; mean_acc #&gt; &lt;dbl&gt; #&gt; 1 0.381 We see that summarise also returns a tibble so the return values differs quite a bit from how one could do it in base R (e.g., mean(tbl1a$resp)). We can also see that summarise() works structurally quite similarly to the mutate(). In the summarise() call we can create new variables using a name = operation syntax. We can also create multiple summary statistics by separating them using ,. We can also add comments to the code as usual using #, as long as it is to the right side of the ,. tbl1a %&gt;% summarise( mean_acc = mean(resp), sd_acc = sd(resp), ## sd() returns the standard deviation mean_pot_loss = mean(loss), mean_pot_gain = mean(gain) ) #&gt; # A tibble: 1 x 4 #&gt; mean_acc sd_acc mean_pot_loss mean_pot_gain #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.381 0.486 19.5 19.2 3.4.3.3 filter(), select(), and arrange() As we have discussed above, one of the key ways of using the tidyverse is by chaining different operations together. For example, we could first select only the \"-$20/+$40\" condition and then calculate the summaries for this condition. For this we pipe first to the filter() function and then pipe the results to the summarise function: tbl1a %&gt;% filter(condition == &quot;-$20/+$40&quot;) %&gt;% summarise( mean_acc = mean(resp), sd_acc = sd(resp), mean_pot_loss = mean(loss), mean_pot_gain = mean(gain) ) #&gt; # A tibble: 1 x 4 #&gt; mean_acc sd_acc mean_pot_loss mean_pot_gain #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.514 0.500 13 26 Note that filter() requires to use the == operator and not the = operator. The reason is that you do not want to set something equal to something, which is done using =, but want to check for equality, which is done using ==. filter() also allows you to chain multiple comparisons together, either by separating them using , or &amp; (i.e., logical and). If only one of multiple conditions needs to hold you can use | (logical or). filter() is also the function for removing observations in the tidyverse. This is done by writing a filter that retains all observations but the ones you want to remove. For this you can also use the logical not operator, ! which inverts a logical vector, or the unequal operator, !=. For example, we discussed that what we are actually interested in in the data from Walasek and Stewart (2015) are the responses to the symmetric lotteries for which the potential loss is equal to the potential gain. We could select those with filter(): symm1 &lt;- tbl1a %&gt;% filter(loss == gain) symm1 #&gt; # A tibble: 1,989 x 6 #&gt; subno loss gain response condition resp #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 8 6 6 accept -$20/+$20 1 #&gt; 2 8 8 8 accept -$20/+$20 1 #&gt; 3 8 10 10 accept -$20/+$20 1 #&gt; 4 8 12 12 accept -$20/+$20 1 #&gt; 5 8 14 14 accept -$20/+$20 1 #&gt; 6 8 16 16 accept -$20/+$20 1 #&gt; # ... with 1,983 more rows Or equivalently: symm2 &lt;- tbl1a %&gt;% filter(!(loss != gain)) all.equal(symm1, symm2) #&gt; [1] TRUE Here, we use the all.equal() function to show that the two objects are the same. This function can compare arbitrary R objects and only returns TRUE if two objects are the same. One problem with the filter is used above is that there are more symmetric lotteries than those that occur in all conditions. The following code, which shows all symmetric lotteries for each condition, demonstrates this. tbl1a %&gt;% filter(loss == gain) %&gt;% select(loss, gain, condition) %&gt;% unique() %&gt;% arrange(condition) %&gt;% print(n = Inf) #&gt; # A tibble: 22 x 3 #&gt; loss gain condition #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 12 12 -$20/+$40 #&gt; 2 16 16 -$20/+$40 #&gt; 3 20 20 -$20/+$40 #&gt; 4 6 6 -$20/+$20 #&gt; 5 8 8 -$20/+$20 #&gt; 6 10 10 -$20/+$20 #&gt; 7 12 12 -$20/+$20 #&gt; 8 14 14 -$20/+$20 #&gt; 9 16 16 -$20/+$20 #&gt; 10 18 18 -$20/+$20 #&gt; 11 20 20 -$20/+$20 #&gt; 12 12 12 -$40/+$40 #&gt; 13 16 16 -$40/+$40 #&gt; 14 20 20 -$40/+$40 #&gt; 15 24 24 -$40/+$40 #&gt; 16 28 28 -$40/+$40 #&gt; 17 32 32 -$40/+$40 #&gt; 18 36 36 -$40/+$40 #&gt; 19 40 40 -$40/+$40 #&gt; 20 12 12 -$40/+$20 #&gt; 21 16 16 -$40/+$20 #&gt; 22 20 20 -$40/+$20 In particular, the output shows that in the asymmetric conditions, -$20/+$40 and -$40/+$20, we only have three symmetric lotteries, whereas in the two symmetric conditions we have quite a lot more. However, the three symmetric lotteries that appear in the asymmetric conditions, 12-12, 16-16, and 20-20, appear in all conditions. Before focussing on how we can build a better filter, let us first see how the code above exactly did what it did. We again first filter all symmetric lotteries. From this data we then retain or select() only the loss, gain, and condition columns. We then use the unique() function to only retain the unique rows. Because we have a specific ordering in which we think about the conditions (which is expressed in the order of the factor levels) we then sort the tibble along the condition variable using arrange(). Finally, because per default a tibble only prints a few rows we use print(n = Inf) which for any tibble prints all rows (i.e., up to row infinity). To better understand the code, you are encouraged to run it line by line to see what the output at each intermediate step is (to get the output at a subset of lines of codes, do not select the %&gt;% at the end of the last line when running it). So how can we select only the symmetric lotteries that appear in all conditions? There clearly are multiple possible filters to do so. The one that easiest comes to my mind is to combine the filter that selects all symmetric lotteries with a filter that only selects lotteries in which the potential loss is either 12, 16, or 20. For the latter we introduce a new operator, the %in% operator. It returns TRUE for any element of a vector that matches an element in another vector. For example in our case, we want to retain all those lotteries where the potential loss is in c(12, 16, 20). This is shown below. We also include the code that shows that we now have the same set of symmetric lotteries in all conditions. tbl1a %&gt;% filter(loss == gain, loss %in% c(12, 16, 20)) %&gt;% select(loss, gain, condition) %&gt;% unique() %&gt;% arrange(condition) #&gt; # A tibble: 12 x 3 #&gt; loss gain condition #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 12 12 -$20/+$40 #&gt; 2 16 16 -$20/+$40 #&gt; 3 20 20 -$20/+$40 #&gt; 4 12 12 -$20/+$20 #&gt; 5 16 16 -$20/+$20 #&gt; 6 20 20 -$20/+$20 #&gt; # ... with 6 more rows 3.4.3.4 group_by() You might remember that the main hypothesis of Walasek and Stewart (2015) was that it is not the absolute value of a lottery, but the relative rank, that matters for whether or not participants think a lottery is attractive. The main evidence for this hypothesis was that the proportion with which the symmetric lotteries are accepted differs across the four conditions. How could we calculate this with the tiydverse? If we only apply what we know so far the only way to do so would be to split the data into four subsets using filter, and then apply summarise() to each of these subsets. Let us do this exemplary by only splitting the data into two subsets, one for the \"-\\$20/+\\$40\" condition that shows loss aversion and one for the -$40/+$20 condition that shows loss seeking. We have to combine this filter with the filter only selecting the symmetric lotteries. To make the logic easier, we separate these two steps into separate filter() calls (but we could also combine them into one). ## &quot;loss aversion&quot; condition: tbl1a %&gt;% filter(loss == gain, loss %in% c(12, 16, 20)) %&gt;% filter(condition == &quot;-$20/+$40&quot;) %&gt;% summarise(mean_acc = mean(resp)) #&gt; # A tibble: 1 x 1 #&gt; mean_acc #&gt; &lt;dbl&gt; #&gt; 1 0.167 ## &quot;loss seeking&quot; condition: tbl1a %&gt;% filter(loss == gain, loss %in% c(12, 16, 20)) %&gt;% filter(condition == &quot;-$40/+$20&quot;) %&gt;% summarise(mean_acc = mean(resp)) #&gt; # A tibble: 1 x 1 #&gt; mean_acc #&gt; &lt;dbl&gt; #&gt; 1 0.670 The results show the previously discussed pattern. Participants in the -$20/+$40 condition are around 50% less likely to accept the symmetric lotteries than participants in the -$40/+$20. Whereas this is great, the code is a bit verbose and clunky. For example, the first filter() call and the summarise() call are identical in both parts. So the question is: Is there not a better way to get these results? This is of course a rhetorical question and the answer is yes, there is a better way to get these results. The answer is the group_by() function, the function that gives dplyr its full power. To use group_by() you need to pass it one or multiple (categorical) grouping variables. group_by() then creates a grouped tibble with the consequence that if any of the further dplyr verbs are applied to this grouped tibble they are applied to each group separately. Lets show it for our example to see what this means in practice: tbl1a %&gt;% filter(loss == gain, loss %in% c(12, 16, 20)) %&gt;% group_by(condition) %&gt;% summarise(mean_acc = mean(resp)) #&gt; # A tibble: 4 x 2 #&gt; condition mean_acc #&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 -$20/+$40 0.167 #&gt; 2 -$20/+$20 0.462 #&gt; 3 -$40/+$40 0.437 #&gt; 4 -$40/+$20 0.670 We can see that the output now contains the average proportion to accept the symmetric lotteries for each of the four conditions separately. So instead of first splitting the data and then calculating the summary statistics (here: mean of resp column) for each split by hand, group_by() does exactly this internally. In other words, whenever we have a categorical variable in our data we can use group_by() to perform operations conditionally across the levels of the categorical variable. Given the ubiquity of categorical variables, at least in experiments the experimental factor is one, but other data sets also usually have at least one categorical variable, this is a huge time and effort saver. And whereas we have shown this here for summarise(), which is probably the most common use case, this also works for the other dplyr verbs for which the results can depend on other rows, mutate() and arrange(). In the example above, we have only passed one variable to group_by(), condition. We can also pass multiple variables. This will perform the operations conditional on the combination of both variables. For example, here we have selected exactly the lotteries with the three symmetric lotteries that are the same across conditions. With group_by() it is easy to get the average accept proportion separately for each of these symmetric lotteries per condition: tbl1a %&gt;% filter(loss == gain, loss %in% c(12, 16, 20)) %&gt;% group_by(condition, loss, gain) %&gt;% summarise(mean_acc = mean(resp)) #&gt; `summarise()` has grouped output by &#39;condition&#39;, &#39;loss&#39;. You can override using the `.groups` argument. #&gt; # A tibble: 12 x 4 #&gt; # Groups: condition, loss [12] #&gt; condition loss gain mean_acc #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 -$20/+$40 12 12 0.190 #&gt; 2 -$20/+$40 16 16 0.119 #&gt; 3 -$20/+$40 20 20 0.190 #&gt; 4 -$20/+$20 12 12 0.469 #&gt; 5 -$20/+$20 16 16 0.458 #&gt; 6 -$20/+$20 20 20 0.458 #&gt; # ... with 6 more rows Note that pretty much the same result would be obtained if only passing one of the gain/loss variable pair to group_by() here. Feel free to try it out. The results are not straight forward to interpret as they do not show a consistent pattern across conditions. For the symmetric conditions the difference in acceptance rate seems to be small. However, for the asymmetric conditions we see somewhat more noticeable differences that suggest that the 16-16 lottery may have somewhat lower acceptance rates than both the 12-12 and 20-20 lottery. However, as these are only descriptive results without any additional inferential statistical evidence, it is very much possible that this small differences within conditions reflects pure noise and should not be taken too seriously. Before moving to the next function, there is one more thing to discuss. As shown in the output above, summarise() still returns a grouped tibble in case one calls group_by() initially with more than one variable. This can be seen by both the status message summarise() has grouped output by  and because the output still shows # Groups: .... This means that if we were to do any further operations on this result it would still be performed grouped. To remove the grouping created by group_by() use the ungroup() function as shown next. tbl1a %&gt;% filter(loss == gain, loss %in% c(12, 16, 20)) %&gt;% group_by(condition, loss, gain) %&gt;% summarise(mean_acc = mean(resp)) %&gt;% ungroup() #&gt; `summarise()` has grouped output by &#39;condition&#39;, &#39;loss&#39;. You can override using the `.groups` argument. #&gt; # A tibble: 12 x 4 #&gt; condition loss gain mean_acc #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 -$20/+$40 12 12 0.190 #&gt; 2 -$20/+$40 16 16 0.119 #&gt; 3 -$20/+$40 20 20 0.190 #&gt; 4 -$20/+$20 12 12 0.469 #&gt; 5 -$20/+$20 16 16 0.458 #&gt; 6 -$20/+$20 20 20 0.458 #&gt; # ... with 6 more rows 3.4.3.5 Counting Another important functionality of dplyr is that it makes it easy to count observations using the count() or n() functions. Counting is one of the operations that seems somewhat unspectacular at the beginning, but is extremely important for understanding your data. For example, an important question we might have for our data is how many participants we have per condition or how many trials every participants worked on. Let us begin by looking at the number of trials per participant (and condition). There are two almost equivalent ways of doing this. Either using group_by() and summarise(n = n()) or using count(): tbl1a %&gt;% group_by(condition, subno) %&gt;% summarise(n = n()) #&gt; `summarise()` has grouped output by &#39;condition&#39;. You can override using the `.groups` argument. #&gt; # A tibble: 358 x 3 #&gt; # Groups: condition [4] #&gt; condition subno n #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 -$20/+$40 5 64 #&gt; 2 -$20/+$40 13 64 #&gt; 3 -$20/+$40 53 64 #&gt; 4 -$20/+$40 61 64 #&gt; 5 -$20/+$40 73 64 #&gt; 6 -$20/+$40 85 64 #&gt; # ... with 352 more rows tbl1a %&gt;% count(condition, subno) #&gt; # A tibble: 358 x 3 #&gt; condition subno n #&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 -$20/+$40 5 64 #&gt; 2 -$20/+$40 13 64 #&gt; 3 -$20/+$40 53 64 #&gt; 4 -$20/+$40 61 64 #&gt; 5 -$20/+$40 73 64 #&gt; 6 -$20/+$40 85 64 #&gt; # ... with 352 more rows We can see that the results are pretty much the same, the only difference is that group_by() retains the grouped tibble whereas count() does not. We can also see that the number of trials for the participants shown in the output is 64. This is exactly the number of trials we would expect from the description of the study in Section 1.2.2. However, so far we cannot be sure that this really holds for all participants. We expect this to be the case and it looks like it for a handful of participants shown, but does it really hold for everyone? When analysing data questions such as these regularly come to ones mind. One has an assumptions about how the data looks like, but one cannot be sure that these assumptions really hold. What if, for example, data collection did not finish for some participants and we only have partial data for those. The reality of analysing actual data is that often some assumptions one believes should hold do not actually hold in the data. So many things can go wrong and will go wrong during data collection that one important step of the analysis is to verify ones assumptions. Checking ones assumptions can provide some assurances that the results are not biased by a mismatch between assumptions and reality. So how can we check if everyone has exactly 64 trials? With the code above this can be done per group by extending it by another group_by() summarise() combination. This also allows us to count, in the same step, the number of participants for each condition. Let me show you how: tbl1a %&gt;% group_by(condition, subno) %&gt;% summarise(n = n()) %&gt;% group_by(condition) %&gt;% summarise( n_participants = n(), all_64 = all(n == 64) ) #&gt; `summarise()` has grouped output by &#39;condition&#39;. You can override using the `.groups` argument. #&gt; # A tibble: 4 x 3 #&gt; condition n_participants all_64 #&gt; &lt;fct&gt; &lt;int&gt; &lt;lgl&gt; #&gt; 1 -$20/+$40 84 TRUE #&gt; 2 -$20/+$20 96 TRUE #&gt; 3 -$40/+$40 87 TRUE #&gt; 4 -$40/+$20 91 TRUE So what exactly is happening here? After the first three lines (i.e., including the first summarise()) we create the once summarised tibble already shown in the previous code. This tibble has one row per participants with the n column given the number of trials for that participant. We then group this tibble again by condition to ensure the following results are shown separately for each conditions (which is not really necessary, as it is still grouped by condition, but it makes the code clearer). We then perform another summarise() which summarises the once summarised tibble on the level of condition. In this summarise() we perform two operations: We create a new variable n_participants, which contains the number of observations (i.e., number of participants) in the once summarised tibble. This is done as before using the n() function for counting observations. We create a new logical variable all_64, which is TRUE only if for all observations/participants within one condition the number of trials is 64. To fully understand the call creating this variable, we have to read it from the inside to the outside. The innermost call is n == 64. This creates a logical variable for each observation in the once summarised tibble that is TRUE if the number of trials (i.e., variable n) is 64 and FALSE otherwise. Then we pass this vector to the all() function which reduces a logical vector to a single logical variable. all() only returns TRUE is all elements of the vector are TRUE and FALSE otherwise. The results thus show two pieces of information per condition. First, we can see the number of participants per condition which is between 84 and 96. Second, for all conditions all participants have exactly 64 trials (i.e., the all_64 vector is TRUE for all conditions). 3.4.3.6 dplyr Summary We have seen that dplyr is a powerful tool for working with data.frames or tibbles. The five verbs, mutate(), summarise(), filter(), select(), and arrange(), each have a relatively intuitive syntax for solving one common data manipulation problem. The most commonly used verbs are mutate() for calculating new variables or changing existing variables for all observations and summarise() which calculates new variables and reduces the number of observations (usually to one). One very common use of summarise() is with the n() function which allows counting the number of observations. Furthermore, filter() is the tool one needs for selecting subsets of observations. The full power of dplyr comes from combining these functions with group_by(), which performs the operations conditional on one or several grouping variables. For example, one of the most important analysis steps is calculating summary statistics for each level of a factor or a combination of factor levels. This can be easily done in dplyr using the powerful group_by(factor1, factor2) %&gt;% summarise(...) combination. In addition to getting informative summaries, this should also be used regularly to check whether the data matches the assumptions one has about ones data. 3.5 Summary In this chapter we have begun with a list of resources that provide a general introduction to R. After this, we have provided a brief example of how to read in data and changes variables to factors with base R. The main functions introduced here were read.csv(), str(), factor(), and head(). For the factor() function we have also shown how we can use the different optional arguments, level and labels. If you have not yet done, maybe now is a good time to check out the internal R help system for factor() to see if your believe of what these arguments do matches their actual description. For this either type ?factor to the prompt or press the F1 key when the cursor is on the factor() function in RStudio. This brings up the help page (which exists for any R function) and you can read the description for each argument. Being able to read and understand R help pages is one of the most important R programming skills that one usually develops over time (i.e., dont be demoralised if you understand very little in the help page now). After the short recap of base R, we have provided a brief introduction to a few tidyverse packages, tibble, readr, and dplyr, and introduced the tidyverse pipe, %&gt;%. We have shown that the back bone of the tidyverse is the tibble, a modern variant of the base R data.frame. A tibble is also what is returned when using the readr function for reading in data, such as read_csv(). For the pipe we have seen that it changes the order in which we can write commands. Whereas for base R the order is from inside to outside (innermost function first), with the pipe we can write or pipe function from left to write. The pipe makes it easy to chain different operations together, which is otherwise difficult in base R (i.e., would lead to unreadable code). This often avoids the need to create intermediate variables and generally leads to more readable code. Finally, we have introduced the functionality of dplyr. For dplyr the most important functions are three verbs, mutate(), summarise(), and filter(), which each solve one specific data manipulation problem. mutate() creates/changes variables within the current data, summarise() creates new variables by summarising the current data, and filter() selects observations (i.e., reduces the number of rows). the powerful feature of dplyr is that these verbs can be combined with group_by(), which performs these operations conditional on one or multiple categorical variables. To help you get going with R and the tidyverse in particular, the next page offers a number of further examples and exercises. References "],["data-visualisation-with-ggplot2.html", "Chapter 4 Data Visualisation with ggplot2 4.1 First ggplot2 Example 4.2 Two Continuous Variables 4.3 Changing ggplot2 Theme for R Session 4.4 One Continuous and one Categorical Variable 4.5 Cheat Sheets", " Chapter 4 Data Visualisation with ggplot2 The perhaps most prominent member of the tidyverse, which technically also pre-dates the tidyverse by several years, is ggplot2, a system for declaratively creating graphics, based on the book The Grammar of Graphics (Wilkinson 1999). To understand what how ggplot2 works, it makes sense to contrast it to the base R graphics engine around the plot() function. For plot() creating a plot is done by drawing individual graphical elements such as points() or lines(). These functions generally only accept individual vectors or data points. In contrast, ggplot2 requires that the data is passed either as a data.frame or tibble. With this in hand, you provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details. (quote from the official documentation). library(&quot;tidyverse&quot;) #&gt; -- Attaching packages --------------------------------------- tidyverse 1.3.1 -- #&gt; v ggplot2 3.3.5 v purrr 0.3.4 #&gt; v tibble 3.1.2 v dplyr 1.0.7 #&gt; v tidyr 1.1.3 v stringr 1.4.0 #&gt; v readr 1.4.0 v forcats 0.5.1 #&gt; -- Conflicts ------------------------------------------ tidyverse_conflicts() -- #&gt; x dplyr::filter() masks stats::filter() #&gt; x dplyr::lag() masks stats::lag() tbl1a &lt;- read_csv(&quot;data/ws2015_exp1a.csv&quot;) #&gt; #&gt; -- Column specification -------------------------------------------------------- #&gt; cols( #&gt; subno = col_double(), #&gt; loss = col_double(), #&gt; gain = col_double(), #&gt; response = col_character(), #&gt; condition = col_double(), #&gt; resp = col_double() #&gt; ) tbl1a &lt;- tbl1a %&gt;% mutate( subno = factor(subno), response = factor(response, levels = c(&quot;reject&quot;, &quot;accept&quot;)), condition = factor(condition, levels = c(40.2, 20.2, 40.4, 20.4), labels = c(&quot;-$20/+$40&quot;, &quot;-$20/+$20&quot;, &quot;-$40/+$40&quot;, &quot;-$40/+$20&quot;)) ) glimpse(tbl1a) #&gt; Rows: 22,912 #&gt; Columns: 6 #&gt; $ subno &lt;fct&gt; 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ~ #&gt; $ loss &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 8, 8, 8, 10, 10, 10, ~ #&gt; $ gain &lt;dbl&gt; 6, 8, 10, 12, 14, 16, 18, 20, 6, 8, 10, 12, 14, 16, 18, 20, ~ #&gt; $ response &lt;fct&gt; accept, accept, accept, accept, accept, accept, accept, acce~ #&gt; $ condition &lt;fct&gt; -$20/+$20, -$20/+$20, -$20/+$20, -$20/+$20, -$20/+$20, -$20/~ #&gt; $ resp &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, ~ 4.1 First ggplot2 Example What this means in practice is the first thing to think about is which variables in your data you want to show on which of the two axes? Once you have decided on this you have to consider which graphical elements, which are called geoms in ggplot2 terminology, you want to use to show the data. Let us exemplify this with a first example from the Walasek and Stewart (2015) data. One intuition we could have is that the probability that a lottery is accepted is related to the size of the potential gain, if we average over all other variables (i.e., potential loss and condition). One way to investigate this would be to calculate the average acceptance probability for each possible gain and plot this on the x-axis against the average accept probability on the y-axis. For this, we first summarise the data in this way. lot_sum &lt;- tbl1a %&gt;% group_by(gain) %&gt;% summarise(mean_acceptance = mean(resp)) lot_sum #&gt; # A tibble: 13 x 2 #&gt; gain mean_acceptance #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 6 0.0943 #&gt; 2 8 0.138 #&gt; 3 10 0.214 #&gt; 4 12 0.222 #&gt; 5 14 0.340 #&gt; 6 16 0.326 #&gt; # ... with 7 more rows Looking at the returned tibble already shows that our intuition is probably not too off, but let us plot the data to get a better look. For this, we pass it to the ggplot() function as shown next. ggplot(lot_sum, aes(x = gain, y = mean_acceptance)) + geom_point() This plot shows a clear relationship between the mean acceptance rate of a lottery and its potential gain. Let us now describe the call in detail: To create a plot with ggplot2 we generally want to call the ggplot() function for which the first argument is the data we want to plot. Because the data is the first argument, we can also directly pipe into the ggplot() function as shown below. The second argument is the aes() function which is used to map variables in the data onto aesthetics. In this call we only consider two aesthetics, the x and y axes. These two are probably the most important aesthetics, but we will also use other aesthetics such as linetype, size, or shape of the data points. In general, an aesthetic is a feature of the plot which changes as a function of the values of the variable. After passing the data and specifying the aesthetics, we close the ggplot() call (i.e., close the opening parenthesis) and then add further arguments, which are passed as function calls, to the plot using +. The most important set of arguments to pass are geoms (or geometric objects). Here we pass geom_point() to indicate that we want to plot all points of the data. Another important thing with ggplot2 is that we cannot only use one geom, but multiple. For example, we could add a line on top connecting all the data points: ggplot(lot_sum, aes(x = gain, y = mean_acceptance)) + geom_point() + geom_line() One thing of note is that usually geom_line() requires the use of the groups aesthetic. And if all data points are connected, as in the present case, group should be 1 (i.e., geom_line(aes(group = 1))). So the following is equivalent in the present case, but is an idiom one should be aware of. ggplot(lot_sum, aes(x = gain, y = mean_acceptance)) + geom_point() + geom_line(aes(group = 1)) 4.2 Two Continuous Variables The examples above show a simple case of plotting two continuous variables, the dependent variable is shown on the y-axis and the independent variable (for this plot) on the x-axis. Let us now consider a few more cases of plotting two continuous variables, by extending the data we are looking at. In the plots above we have averaged over all possible potential losses when looking at the effect of gain on average acceptance rates. We will now plot the case in which we consider each individual lottery  that is, each unique combination of potential gain and loss  as one data point (i.e., we will not average over all potential losses for one gain). Additionally, to reduce the influence of the condition manipulation we will only consider the two symmetric condition in which the range for losses is equal to the range for gains. Let us begin the analysis by preparing the corresponding data. lot_sum2 &lt;- tbl1a %&gt;% filter(condition %in% c(&quot;-$20/+$20&quot;, &quot;-$40/+$40&quot;)) %&gt;% group_by(loss, gain) %&gt;% summarise(mean_acceptance = mean(resp)) #&gt; `summarise()` has grouped output by &#39;loss&#39;. You can override using the `.groups` argument. lot_sum2 #&gt; # A tibble: 119 x 3 #&gt; # Groups: loss [13] #&gt; loss gain mean_acceptance #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 6 6 0.510 #&gt; 2 6 8 0.615 #&gt; 3 6 10 0.729 #&gt; 4 6 12 0.844 #&gt; 5 6 14 0.833 #&gt; 6 6 16 0.906 #&gt; # ... with 113 more rows For our first plot, we begin with the same call as above and only change the data that is passed to the ggplot() function. ggplot(lot_sum2, aes(x = gain, y = mean_acceptance)) + geom_point() This plot is difficult to interpret. We see that there are some lotteries with very low acceptance rates between 0 and around 0.2 as well as another group with acceptance rates between around 0.3 and 0.85. In this plot it does not look as if a larger potential gain is associated with a larger mean acceptance rates. One possibility for these differing visual impressions might be that there is some overlap of data points that are near 0; that is, there is evidence for over-plotting. That means that in the current plot we cannot differentiate between one or multiple data points that share the same or approximately same x and y coordinates. One way to allow for this differentiation an see whether there potentially is over-plotting is through alpha blending (i.e., by choosing alpha &lt; 1). Alpha blending is a computer graphics effect that creates the visual impression of semi transparency and one of the most helpful techniques for learning about over-plotting. What this means is that with alpha blending overlapping points appear darker whereas non-overlapping points do not. For example, we can set alpha = 0.25 for geom_point() as shown below. If you are wondering why alpha = 0.25 and not any other value? The answer is trial-and-error. I just tried some value (I usually start with a value below 0.5) and the tried a few until I saw a plot that looked good to me. ggplot(lot_sum2, aes(x = gain, y = mean_acceptance)) + geom_point(alpha = 0.25) As you can see in the plot, there is some evidence for over-plotting. Especially for mean acceptance rates near 0 (and especially for low values of potential gains), we see several points clearly darker than the rest of the data points. However, the plot is still not very clear. For example, it is still difficult to judge how many points there are for the darker points. One possibility to further improve on this figure is by introducing some random jitter for the plotted points using geom_jitter(). Given that the points are all on discrete x-axis positions (i.e., the even whole numbers that act as potentially gains) it makes sense to only add a small amount of jitter on the x-axis. This is done by specifying the width argument to geom_jitter(), which requires a number specifying the amount of (horizontal) jitter in units of the x-axis. Trial-and-error led me to conclude that width = 0.4 produces an appealing result.30 With jitter only on the x-axis the points retain their exact y-axis positions but are still shown around their original x-axis position. ggplot(lot_sum2, aes(x = gain, y = mean_acceptance)) + geom_jitter(width = 0.4, alpha = 0.25) An important thing to know about using geom_jitter() is that due to the randomness that is used to add the jitter, this plot will look slightly different every time it is created (i.e., the code above executed). Try it out a few times to see what I mean. And while you are at it feel free to try out a few different values for the amount of jitter or what happens if you remove width = 0.4 from the geom_jitter() call. You can also try and see what happens of you add some value for height (e.g., height = 0.05). The plot above that combines jitter and alpha blending makes it easier to see why we see such an effect of potential gain on mean acceptance rates when averaging over losses. There are many more data points with very low mean acceptance rates on the left side of the plot and many more data points with medium to high acceptance rates on the right side of the plot. If we imagine taking the mean of this points we can imagine a monotonically increasing mean acceptance rate. One question that this plot leaves open is the question of why we see this qualitatively different pattern for mean acceptance rates of lotteries. Why are some so low and other larger? To elucidate this question we are going to create a new factor, ev, which separates the expected value of the lottery (i.e., loss plus gain) into three bins: expected value = 0 (i.e., symmetric lotteries), negative expected value (i.e., potential loss larger than potential gain), and positive expected value (potential loss smaller than potential gain). To create the ev variable, we use another dplyr function, case_when(). case_when() is a vectorised variant of multiple branching (i.e., if-else). This allows to create a new variable based on multiple logical conditions in a convenient way. Each argument to case_when() consists of a logical statement, the ~ operator (which I call the tilde-operator), and a return value that is returned in case the logical condition is true. Here we have three logical cases which are each mapped onto one label describing the sign of the expected value. We also convert the ev variable into a Factor using factor(). lot_sum2 &lt;- lot_sum2 %&gt;% mutate(ev = factor(case_when( gain == loss ~ &quot;EV = 0&quot;, gain &lt; loss ~ &quot;EV negative&quot;, gain &gt; loss ~ &quot;EV positive&quot; ))) lot_sum2 #&gt; # A tibble: 119 x 4 #&gt; # Groups: loss [13] #&gt; loss gain mean_acceptance ev #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; #&gt; 1 6 6 0.510 EV = 0 #&gt; 2 6 8 0.615 EV positive #&gt; 3 6 10 0.729 EV positive #&gt; 4 6 12 0.844 EV positive #&gt; 5 6 14 0.833 EV positive #&gt; 6 6 16 0.906 EV positive #&gt; # ... with 113 more rows We can then use the new ev variable to print points in a different colour depending on the value of the ev variable. For this we just need to map the ev variable to the colour aesthetic in the aes() call and get a very interesting plot. ggplot(lot_sum2, aes(x = gain, y = mean_acceptance, colour = ev)) + geom_jitter(width = 0.25, alpha = 0.5) And whereas this plot already looks very interesting, it uses both green and red colours which can be difficult to see for individuals with colour blindness (such as me, even if my colour blindness is very mild). Therefore, we can make this plot nicer in two regards: (1) by using a somewhat nicer theme than the default which removes the grey background (e.g., my favourite is theme_bw()) and (2) we can use a colour blind friendly colour palette. For this we use ggthemes::scale_colour_colorblind() (here ggthemes:: just means we use a a function from the ggthemes package without loading it explicitly beforehand). This gives us a very appealing and interesting plot. ggplot(lot_sum2, aes(x = gain, y = mean_acceptance, colour = ev)) + geom_jitter(width = 0.25, alpha = 0.5) + ggthemes::scale_colour_colorblind() + theme_bw() This plot shows that if the expected value is negative, participants are highly unlikely to accept a lottery. If the expected value is 0 (i.e., for symmetric lotteries) the acceptance rates start out at around 0.5 for small potentials losses and gains and decrease a bit with increasing loss/gain (just as predicted by loss aversion, see Section 1.2.1). Finally, for lotteries with positive expected value (i.e., where on average we should win money) acceptance rate are at or above 0.5. Just as one would expect. Are we done yet with this data? Not yet. There is one more trick we can use to make the plot even more informative. We can map the loss variable onto the size of the data points. For this, we just add size = loss to the aes() call. This allows us to further understand the pattern. ggplot(lot_sum2, aes(x = gain, y = mean_acceptance, colour = ev, size = loss)) + geom_jitter(width = 0.25, alpha = 0.5) + ggthemes::scale_colour_colorblind() + theme_bw() We can now see something that should not be too surprising. If the potential loss is small, which is now indicated by a small data point, the average acceptance rate is high. And the higher the potential gain for one specific potential loss (i.e., point size) the larger the mean acceptance rate. For each x-axis position on which we have data, we can also nicely see a size based ordering: The small losses have the largest mean acceptance rates and the large losses the smallest. In sum, even though the plot reveals a few pattern that should not be too surprising, it provides a comprehensive overview of the data of Walasek and Stewart (2015) (at least the two symmetric conditions). 4.3 Changing ggplot2 Theme for R Session So far we have looked at two continuous variables and seen how by mapping different variables to different aesthetics and by changing the arguments to the aesthetics, we can create appealing figures. Below we are trying to do the same for a situation that is potentially more common for experimental data, a situation with one continuous dependent variable and one categorical independent variable. Before producing the data, we are going to do something we will continue to do throughout this book. We will change the default theme (i.e., the theme that will be automatically used by all plots in one R session unless a different theme is specifically requested) to a theme without the grey background. In this call, we also make a few more changes such as larger axes labels and remove unnecessary grid lines which I feel makes such plots generally nicer. theme_set(theme_bw(base_size = 15) + theme(legend.position=&quot;bottom&quot;, panel.grid.major.x = element_blank())) Note, this is a global options that will affect all plots created after executing this command within an R session (i.e., to reset to the default theme you would have to restart your R session). 4.4 One Continuous and one Categorical Variable Before creating plots with one continuous dependent variable and one categorical independent variable, we need to create the corresponding data from. In the plots above we have aggregated the data of Walasek and Stewart (2015) on the level of the individual lotteries (i.e., unique combination of gains and losses). Whereas this made sense for the plots above, it ignored the different participants (i.e., we aggregated across participants), which are usually considered a major source of noise. In other words, the plots above focussed on differences across lotteries but ignored differences across participants. However, as we have have discussed before, different people can do what they do for a number of different reasons, so we know that people differ quite a bit; we expect individual differences. What this means that it usually is a good idea to show the distribution of responses over participants, which provides a visual impression of the level of noise in the data. Therefore, instead of aggregating across participants we will create a new data set for which first calculate one score per participant and condition. Then, we will plot the distribution across participants for each condition. We begin by creating this data for all lotteries. Whereas this might not be the most informative plot substantively (e.g., compared to only plotting the shared symmetric lotteries), it is more instructive in the present case as our main goal is to try out a few different plotting options. In a later section we will produce a plot of only the shared lotteries that will be more interesting for the research question of loss aversion. part_sum &lt;- tbl1a %&gt;% group_by(condition, subno) %&gt;% # aggregate for both, condition and subno summarise(mean_acc = mean(resp)) #&gt; `summarise()` has grouped output by &#39;condition&#39;. You can override using the `.groups` argument. part_sum #&gt; # A tibble: 358 x 3 #&gt; # Groups: condition [4] #&gt; condition subno mean_acc #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 -$20/+$40 5 0.562 #&gt; 2 -$20/+$40 13 0.5 #&gt; 3 -$20/+$40 53 0.344 #&gt; 4 -$20/+$40 61 0.484 #&gt; 5 -$20/+$40 73 0.266 #&gt; 6 -$20/+$40 85 0.375 #&gt; # ... with 352 more rows The output shows that the individual mean acceptance rates already for the first few participants in one condition are quite variable. It also shows that the returned tibble is still grouped (See message and Groups ...):. If we were to perform other operations with it, instead of just plotting, it might be necessary to add an ungroup() as a call to the pipe to remove the grouping before performing other operations. However, as we just pass it to the ggplot() function, this is not necessary here. 4.4.1 Displaying All Data Points As in the plot above and is a general convention, we plot the dependent variable, the average acceptance rate per participant, on the y-axis and the independent variable, experimental condition, on the x-axis. As above we begin by plotting the individual data point as points using the point geom (i.e., geom_point()). ggplot(part_sum, aes(x = condition, y = mean_acc)) + geom_point() The resulting plot is a bit difficult to interpret. We can see that there seems to be some differences between conditions (e.g., less large mean acceptance rates in the two right-most conditions), but it is difficult to judge if points are on top of each other  that is, whether there is over-plotting or not. Because of this, it is difficult to adequately perceive the distribution of points. We can begin as above and add jitter to the points using geom_jitter() and also use alpha blending (i.e., alpha = 0.25). In contrast to above, let us start without specifying any amount of horizontal (i.e., width) or vertical (i.e., height) jitter. In this case, geom_jitter() automatically adds both horizontal and vertical jitter. ggplot(part_sum, aes(x = condition, y = mean_acc)) + geom_jitter(alpha = 0.25) The resulting plot is not very visually appealing. The amount of horizontal jitter is too large making it difficult to see to which x-axis position (i.e., experimental condition) a point belongs to. To improve the visual impression it is better to choose the amount of jitter by hand. To do so, we need to choose the amount of jitter in x-axis units and pass it to the width argument. The problem for this is that the x-axis shows a categorical variable which does not have a easily identifiable unit. The solution to this problem is that for a categorical variable each x-axis position is shown at one whole number starting at 1 with a difference of 1 between levels of the categorical variable. That means, in the plot above the actual x-axis positions are 1, 2, 3, and 4, corresponding to the four factor levels. Thus, let us try to improve the plot by specifying an amount of horizontal jitter, such as width = 0.2 (which again was found by trial-and-error) and no height (which implies height = 0). ggplot(part_sum, aes(x = condition, y = mean_acc)) + geom_jitter(width = 0.2, alpha = 0.25) This plot provides a better overview of the distribution. We can for example clearly see that only for the -$40/+$20 (i.e., right-most) condition the majority of data points is below 0.25. 4.4.2 Bee Swarm Plot An alternative to geom_jitter() in such a case that avoids overplotting is offered by the two geoms in the ggbeeswarm package, geom_beeswarm() and geom_quasirandom(). The choice between both geoms usually depends on the amount of data points and overlap of points. Here to me the structure provided by geom_beeswarm() seems slightly nicer, but please try geom_quasirandom() as well (which as the name suggests uses randomness as well). To use both geoms, we first need to load the ggbeeswarm package (and if it is not yet installed, do so first through install.packages()). library(&quot;ggbeeswarm&quot;) ggplot(part_sum, aes(x = condition, y = mean_acc)) + geom_beeswarm() What geom_beeswarm() does is produce a bee swarm plot; that is, a plot in which over-plotted points are displaced so they are shown adjacent or next to each other (in this case on the x-axis). This allows some interesting conclusions for our data. For example, for the -$20/+$40 condition the distribution is very wide with some data points along the whole range and almost no part with a particular bump in the shape or cluster of the data. In contrast, the other condition have a more or less clearly identifiable centre with a cluster of data points (around 0.5 for the two symmetric distributions and near the bottom for the -$40/+$20 condition). 4.4.3 Box Plot Alternatives for visualising the distribution of data points are geoms that do not plot each data point, but a summary of the distribution. The most popular of these are the box plot and the violin plot, geom_boxplot() and geom_violin(). Box plots, also known as a box and whiskers plot, visualise a distribution through several summary statistics plus showing potential outliers. This provides a compact summary of the data that can also be used in case of many data points. Let us show how it looks for our data, before explaining the visual elements in detail. ggplot(part_sum, aes(x = condition, y = mean_acc)) + geom_boxplot() We can see that each box plot consists of a box, with a thicker line somewhere inside, as well as two whiskers. In some cases, we see additional data points outside the whiskers, these are the outliers. The thick line inside the box is a measure of the central tendency of the data (i.e., a measure of the centre of the distribution). Here it shows the median, which is the most common choice for box plots (sometimes box plots also show the mean as the measure of central tendency). The median is the value that separates or cuts the distribution of points into a lower and upper half. Technically, this means the median is the 50% quantile, the data points for which 50% of data points are smaller and 50% of points are larger.31 The upper and lower bound of the box, the two hinges, show the 25% and 75% quantiles of the data. That is the data point for which either 25% or 75% of data points are smaller. As a consequence the box encompasses 50% of all data points. This allows one to judge where most of the data is. For example, in line with the earlier visual impression that the -$20/+$40 spans the whole range of the scale, the box encompassing 50% of the data also spans roughly 50% of the scale. For the other conditions with a more clearly defined centre, the box is noticeable smaller. The two whiskers on both ends of the box (or hinges) extend from the hinges to the largest value no further than 1.5 times the size of the box (i.e., 75% quantile - 25% quantile, which is known as the interquartile range). The idea of the whiskers is that they represent, in some sense, the typical range of a distribution and data points that are outside this typical range can be considered un-typical. Such un-typical data points are often called outliers, but it is unclear in what sense this terminology is appropriate. For example, in the present case we have quite a few data points per condition (between 80 and 90) so it does not seem unlikely to observe some cases that look somewhat different from the other cases (i.e., un-typical). The issue of what to do with outliers such as those identified by a box plot is not trivial. If the outlier is a genuine response of a participant then simply removing it seems not appropriate. More specifically, omitting data because it does not fit with our idea of the data can be seen as an instance of data fabrication. On the other hand, if a single response has an extraordinary influence on the results (e.g., a single data point is so far away from all the others that is drives an observed effect) this also is problematic. We usually want that our results represent the data overall and not just a single observation. Thus, as with many situations in an analysis, how to deal with outliers depends on the specific context and situation. A generally reasonable strategy is to see if the qualitative pattern of results changes whether extreme data points are included or not. If not, this shows that they do not have an extraordinary influence on the results. 4.4.4 Violin Plots Another possibility for visualising a distribution is through its shapes and not only through summary statistics. One popular way of doing so is through a violin plot. To create a violin plot with ggplot2, we simply need to change the geom to geom_violin(). ggplot(part_sum, aes(x = condition, y = mean_acc)) + geom_violin() As is clear from the plot, a violin plot makes it easy to see whether a distribution is relatively flat, like for the -$20/+$40 condition, or has one or multiple bumps, which we would call modi in statistical terminology (with the singular being modus). The distributions for the other three conditions could all be called unimodal. For these three conditions, the modi are located where we would expect them to be, given the previous plots, around 0.5 for the symmetric conditions and near the bottom for the -$40/+$20 condition (this is of course not surprising but has to be the case, the violin plot is just a different visualisation compared to the bee swarm plot above). If a distribution had two bumps we would call it bimodal, but this does not appear to be justified for the distributions shown here. One way to increase the amount of information shown in a violin plot is by adding lines that correspond to different quantiles of the distribution. For example, to add the 25%, the 50% (i.e., median), and 75% quantile as in the box plot we could do as follows: ggplot(part_sum, aes(x = condition, y = mean_acc)) + geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) 4.4.5 Plotting The Mean If you are already comfortable with the scientific literature you might know that plots of one one continuous dependent variable and one categorical independent variable are quite common. Given that this is the structure of a basic experiment, this is probably the most common type of data visualisation in the scientific literature. If you already know that, you might have also noticed that the plots we have created so far diverge quite considerably from the plots you find in the literature. In particular, many published plots do not show a visualisation of the complete distribution of the data, as we have done above, but focus on one particular summary statistic of the data, the condition means. For example, one common plot type is the so-called dynamite plot in which the means are shown in terms of a bar graph. In addition, an error bar is added to the mean as a measure of uncertainty for the mean (we will provide a more thorough explanation of error bars in later chapters). An example for the present data looks like the following. #&gt; Warning: `fun.y` is deprecated. Use `fun` instead. #&gt; Warning: Ignoring unknown aesthetics: width As can be seen, the bar graph together with the error bar looks like a cartoon dynamite plunger, which is how the plot got its name. Even though this can be created in R, I am hiding the code here so you do not learn how to create it. As you can maybe read between the lines, I am not a big fan of this type of plot and so are many other people interested in statistics (e.g., the Vanderbilt biostatistics department). So what is the problem with dynamite plots? Clearly the issue is not that the plot shows the means. If you remember the discussion of results of a particular study in the previous Chapter 1, the discussion of results always focussed on the conditions means (e.g., the mean acceptance rate of symmetric lotteries was 21% in the -$20/+$40 condition, but 71% in the -$40/+$20 condition). SO clearly the means are very important. As we will see in the coming chapters, there is a statistical reason for that so it makes sense to say the mean generally is the most important summary statistic of the data. The problem with the dynamite plot is not that it shows the mean, but that it only shows the mean.32 One and the same mean can come from many different data distributions. The best example of this problem is shown in the datasaurus plot below (Matejka and Fitzmaurice 2017). Note that this plot shows two continuous variables and not a continuous plus categorical variable, but it nicely exemplifies the point made here. What the datasaurus plot shows is data that dramatically changes its shape, it cycles through 13 qualitatively different patterns one of which is a dinosaur, while maintaining its means plus other summary statistics (up to two decimal points) on both x-axis and y-axis. Figure 4.1: The datasaurus dozen. Two variables that maintain their summary statistics while dramatic changing the shape of the data points. From Justin Matejka and George Fitzmaurice: https://www.autodesk.com/research/publications/same-stats-different-graphs What the datasaurus impressively shows is that if one just focuses on the means or other summary statistics, there is a high chance one can miss important features in the data. Consequently, a better approach for plotting ones data is by combining a visualisation of the full distribution of the data with the mean. This is the approach we will be using throughout this book. Another benefit of showing a visualisation of the full distribution of the data instead of just the means is that it provides a more realistic picture of the noise in the data. Remember that we said that one goal of statistics is to help us distinguishing what is signal and what is noise. If we only focus on the means, even if it includes a measure of the uncertainty of the means through the error bars, we can forget the actual level of noise in the data. By ignoring the actual level of noise, we may be inclined to draw overly optimistic conclusions from our data. The problem with such overly optimistic conclusions is that they are less likely to be correct or true than appropriate conclusions. Combining a visualisation of the distribution of the data with the mean in ggplot2 can be achieved by adding a call to the stat_summary() function to the plot. This allows us to add a summary geom to the plot, which without additional arguments adds the mean of the data plus an error bar. Let us add this call to our bee swarm plot from above and see what happens. ggplot(part_sum, aes(x = condition, y = mean_acc)) + geom_beeswarm() + stat_summary() #&gt; No summary function supplied, defaulting to `mean_se()` We can see that just doing this does not lead to a dramatically different plot. The most noticeable difference is the status message telling us that the default summary function, mean_se(), was used which is what we want (so we can ignore this message). The problem with this plot is that we add the black summary point on top of the black data points. One way to improve this plot is by plotting the data points in the background in a semi-transparent manner using alpha = 0.2. And alternative would be to not change the geom_beeswarm() plot, but the stat_summary() plot (e.g., by passing colour = \"red\"), but this is left as an exercise to the reader. ggplot(part_sum, aes(x = condition, y = mean_acc)) + geom_beeswarm(alpha = 0.2) + stat_summary() #&gt; No summary function supplied, defaulting to `mean_se()` This plot now makes it easy to see both, the full distribution of the data as well as the mean (plus error bar). As said above, we ignore the technical meaning of the error bars until later and for now accept that they represent the uncertainty we have in the means. The plot allows us an interesting conclusion with regards to the distribution and the mean. For the three conditions that have a clearly visible mode (or bump; i.e., all but the -$20/+$40 condition), the mode differs noticeably from the mean. The reason for this is that the distributions are asymmetric around the mode with a long tail towards one end of the distribution. Such a long tail pushes the mean away from the mode towards the tail as can be seen here. What this means is that any statistical analysis that only focuses on the mean (which are all statistical analyses discussed in this book) provides in some sense an imperfect picture of the data. Whereas the mean of course represents the average of all values (because it is defined that way) it might not always represent the most typical value of a distribution (if we want to understand a typical value as one near the mode). The message from the previous paragraph is not that in a case with a rather asymmetric distribution focussing on the mean as a summary statistic is necessarily wrong, because it is not wrong. The message is something else: If we only focus the mean and do not show the actual data, we miss out on the nuances that real data usually have. Of course it is interesting to see what happens to the mean, it is the most important summary statistic (and in the absence of additional information the best prediction for a new observation from data is the mean, even if it is an asymmetric distribution). However, if the mean is not very typical for the distribution because of an asymmetric shape, this is an important result in addition to whatever happens to the mean. To fully understand the evidence provided by a data set, we have to understand the data fully, including its level of noise, peculiarities, and nuances. Therefore, always plot the full distribution when plotting the mean. only the symmetric lotteries that are shared across conditions separately for each participant and condition. part_sum &lt;- tbl1a %&gt;% filter(loss == gain, loss %in% c(12, 16, 20)) %&gt;% group_by(condition, subno) %&gt;% summarise(mean_acc = mean(resp)) %&gt;% ungroup() #&gt; `summarise()` has grouped output by &#39;condition&#39;. You can override using the `.groups` argument. part_sum #&gt; # A tibble: 358 x 3 #&gt; condition subno mean_acc #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; #&gt; 1 -$20/+$40 5 0 #&gt; 2 -$20/+$40 13 0 #&gt; 3 -$20/+$40 53 0 #&gt; 4 -$20/+$40 61 0 #&gt; 5 -$20/+$40 73 0.667 #&gt; 6 -$20/+$40 85 0.667 #&gt; # ... with 352 more rows ggplot(part_sum, aes(x = condition, y = mean_acc)) + geom_point() ggplot(part_sum, aes(x = condition, y = mean_acc)) + geom_jitter(width = 0.2, height = 0.1, alpha = 0.25) ggplot(part_sum, aes(x = condition, y = mean_acc)) + geom_jitter(width = 0.2, height = 0.1, alpha = 0.25) 4.4.6 One Continuous Variables: Histograms 4.5 Cheat Sheets References "],["standard1.html", "Chapter 5 The Standard Approach for One Independent Variable 5.1 Example Data: Note Taking Experiment 5.2 The Logic of Inferential Statistics 5.3 The Basic Statistical Model 5.4 Estimating the Statistical Model in R 5.5 Summary", " Chapter 5 The Standard Approach for One Independent Variable In this chapter we are introducing the standard statistical approach for analysing experimental data with one independent variable (i.e., one factor). The simple case for this is a study comparing two experimental conditions on one dependent variable. We will exemplify the standard approach for this design using a recent and straightforward experiment. 5.1 Example Data: Note Taking Experiment Heather Urry and 87 of her undergraduate and graduate students (Urry et al. 2021) (yes, all 87 students are co-authors!) compared the effectiveness of taking notes on a laptop versus longhand (i.e., pen and paper) for learning from lectures. 142 participants (which differed from the 88 authors) first viewed one of several 15 minutes lectures (TED talks) during which they were asked to take notes either on a laptop or with pen and paper. As this was a proper experiment, participants were randomly assigned to either the laptop (\\(N = 68\\)) or longhand condition (\\(N = 74\\)). After a 30 minutes delay, participants were quizzed on the content of the lecture. The answers from each participant were then independently rated from several raters (which agreed very strongly with each other) using a standardised scoring key resulting in one memory score per participant representing the percentage of information remembered ranging from 0 (= no memory) to 100 (= perfect memory).33 Figure 5.1 below shows the memory scores across both note taking conditions. Figure 5.1: Distribution of memory scores from Urry et al. (2021) across the two note taking conditions. In Figure 5.1, each black point shows the memory score of one participant so the full distribution of the data is visible. The shape of the distribution is also shown via a violin plot (i.e., the black outline around the points) to which we have added three lines representing three summary statistics of the data. From top to bottom these lines are the 75% quantile, the 50% quantile (i.e., the median), and the 25% quantile. The red points show the mean and the associated error bars show the standard error of the mean34. We see that the two means are quite similar, although the mean in the laptop condition is slightly larger, by 2.0 points (mean laptop = 68.2, mean longhand = 66.2). 5.2 The Logic of Inferential Statistics The previous paragraph provide us with descriptive statistics describing the results in the experiment by Urry et al. (2021): There is a memory difference of 2.0 on the scale from 0 to 100 between the laptop and the longhand condition for the sample of 142 participants. However, as researchers we are usually not primarily interested what happens in our sample. What we would like to know if our results generalises to the population from which this sample is drawn. In this case, we would like to know whether there is a memory difference between note taking with a laptop or in longhand format for students (as this is roughly the population the sample is drawn from). Going beyond the the present sample is the goal of inferential statistics. There are different inferential statistical approaches, and we are focussing on the most popular one, null hypothesis significance testing (NHST). We will describe NHST more thoroughly in the next chapter, but will already introduce its main ideas here. For NHST, the question of generalising from sample to population is about two different possible true states of the world: There either is no difference in conditions means in the population (i.e., there only is a mean difference in our sample due to chance) or there is a difference in the condition means in the population. To decide between these possibilities, we set up a statistical model for the data. This statistical model allows us to assess if there is no difference in the population  we call this possible state of the world the null hypothesis. More specifically, the statistical model allows us to test how compatible the data is with the null hypothesis of no difference. The test of the null hypothesis proceeds as follows: (1) We assume that the state of the world in which there is no difference in the population means is true. (2) Based on this assumption we calculate how likely it is to observe a difference as large as the one we have observed in our sample. (3) If the probability of observing a difference as large as the one we have is very small, we take this as evidence that the null hypothesis of no difference is not true  we reject the null hypothesis. (4) We act as if there were a difference in the population. In this case (i.e., we reject the null hypothesis and act as if there is a difference), we say our experimental manipulation has an effect. As is clear from this description, the logic underlying inferential statistics using NHST is not trivial. To make it clearer, let us apply the logic to the example data. We want to know whether the observed mean memory difference between note taking with a laptop and note taking in long hand format in our sample generalises to the population of students that take note in either of these formats. To do so, we set up a statistical model for our data. This model allows us to test how compatible our data is with the null hypothesis that there is no mean memory difference in the population. Specifically, the model allows us to calculate the probability of obtaining a memory difference as large as the one we have observed assuming the there is no mean memory difference in the population. If our data is incompatible with the null hypothesis (i.e., it is unlikely to obtain a memory difference as large as the one we have observed if the null hypothesis were true), we reject the null hypothesis of no mean memory difference in the population. Because we reject the null hypothesis we then act as if there was a mean memory difference in the population. In other words, we then act as if the type of note taking had an effect on memory after lectures in the population. Even though the logic of NHST is not necessarily intuitive, it is clearly helpful for researchers. After running an experiment we really would like to know if the difference observed in our experiment (i.e., in our sample) is meaningful in the sense that it generalises to the population from which we have sampled our participants. And in almost every actual experiment there is some mean difference between the condition (i.e., it is extreme unlikely that both conditions have exactly the same mean). Thus, we pretty much always face this question. NHST allows us to test whether the observed difference is compatible with a world in which there is no difference. If this is unlikely, we decide (i.e., act as if) there were a difference. What we can see from spelling out the logic in detail is that there are quite a few inferential steps we have to make to get to what we want. We design experiments with the goal in mind to find a difference between the different experimental conditions. However, we then do not test this directly. Instead, we test the compatibility of the data with the converse of what we are actually interested in  the null hypothesis of no effect. If this test fails (i.e., shows that the data is likely incompatible with the null hypothesis) we then make two inferential steps. First we reject the null hypothesis and then we act as if there were a difference. Both of these inferential steps are not necessitated logically. What this means is that inferences based on NHST alone are never extremely strong. NHST is the de facto standard procedure for inferential statistics across empirical sciences (i.e., not only in psychology and related disciplines). Understanding the logic of NHST will enable you to understand the majority of empirical papers and will also allow you to apply inferential statistics in your own research. Nevertheless, there exist a long list of popular criticisms of NHST (e.g., Rozeboom 1960; Meehl 1978; Cohen 1994; Nickerson 2000; Wagenmakers 2007). We will discuss these criticisms in more detail in later chapters, but for now it is important to realise that NHST does not allow to test, or prove, whether there is a mean difference in the population. The only thing NHST calculates is a probability of how compatible the data is with the null hypothesis. If this probability is low that does not necessarily mean that there is a difference. Likewise, if this probability is high that does not necessarily mean there is no difference. All inferences we draw based on NHST results are probabilistic in itself (i.e., can be false). So the most important rule when interpreting the results from NHST is to be humble. NHST never proves or confirms anything. Instead NHST results suggest or indicate certain interpretations. If we do not over-interpret results, but stay instead stay humble in our interpretations, we are unlikely to fall prey to the common (and often justifiable) criticisms of the NHST framework. 5.3 The Basic Statistical Model To apply inferential statistics in the NHST framework to our data, we begin by setting up a statistical model to the data. A statistical model attempts to explain (or predict) the observed values of the dependent variable (DV) from the independent variable (IV).35 In the experimental context this means predicting our observed outcome, the DV, from the experimental manipulation, the IV. The basic statistical model partitions the observed DV into three parts that: the overall mean, which for reasons that will become clear later is called the intercept, the effect of the IV, and the part of the data that cannot be explained by the model, the residuals. When summing these three parts together, they result in the observed value. In mathematical form we can express this as \\[\\begin{equation} \\text{DV} = \\underbrace{\\text{intercept}}_{\\text{overall mean}} + \\text{IV-effect} + \\text{residual}. \\tag{5.1} \\end{equation}\\] (For those not used to reading mathematical expressions, the point at the end of the equation is simply a full stop that ends the sentence and has no mathematical meaning.) As someone without a mathematics background myself, I know that equations in a text are often more intimidating than immediately useful. Consequently, before moving on it makes sense to go through this equation in more detail. Furthermore, all statistical analyses discussed in this book are applications of Equation (5.1). This equation forms the foundation for the statistical analysis of experimental data and thus understanding it will unlock all analyses discussed in this book. Consequently, it makes sense to spend more time on it. Let us consider the the variables in Equation (5.1) in more detail. When doing so, we also consider how many different possible values each variable can take on.36 The following Figure, a variant of Figure 5.1, shows the elements graphically and we explain them in the text just below. Figure 5.2: Data from Urry et al. (2021) showing the overall mean (intercept, blue dotted line), the condition specific effects (difference between dashed red lines for the condition means and the blue line), and the residuals (grey lines from condition means to data points). \\(\\text{DV}\\): The dependent variable, DV, are the observed values, one for each observation/participant. For the example data this are all the 142 black data points shown in Figure 5.2. Thus, our statistical model tries to explain the individually observed values. \\(\\text{intercept}\\): The intercept represents the overall mean. Consequently, we only have one intercept (i.e., the intercept is the same for each observation). In experimental designs we define this as the mean of all condition means. For the example data the intercept is (68.2 + 66.2) / 2 = 67.2 and is shown as a blue dotted line in Figure 5.2. \\(\\text{IV-effect}\\): The IV-effect represents the effect of our independent variable which we define as the difference between the condition means and the intercept (i.e., the deviation of the condition means from the intercept). Thus, we always have as many different IV-effects as we have conditions. For the example data with only two conditions, we only have two different IV-effects, both of which with the same magnitude and only differ in sign, 1.0 for the laptop condition and -1.0 for the longhand condition. If we add these values to the intercept, we get the condition means. As we will discuss further below, this is the most relevant part for answering the statistical question of interest. In Figure 5.2, the red dashed line (and the red points) show the condition means, thus the condition effects are the differences between the blue line and the red lines. \\(\\text{residual}\\): The residuals are the idiosyncratic aspects of the data that are left unexplained by the statistical model. As the model only predicts the condition means (i.e., intercepts plus independent variable), these are the deviations of the individual observations from the condition means. Thus, as for the DV, we have as many residuals as we have values of the DV. In Figure 5.2, the residuals are shown as grey lines from the condition means to each data point. This is all the information (or variability in the data) our model cannot explain. 5.3.1 Model Predictions A simplification of Equation (5.1) that makes it clearer what the statistical model predicts is obtained if we ignore the residuals for a moment. As a reminder, the residuals are the part of the data that remains unexplained. In other words, these represent all the idiosyncratic parts of the data independent of our manipulation (e.g., some participants have better memory than others independent of how they took notes). What remains from our statistical model if we ignore all idiosyncratic aspects are only the predictions based on our IV. In the case of experimental data, the IV is the experimental condition. Thus, what a statistical model actually predicts is the means of the experimental conditions. We can again formalise this as \\[\\begin{equation} \\hat{\\text{DV}} = \\text{intercept} + \\text{IV-effect}. \\tag{5.2} \\end{equation}\\] Here, the hat symbol (\\(\\hat{}\\)) means predicted value. Thus in contrast to the actual DV above, we only have the predicted DV in this equation. When performing statistical analyses it sometimes help to remind oneself that all a standard statistical model predicts are the condition means. We generally do not make predictions about individual participants or consider other factors that are not part of the model. We only predict, and are interested in, the condition means. 5.3.2 Statistical Model for the Example Data Let us take a look at the first six participants and their values for all the variables in the basic statistical model to get a better understanding of Equation (5.1). pid condition overall  intercept iv_effect prediction residual 1 laptop 65.8 67.2 1 68.2 -2.4 2 longhand 75.8 67.2 -1 66.2 9.6 4 longhand 50.0 67.2 -1 66.2 -16.2 5 laptop 89.0 67.2 1 68.2 20.8 6 longhand 75.6 67.2 -1 66.2 9.4 8 longhand 83.3 67.2 -1 66.2 17.1 The first three columns show the data. pid is the participant identifier (id) column. As it is often the case for real data, some ids are missing (here 3 and 7) for various reasons (e.g., potential participants were interested in the study and received an id, but then did not finish or start the experiment) so the first 6 rows already go up to pid = 8. condition tells us in which note taking condition a participant was and overall is their memory score on the scale from 0 to 100 which serves as the DV in the statistical model (i.e., the left-hand side in Equation (5.1)). The four right most columns contain the values of the variables on the right-hand side of Equation (5.1), the intercept, the iv_effect, and the residual. In addition, the prediction column shows the left-hand side of Equation (5.2). As described above, every observation (i.e., row) has a idiosyncratic DV and residual. We also see that all values share one intercept, and the IV-effect is condition specific. As a consequence, the prediction column (which is the sum of intercept and iv-effect) also has two values, one for each condition. Finally, we can see that the sum of the three values on the right-hand side of Equation (5.1) equals the observed value of the DV. For example, consider pid = 4. If we enter the values into Equation (5.1) we have \\[ 50.0 = 67.2 + (-1) + (-16.2). \\] From this example data we can also understand better what the residuals mean, they are the difference between the observed value and the predicted value, \\(\\text{residual} = \\text{DV} - \\hat{\\text{DV}}\\). Consider again pid = 4. Here we have \\[ -16.2 = 50- 66.2. \\] We can also see how the residual captures the idiosyncratic aspects of our data that cannot be explained by the condition means. For example, some participants  such as pid = 5 and pid = 8  have large positive residuals indicating that they have good memory independent of their note taking condition. Likewise, pid = 4 has a large negative residual indicating comparatively worse memory (again independent of the note taking condition). 5.3.3 Understanding the Statistical Model Now that we have described the parts of the statistical model we are almost ready to fit the model and interpret the output. Before doing so it makes sense to look at all the parts again individually and try to understand why we set up the statistical model in the way we do. Remember, our goal is to evaluate whether there is an effect of the experimental manipulation (i.e., a difference between the two note taking conditions) in the population from which the data is sampled. To do so, we set up a model that partitions the observed data into three parts, the intercept representing the overall mean, the condition specific effect (IV-effect) representing the difference of the condition means from the intercept, and the residuals representing the idiosyncratic part not explained by the model. The reason for doing so is that it allows us to zoom in on what matters for our statistical question, the condition specific effect. To answer the question if there is a difference between the conditions in the population, we can now focus on this part of the model. The overall level of performance captured in the intercept and the residuals can (for now) be ignored for this question. Consequently, the statistical test reported below is a statistical test of the condition effect. Thus, the reason for setting up the statistical model in this way is to make it easy to get an answer to the question that interests us: Is there an effect of the note taking manipulation/conditions on memory? To answer this question we only need to consider the condition effect. 5.4 Estimating the Statistical Model in R 5.4.1 Package and Data Setup For the statistical analyses reported in this book we generally use the afex package (R-afex?) in combination with the emmeans package (Lenth 2021). afex stands for analysis of factorial experiments and simplifies many of the things we want to do (full disclaimer: I am the main developer of afex). Most analyses can also be performed with different functions, but it is often easiest to use afex functions as they are developed particularly for cognitive and behavioural researchers working with experimental data. More specifically, afex functions provide the expected results for experimental data sets out-of-the-box without the need to change any settings (which is not true for the corresponding non-afex functions). emmeans stands for estimated marginal means and is the package we use once a statistical model is estimated to further investigate the results. afex and emmeans are fully integrated with each other which allows to test practically any hypotheses of interest with a combination of these two packages in a straight forward manner. We already introduce the interplay of these two packages here, and the next chapters will showcase the full power of this combination. We also regular use functions from the tidyverse package (e.g., for plotting). tidyverse is a collection of packages developed mainly by RStudio and their head data scientist Hadley Wickham. A full introduction of the tidyverse is beyond the scope of the present book, interested readers are encouraged to read the introductory book, Wickham and Grolemund (2017), which is also available for free online. We begin the analysis by loading the three packages first (use install.packages(c(\"afex\", \"emmeans\", \"tidyverse\")) in case they are not yet installed). We also change the default ggplot2 theme using theme_set() to a nicer one. library(&quot;afex&quot;) library(&quot;emmeans&quot;) library(&quot;tidyverse&quot;) theme_set(theme_bw(base_size = 15) + theme(legend.position=&quot;bottom&quot;, panel.grid.major.x = element_blank())) The next step would be loading in the data. This is made easy here as the data from Urry et al. (2021) is part of afex, under the name laptop_urry. So we can load it with the data() function. We then also get an overview of the variables in this data set using str(), which returns the structure of a data.frame. data(&quot;laptop_urry&quot;) str(laptop_urry) #&gt; &#39;data.frame&#39;: 142 obs. of 6 variables: #&gt; $ pid : Factor w/ 142 levels &quot;1&quot;,&quot;2&quot;,&quot;4&quot;,&quot;5&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ condition : Factor w/ 2 levels &quot;laptop&quot;,&quot;longhand&quot;: 1 2 2 1 2 2 1 2 2 1 ... #&gt; $ talk : Factor w/ 5 levels &quot;algorithms&quot;,&quot;ideas&quot;,..: 4 4 2 5 1 3 5 2 5 4 ... #&gt; $ overall : num 65.8 75.8 50 89 75.6 ... #&gt; $ factual : num 61.7 68.3 33.3 85.7 69.2 ... #&gt; $ conceptual: num 70 83.3 66.7 92.3 82.1 ... The str function shows six variables, three of which we have already mentioned above: pid: participant identifier, a factor with 142 levels, one for each participant. condition: factor identifying which note taking condition a participant belongs to, with two levels, laptop and longhand. talk: A factor identifying which TED talk a participant saw, with 5 level. overall: Numeric variable with participants overall memory performance on a scale from 0 (= no memory) to 100 (= perfect memory). This variable is called overall because it is the average of two separate memory performance scores given below. factual: Numeric variable with participants memory score for factual questions (ignored in this chapter). conceptual: Numeric variable with participants memory score for conceptual questions (analysed in the next chapter). 5.4.2 Estimating the Statistical Model For estimating a basic statistical model using afex we can use the aov_car() function. The next code snippet show how to do so for the example data, when saving the output in object res1 . res1 &lt;- aov_car(overall ~ condition + Error(pid), laptop_urry) #&gt; Contrasts set to contr.sum for the following variables: condition The first argument to aov_car() is a formula specifying the statistical model, overall ~ condition + Error(pid). The second argument identifies the data.frame containing the data (i.e., all the variables appearing in the formula), laptop_urry. We can also see that calling aov_car() produces a status message informing us that contrasts are set to contr.sum for the IVs in the model. This message is only shown for information purposes and can be safely ignored (we want contr.sum as contrasts for our variables, but as this is not the default R behaviour a message is shown). A formula in R is defined by the presence of the tilde-operator ~ and the main way for specifying statistical models. It allows specifying statistical models in a similar way to the mathematical formulation, specifically the prediction equation of the statistical model, Equation (5.2). Therefore, a formula provides a comparatively intuitive approach for specifying a statistical model. On the left hand side of the ~ we have the dependent variable, overall. On the right hand side we have the variables we want to use to predict the dependent variable. In the present case, the right-hand side consists of two parts concatenated by a +, the independent variable condition and an Error() term with the participant identifier variable pid. Thus, there are two difference between the formula used here and the prediction Equation (5.2), the formula misses an explicit intercept and we have specified an Error() term that is missing in Equation (5.2). Let us address these two difference in turn. The intercept is not actually missing from this equation, but implicitly included. More specifically, an intercept is specified using a 1 in a formula. However, unless an intercept is explicitly suppressed  which can be done by including 0 in the formula (and which should only be done if there are very good statistical reason to do so; i.e., it makes very rarely sense)  it is always assumed to be part of the models. Consequently, including it explicitly produces equivalent results. The following code shows this by comparing the previous result without explicit intercept, res1 with an aov_car call with explicit intercept using the all.equal() function. This function can be used to compare arbitrary R objects and only returns TRUE if they are equal. res1b &lt;- aov_car(overall ~ 1 + condition + Error(pid), laptop_urry) #&gt; Contrasts set to contr.sum for the following variables: condition all.equal(res1, res1b) #&gt; [1] TRUE The Error() term is a mandatory part of the model formula when using aov_car() and is used to specify the participant identifier variable (i.e., pid in this case). For a simple example as the present one that seems unnecessary, but later in the book we will see why the requirement of the Error() term is useful. Before looking at the results, let us quickly explain why the function for specifying models is called aov_car(). A regular statistical model such as the ones considered here that solely includes factors (i.e., categorical variables) as independent variables is also known as analysis of variance, which is usually shortened to ANOVA.37 The basic R function for ANOVA models is simply called aov(). However, aov() does not in all cases return the expected results for all types of ANOVA models considered in this book (i.e., in some situations aov() can return results that would be considered inappropriate, even when used carfeully). An alternative to aov() is the Anova() function from package car (Fox and Weisberg 2019) (where car stands for the book title, Companion to Applied Regression). Anova() always returns the expected and appropriate ANOVA results when used correctly. However, calling Anova() requires at least two function calls and can become tricky with more complicated models discussed in later chapters. aov_car() combines the simplicity of model specification of the aov() function with the appropriate statistical results from the Anova() function from the car package (i.e., aov_car() calls Anova() internally). 5.4.3 Interpreting the Results We can now look at the results of our statistical model. For this, we simply call the object that contains the results res1 (we would get the same output when calling print(res1) or nice(res1)). res1 #&gt; Anova Table (Type 3 tests) #&gt; #&gt; Response: overall #&gt; Effect df MSE F ges p.value #&gt; 1 condition 1, 140 269.66 0.52 .004 .471 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 The default aov_car() output is an Anova Table we will see throughout the book. We can also see that the results table contains Type 3 tests, but we will ignore this for now. The only other option, Type 2 tests, produces the same results for the example data. We will get back to the meaning of type of test in later chapters when it makes a difference and ignore this part until then. The next line of the results table is only reference information. We see that the response variable, which we also know as DV, is overall, just as we intended. We then get a table of effects, which in this case only has one row, the effect of condition. This row contains all the information for our null hypothesis significance test (NHST) for the condition effect. The most important column in this output is the last column, p.value, or \\(p\\)-value. The \\(p\\)-value in this column is the main results of NHST and allows us to judge the compatibility of the data with the null hypothesis. It is the probability of obtaining a difference as extreme as observed when assuming that the null hypothesis of no difference is true. We see that in this case the \\(p\\)-value is not small, it is .47. Thus, the data are not incompatible with the null hypothesis and does not suggest that there is a memory difference between note taking with a laptop or in longhand format during lectures. In general, researchers have adopted a significance level of .05. This means that if a \\(p\\)-value is smaller than .05 we treat this as evidence that the data is incompatible with the null hypothesis. In this case we would say the result is significant. However, as in our case the result is not smaller than .05 the result is not significant (I would avoid saying insignificant if the \\(p\\)-value is larger than .05, as significant is a technical term here). Thus, in the present case we do not reject the null hypothesis. The present data therefore do not provide evidence that the observed difference between the two modes of note taking generalises from the sample to the population according to NHST. There are two further important columns whose results generally need to be reported, df, which stands for degrees of freedom (or df), and F. Understanding these columns in detail is beyond the scope of the present chapter, so we will only introduce them briefly. There are two degrees of freedom reported here, the first value, 1, is the numerator degree of freedom. It is always given by number of conditions minus 1. In the present case, we have two conditions, laptop and longhand, so the numerator df are 2 - 1 = 1. The second value is the denominator df, which are generally given by number of participants minus numerator df minus 1. Here we have 142 participants and therefore 142 - 1 - 1 = 140. In general, the larger the denominator df (i.e., the more participants we have) the better we can detect incompatibility with the null hypothesis (i.e., the easier it is to get small \\(p\\)-values). The \\(F\\)-value is a value expressing the observed incompatibility of the data with the null hypothesis. If \\(F \\leq 1\\), the data are compatible with the null hypothesis. If \\(F &gt; 1\\) the data are to some degree incompatible with the null hypothesis, with larger values indicating more incompatibility. The \\(p\\)-value is calculated from df and \\(F\\)-value. Consequently, the results are usually reported in the following way: \\(F(1, 140) = 0.52\\), \\(p = .471\\). The next column that is important is ges which stands for generalised eta-squared, using the mathematical notation with Greek letters, \\(\\eta^2_G\\). \\(\\eta^2_G\\) is a standardised effect size that tells us something about the absolute magnitude of the observed effect (Olejnik and Algina 2003; Bakeman 2005). More specifically, \\(\\eta^2_G\\) is supposed to be a measure of the proportion of variance in the DV that can be accounted for by a specific factor or IV in the model. For example, in the present case the condition effect is supposed to explain 0.4% of the variance in performance. In general, we should avoid standardised effect sizes such as \\(\\eta^2_G\\) and instead report simple effect sizes. A simple effect size is expressed in units of our measured DV. For example, throughout this chapter we have mentioned that the observed difference in memory performance between both note taking conditions is 2.0 on the scale from 0 to 100. Here, the difference of 2.0 is a simple effect size. We will have to say more about effect sizes later, but as some journal editors or publishing guidelines require standardised effect sizes (which is statistically not a reasonable recommendation in my eyes) the default output contains it. Finally, the default output contains the MSE column, which stands for mean squared errors. This column is mainly included for historical reasons. Traditionally, ANOVA models could relatively easily be calculated by hand or by calculator based on different variance terms (hence the name, analysis of variance). One of this term is the mean squared error from which, in combination with the residual squared error, the \\(F\\)-value can be calculated. In my undergrad studies I still learned to calculate ANOVA by hand, but this seems rather unnecessary nowadays. Hence, we will simply ignore this column. Interested reader can find a detailed explanation about the meaning of MSE for example in Howell (2013) or Baguley (2012). One thing we note in the results table is that it does not contain any information about the intercept. However, as discussed above, the intercept is included in the model. The reason for omitting the intercept from the default output is that it is generally not of primary interest. In experimental research usually the main interest is in the effect of our independent variables, the effect of the experimental manipulation. The statistical model that separates the intercept (i.e., overall mean) from the condition effect allows to zoom in on the relevant part. In line with this, the default output of aov_car does the same. Later chapters will show how we can also get information about the intercept. Estimating a statistical model with aov_car() provides us with the inferential statistical results, the null hypothesis tests for the IV-effects shown above. To get these, we just need to call the object containing the results at the R prompt (e.g., calling res1 in the present case). However, we can use the results object also for others parts of the statistical analyses, for data visualisation and follow-up analyses. 5.4.4 Data Visualisation For data visualisation we can use the afex function afex_plot() which is built on top of the ggplot2 package. afex_plot() requires an estimated model object (e.g., as returned from aov_car()) and specifying which factors of the model we want to plot. In the present case, we only have one factor, condition, so we can only choose this one. Importantly, all factors passed to afex_plot() need to be passed as character strings (i.e., enclosed with \"...\"). afex_plot(res1, &quot;condition&quot;) Figure 5.3: afex_plot() figure for data from Urry et al. (2021) This simple call to afex_plot() produces already a rather good looking results figure combining the individual-level data points (in the background in grey) with the condition means (in black). Individual data points in the background that have the same or very similar values are displaced on the x-axis so they do not lie on top of each other. This is achieved through package ggbeeswarm (which needs to be installed once: install.packages(\"ggbeeswarm\")). The plot also per default shows 95% confidence intervals of the means, which we will explain in detail in a later chapter. As afex_plot() returns a ggplot2 plot object, we can manipulate the plot to make it nicer. p1 &lt;- afex_plot(res1, &quot;condition&quot;) p1 + labs(x = &quot;note taking condition&quot;, y = &quot;memory performance (0 - 100)&quot;) + coord_cartesian(ylim = c(0, 100)) + geom_line(aes(group = 1)) For example, in the code snippet above we first save the plot object as p1 and then call a number of ggplot2 function on this plot object to alter the plot appearance (in ggplot2 graphical elements are added to a plot using +). Function labs() is used to change the axis labels, coord_cartesian() changes the extent of the y-axis (i.e., the plot now show the full possible range of memory performance score), and geom_line(aes(group = 1) adds a line connecting the two means. This figure could now be used in a results report or manuscript as is. 5.4.5 Follow-Up Analysis Follow-up analysis refers to an inspection of the predicted condition means and their relationships. In the case of a single independent variable with two levels (e.g., laptop versus longhand) their is not much to investigate in this regard. We can nevertheless show the general procedure. For follow-up analyses we generally begin with function emmeans() from package emmeans (Lenth 2021). Function emmeans() then returns the estimated marginal means, which is a slightly complicated way of saying condition means, plus additional statistical information. Similarly to afex_plot(), emmeans() requires an estimated model object as well as the specification of a factor in the model for which we want to get the condition means: emmeans(res1, &quot;condition&quot;) #&gt; condition emmean SE df lower.CL upper.CL #&gt; laptop 68.2 1.99 140 64.3 72.1 #&gt; longhand 66.2 1.91 140 62.4 70.0 #&gt; #&gt; Confidence level used: 0.95 For now we only focus on the estimates means in column emmean and ignore the additional inferential statistical information in columns SE to upper.CL. We can see that the reported means match the means given in the text at the very beginning of the chapter, 5.1. The power of emmeans is not only to provide the condition means, but it also allows us to perform calculation on the condition means. For example, in the case of a factor with two levels we can easily calculate the difference between the condition means as our simple effect size. For this, we can save the object returned by emmeans() and then call the pairs() function on this object which gives us all pairwise comparisons of conditions means of which there is only one in the present case (we would get the same results by combining both calls into one: pairs(emmeans(res1, \"condition\"))): em1 &lt;- emmeans(res1, &quot;condition&quot;) pairs(em1) #&gt; contrast estimate SE df t.ratio p.value #&gt; laptop - longhand 1.99 2.76 140 0.722 0.4715 The output shows a mean difference of 1.99 which slightly differs from the 2.0 reported above, which is slightly concerning. However, the results reported above are rounded to one decimal only. If we do so for the present results, we also get an estimated difference of 2.0 (we will not explain this code in detail here): em1 %&gt;% pairs() %&gt;% as.data.frame() %&gt;% format(digits = 1, nsmall = 1) #&gt; contrast estimate SE df t.ratio p.value #&gt; 1 laptop - longhand 2.0 2.8 140.0 0.7 0.5 5.5 Summary The goal of this chapter was to introduce the standard statistical approach for analysing experimental data with one independent variable with two levels  an experiment with two conditions. Practically every time when we run such an experiment, we observe that there is some mean difference in the dependent variable between the two conditions. For our example data by Urry et al. (2021) there was a memory difference of 2.0 points between the two note taking conditions (laptop versus longhand) on the response scale from 0 to 100. The important statistical question we then have is whether there is any evidence suggesting that the observed difference in our sample generalises to the population. The sample are the participants in our experiment and the population refers to all possible participants that could have been sampled. For Urry et al. (2021) this population could be loosely described as students taking notes or maybe more precisely undergraduate students at research intensive (R1) US universities. The question we would like to get a statistical answer to is: Should we believe that there generally is a memory difference between note taking with a laptop versus longhand? To answer this question we need inferential statistics. The inferential statistical approach we are using is called null hypothesis significance testing or NHST. However, NHST does not directly address the question whether there is evidence for a difference in the population. Instead, NHST tests the compatibility of the data with the null hypothesis  the assumption that there is no difference between the condition in the population. The most important result from NHST is the \\(p\\)-value. The \\(p\\)-value is a measure of the compatibility of the data with the null hypothesis; it is the probability of obtaining a results as extreme as observed assuming the null hypothesis is true. If the \\(p\\)-value is smaller than .05 we reject the null hypothesis that there is no difference. In this case we decide that there is evidence for a difference (although this does not follow with logical necessity). To apply NHST to the data we set up a statistical model that observed partitions the data into three parts (Equation (5.1)): the intercept representing the overall mean, the effect of the independent variable (i.e., the difference of the condition means from the intercept), and the residuals representing the idiosyncratic aspects not explained by the other parts of the model. This partitioning allows us to zoom in on the part of the data that we are interested in, the effect of our independent variable, the experimental manipulation. To estimate a statistical model to the data we used function aov_car() from the afex package. aov_car() allows us to specify the statistical model using a formula of the form dv ~ iv + Error(pid) (where pid refers to the variable in the data with the participant identifier) mimicking the mathematical specification of the statistical model. The default output returns an ANOVA table which provides a null hypothesis significance test for our iv, the independent variable. The returned table is called an ANOVA table because statistical models that only contain factors are called analysis of variance or ANOVA. In the present case, the statistical model only has a single factor, note taking condition, with two levels, laptop versus longhand. In the returned ANOVA table, we do not only have the \\(p\\)-value for our experimental factor, but additional inferential statistical information such as the degrees of freedom, df, and the \\(F\\)-value. We can also use the object returned from aov_car() for plotting using function afex_plot(). This function produces a plot combining the individual-level data points with the condition means. This provides a comprehensive display of the data of the experiment. As the function returns a ggplot2 object, this plot can be be easily modified to create a figure that can be used in a results report. We can also use the object returned from aov_car for follow-up analyses using emmeans. With emmeans we can easily obtain the condition means (or estimated marginal means) on the dependent variable. Based on these condition means we can calculate the observed effect size (i.e., the mean difference). Applying the statistical model to the data from Urry et al. (2021) showed a non significant difference, \\(F(1, 140) = 0.52\\), \\(p = .471\\). This suggests that there is no difference in memory performance after watching a talk and taking notes with either a laptop or in longhand format. References "],["case-study-1-more-results-from-note-taking-studies.html", "Chapter 6 Case Study 1: More Results from Note Taking Studies 6.1 Conceptual Memory Data from Urry et al. (2021) 6.2 Why are Experiments Replicated? 6.3 Conceptual Memory Data from Mueller and Oppenheimer (2014) 6.4 Summary", " Chapter 6 Case Study 1: More Results from Note Taking Studies In this chapter we will apply what we have learned in the previous chapter - how to analyse experimental data with one experimental manipulation and two conditions. For this, we will again take a look at the data from Urry et al. (2021). Additionally, we will analyse data from Mueller and Oppenheimer (2014). This study was the first published study investigating the question of note taking with a laptop or in longhand format and was the basis on which Urry et al. (2021) planned their study. For the data of Mueller and Oppenheimer (2014) we will perform a full analysis starting with reading in the data. So in addition to performing the statistical hypothesis test, we will calculate some descriptive statistics. We start the analysis in this chapter in the same way as in the previous chapter, by loading the three packages we generally use, afex, emmeans, and tidyverse, and set a nicer ggplot2 theme. Before doing so it is probably a good idea to restart R (unless, of course, you are just starting R). In RStudio this can be conveniently done through the menu by clicking on Session and then Restart R. In other R environments you might need to restart the program. The benefit of restarting R is that it should create a blank R session in which no packages are loaded and no objects exist in the workspace. Only such a blank sessions ensures that, once we have obtained a set of results, we can recreate them later using the same code. That is, a blank R session avoids any potential problems due to analyses performed in a previous session that are still lingering. Restarting R should generally be done when starting a new analysis or after one is completely done with an analysis. In the latter case, it makes sense to restart R and the rerun all code one has saved in ones script to ensure that all results replicate based on only the code in the script (and do not require some additional code not saved). library(&quot;afex&quot;) #&gt; Loading required package: lme4 #&gt; Loading required package: Matrix #&gt; ************ #&gt; Welcome to afex. For support visit: http://afex.singmann.science/ #&gt; - Functions for ANOVAs: aov_car(), aov_ez(), and aov_4() #&gt; - Methods for calculating p-values with mixed(): &#39;S&#39;, &#39;KR&#39;, &#39;LRT&#39;, and &#39;PB&#39; #&gt; - &#39;afex_aov&#39; and &#39;mixed&#39; objects can be passed to emmeans() for follow-up tests #&gt; - NEWS: emmeans() for ANOVA models now uses model = &#39;multivariate&#39; as default. #&gt; - Get and set global package options with: afex_options() #&gt; - Set orthogonal sum-to-zero contrasts globally: set_sum_contrasts() #&gt; - For example analyses see: browseVignettes(&quot;afex&quot;) #&gt; ************ #&gt; #&gt; Attaching package: &#39;afex&#39; #&gt; The following object is masked from &#39;package:lme4&#39;: #&gt; #&gt; lmer library(&quot;emmeans&quot;) library(&quot;tidyverse&quot;) #&gt; -- Attaching packages --------------------------------------- tidyverse 1.3.1 -- #&gt; v ggplot2 3.3.5 v purrr 0.3.4 #&gt; v tibble 3.1.2 v dplyr 1.0.7 #&gt; v tidyr 1.1.3 v stringr 1.4.0 #&gt; v readr 1.4.0 v forcats 0.5.1 #&gt; -- Conflicts ------------------------------------------ tidyverse_conflicts() -- #&gt; x tidyr::expand() masks Matrix::expand() #&gt; x dplyr::filter() masks stats::filter() #&gt; x dplyr::lag() masks stats::lag() #&gt; x tidyr::pack() masks Matrix::pack() #&gt; x tidyr::unpack() masks Matrix::unpack() theme_set(theme_bw(base_size = 15) + theme(legend.position=&quot;bottom&quot;, panel.grid.major.x = element_blank())) 6.1 Conceptual Memory Data from Urry et al. (2021) As a quick reminder, Urry et al. (2021) showed their participants short lectures (TED talks) on video during which participants were allowed to take notes. One group of participants, the laptop condition, could take notes on a laptop, whereas the participants in the longhand condition could take notes with pen and paper. After the lecture participants were quizzed on two aspects of the content of the lecture, factual questions and conceptual questions. In the previous chapter we have analysed the overall memory score which was the average of the performance for the factual questions and the conceptual questions. Here, we are only concerned with the memory performance for conceptual questions. We begin our analysis by loading in the data (which is part of afex can be loaded with data()) and getting an overview of the variables using str(): data(&quot;laptop_urry&quot;) str(laptop_urry) #&gt; &#39;data.frame&#39;: 142 obs. of 6 variables: #&gt; $ pid : Factor w/ 142 levels &quot;1&quot;,&quot;2&quot;,&quot;4&quot;,&quot;5&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ condition : Factor w/ 2 levels &quot;laptop&quot;,&quot;longhand&quot;: 1 2 2 1 2 2 1 2 2 1 ... #&gt; $ talk : Factor w/ 5 levels &quot;algorithms&quot;,&quot;ideas&quot;,..: 4 4 2 5 1 3 5 2 5 4 ... #&gt; $ overall : num 65.8 75.8 50 89 75.6 ... #&gt; $ factual : num 61.7 68.3 33.3 85.7 69.2 ... #&gt; $ conceptual: num 70 83.3 66.7 92.3 82.1 ... As before, we have the participants identifier variable in pid and the note taking condition in variable condition. We can also guess that the conceptual memory scores are in the aptly name variable conceptual (if we were unsure about this, we could also check the documentation of the data at ?laptop_urry). Usually, once the data is sufficiently prepared (i.e., we have performed some sanity checks and identified DV and IV), the first step in an analysis should be plotting the data. This could be done using ggplot2 directly. However, in cases such as the present one where it is very clear which statistical model we are going to estimate it is often a bit less effort to plot the data with afex_plot(). Thus, we start by estimating the statistical model for the conceptual memory performance of the data from Urry et al. (2021) and save the estimated model object as mc_urry. For this, we again use aov_car() on the laptopt_urry data and specify the model using the formula interface. The DV we are considering here is conceptual, our IV is condition, and the participant identifier is pid. Consequently, the formula is conceptual ~ condition + Error(pid). Then, before looking at the inferential statistical results, we use this model object to plot the data using afex_plot. mc_urry &lt;- aov_car(conceptual ~ condition + Error(pid), laptop_urry) #&gt; Contrasts set to contr.sum for the following variables: condition afex_plot(mc_urry, &quot;condition&quot;) Figure 6.1: Conceptual memory scores from Urry et al. (2021) across note taking conditions The goal behind beginning with plotting the data is that it allows to see whether the data looks alright. That is, we check whether there are any features that stand out such as clear outliers or an unusual pattern in the data. If this were the case, we would try to figure out if we can find a reason for this issue or how we deal with it. But, as the data looks alright, we continue and consider the results of the significance test: mc_urry #&gt; Anova Table (Type 3 tests) #&gt; #&gt; Response: conceptual #&gt; Effect df MSE F ges p.value #&gt; 1 condition 1, 140 441.76 1.00 .007 .319 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 The ANOVA table reveals that the significance test for the effect of condition is not significant with \\(p = .319\\). Thus, in line with the finding that there is no evidence for a difference in overall memory performance, there also is no evidence for a difference in memory for conceptual information. We can also again use emmeans to see the condition means (or estimated marginal means). In line with Figure 6.1 (as afex_plot internally also uses emmeans it shows exactly the same means in graphical form), the memory score in the laptop condition is descriptively around 3.5 points higher than the score in the longhand condition. emmeans(mc_urry, &quot;condition&quot;) #&gt; condition emmean SE df lower.CL upper.CL #&gt; laptop 73.5 2.55 140 68.5 78.6 #&gt; longhand 70.0 2.44 140 65.2 74.8 #&gt; #&gt; Confidence level used: 0.95 Before moving to the next data set, let us consider how we could report this analysis in a research report. We could for example write: As shown in Figure 6.1, participants conceptual memory scores (on a scale from 0 to 100) are descriptively slightly larger in the laptop condition compared to the longhand condition. We analysed these scores with an ANOVA with one factor, note taking condition, with two levels (laptop vs. longhand). The effect of note taking condition was not significant, \\(F(1, 140) = 1.00\\), \\(p = .319\\). This indicates that the data does not provide evidence for a difference in memory for conceptual information based on how notes are taken during lectures. 6.2 Why are Experiments Replicated? The experiment by Urry et al. (2021) was not the first experiment investigating the effect of note taking during lectures on memory. In contrast, their study was a replication of Mueller and Oppenheimer (2014). A replication is the act of rerunning an existing study to see if one can obtain (or replicate) the results of the previous study. As we have discussed before, inferences from NHST are never conclusive as they are probabilistic and require multiple inferential steps. Replications are one of the most important tools in science for overcoming at least the probabilistic uncertainties associated with the inferences we draw from experimental data. For example consider that several independent but otherwise as similar as possible experiments  that is, replications of the same experiment  all obtain a significant result (i.e., indicate that the data are incompatible with the null hypothesis). Such a pattern would dramatically increase our confidence that the null hypothesis is likely false. In addition to the gain in confidence for specific results, there are good practical reasons for replicating an existing experiment. For example, when beginning to work on a new topic it is generally a good idea to replicate the experiment on which one wants to build on. If one already has problems replicating what exists that shows that the topic is maybe not as simple as portrayed in the literature. Another excellent reason for performing a replication is if one simply does not believe an existing result. Remember, one of the key components of the scientific method is scepticism (at least according to Wikipedia). And if a results is difficult to believe, the reasonable sceptical position to take is to require more evidence. A replication is one way (if not the best way) to produce such evidence. Not believing existing experiments also does not imply that one questions the integrity of the researchers who did the experiment. There are many completely harmless reasons why a study might not replicate. For example, researchers might have just obtained a significant results by chance (which happens in 5% of cases, as discussed in the next chapters). Sadly, replicating existing experiments and publishing the results, is still not the norm in psychology and related disciplines. Quite to the contrary, the situation is so dire that many fields are currently considered to be in a replication crisis. For example, a large scale effort to replicate 100 studies in psychology (Open Science Collaboration 2015) showed that less than 50% could be replicated successfully. Similarly sobering results have since been observed across the social sciences (Camerer et al. 2018, 2016; Klein et al. 2018). Much has been written about this problem and this is not the right place to rehash all arguments. The best summary of the situation is the book by Chris Chambers (Chambers 2017). The important thing is to realise that science is a cumulative endeavour. Every new experiment builds on existing research. If the existing research has never been replicated, our confidence in this research has to be somewhat low. This questions the foundations of any new work that builds up on this non-replicated research. To move forward we researchers need to replicate work that is important for our research, value replications done by others (especially if it is of our work), and let findings that do not replicate fade into obscurity. 6.3 Conceptual Memory Data from Mueller and Oppenheimer (2014) As Urry et al. (2021) is a direct replication of Mueller and Oppenheimer (2014), the design is the same and uses the same materials (i.e., the same TED talks and same questions). Participants watched short lecture videos (projected onto a screen) and could take notes either on a laptop or with longhand format. 30 minutes after the lecture they were asked factual and conceptual questions about the lectures. Their answers were coded by the first author. As in the previous analysis, we will transform the answers to a memory index from 0 (= no memory) to 100 (= perfect memory). In line with the analysis of Urry et al. (2021) above, we are only interested in the conceptual memory here. The experiment by Urry et al. (2021) was a direct replication of Experiment 1 of Mueller and Oppenheimer (2014). Here we focus on Experiment 2 by Mueller and Oppenheimer (2014), which is also a direct replication of their Experiment 1 and only included an additional experimental manipulation which we will ignore here. The reason for focussing on their Experiment 2 instead of Experiment 1 is that the data of Experiment 2 come out a bit more interesting (feel free to rerun the analysis reported here for their Experiment 1 to see what I mean). However, to not provide an incomplete picture for the research question of whether the mode of taking note during lectures affects memory, we will consider the overall evidence (i.e., all 3 experiments of Mueller and Oppenheimer, the experiment of Urry et al., and further data) at the end of this chapter. Luckily for us, the data from Mueller and Oppenheimer (2014), including the data from their Experiment 2, is available online on the Open Science Framework (OSF). The OSF is one of the most visible developments resulting from the replication crisis. It is a free website that allows researchers to share their data and other materials associated with their research. Before the replication crisis and the OSF it was very rare to get access to the data underlying published studies. Nowadays many researchers depose their (anonymised) data for published studies on the OSF and include the links to the data in their papers. This allows other researcher, such as us, to reanalyse existing data and ensure that the reported results can be reproduced.38 6.3.1 Preparing the Data To get into the habit of downloading data from OSF and reanalysing them, this is what we are going to do now. The file we need is called Study 2 abbreviated data.csv and can be found at the following OSF link: https://osf.io/t43ua/ Please go ahead and download it now and put it in a folder so you can access it. I have already done so and copied it into folder data. We then use the tidyverse function read_csv(), which always returns a tibble (the tidyverse version of a data.frame), to read in the data, as object mo2014. Then we use the glimpse() function (also a tiydverse function) to get an overview of the data (it is very similar to str() but less verbose for tibbles). mo2014 &lt;- read_csv(&quot;data/Study 2 abbreviated data.csv&quot;) glimpse(mo2014) #&gt; Rows: 153 #&gt; Columns: 22 #&gt; $ participantid &lt;dbl&gt; 103, 122, 142, 152, 172, 183, 193, 213, 228, ~ #&gt; $ notetype &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, ~ #&gt; $ whichtalk &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~ #&gt; $ Wcount &lt;dbl&gt; 148, 273, 124, 149, 182, 104, 178, 170, 289, ~ #&gt; $ threeG &lt;dbl&gt; 0.08219178, 0.05535055, 0.18032787, 0.0544217~ #&gt; $ factualindex &lt;dbl&gt; 2.499, 2.833, 2.333, 4.499, 4.999, 1.833, 3.4~ #&gt; $ conceptualindex &lt;dbl&gt; 2.0, 1.5, 2.0, 2.0, 2.0, 1.5, 2.0, 1.5, 2.0, ~ #&gt; $ factualraw &lt;dbl&gt; 6, 7, 5, 11, 12, 4, 7, 11, 12, 11, 7, 6, 8, 5~ #&gt; $ conceptualraw &lt;dbl&gt; 4, 3, 4, 4, 4, 3, 4, 3, 4, 5, 3, 1, 4, 1, 2, ~ #&gt; $ perfectfactindexscore &lt;dbl&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ~ #&gt; $ perfectconceptindexscore &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ~ #&gt; $ perfectfactscore &lt;dbl&gt; 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 1~ #&gt; $ perfectconceptscore &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ~ #&gt; $ `filter_$` &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~ #&gt; $ ZFindexA &lt;dbl&gt; -0.17488644, 0.07568357, -0.29942123, 1.32553~ #&gt; $ ZCindexA &lt;dbl&gt; 0.52136737, 0.03238307, 0.52136737, 0.5213673~ #&gt; $ ZFrawA &lt;dbl&gt; 0.32848714, 0.68152314, -0.02454886, 2.093667~ #&gt; $ ZCrawA &lt;dbl&gt; 0.9199338, 0.2942131, 0.9199338, 0.9199338, 0~ #&gt; $ ZFindexW &lt;dbl&gt; -0.76333431, -0.47725123, -0.90551931, 0.9497~ #&gt; $ ZCindexW &lt;dbl&gt; 0.79471336, 0.06811829, 0.79471336, 0.7947133~ #&gt; $ ZFrawW &lt;dbl&gt; -0.6437995, -0.2476152, -1.0399839, 1.3371221~ #&gt; $ ZCrawW &lt;dbl&gt; 0.9647838, 0.1731663, 0.9647838, 0.9647838, 0~ We can see data from 153 participants on 22 columns. Many of the columns have names that are not immediately clear. This is not uncommon. An important task when getting any new data set is trying to figure out what the variables mean. One usually also has to do this for the data for the own experiments. For example, software for running experiments often collects more variables than needed for analysis. Consequently, the first analysis step is usually to figure out what is needed and what not. Before doing so however, we note that 153 participants is not the final number of participants reported by Mueller and Oppenheimer (2014). Instead, they removed two participants before the analysis resulting. Studying their OSF repository in detail (in particular the published SPPS script with output Output and Syntax - Study 2.doc) shows that participants with number 194 and 237 needs to be removed (the same information can be found in variable filter_$ in the current data set). This file also shows that the two relevant notetype conditions are 1 = longhand and 2 = laptop and we will remove notetype == 3 before analysis. Before moving on, we filter our data and remove these observations. For this we use filter() from the tidyverse in combination with the pipe operator %&gt;% (i.e., we pipe our tibble to filter() and only retain those rows that we want). Importantly, we overwrite mo2014 with the filtered tibble as we do not need the filtered out observations any more (to use them, we woul dhave to read the data in again). We then see how many participants remain using nrow() (which returns the number of rows in the data). mo2014 &lt;- mo2014 %&gt;% filter(notetype != 3, participantid != 194, participantid != 237) nrow(mo2014) #&gt; [1] 99 The reported 99 participants matches the 99 participants reported on OSF for the two conditions, laptop versus longhand. Sadly, the OSF does not include a codebook describing all variables for this particular data set (only for an earlier version of the data set with variables that only overlaps to some degree with the present one). However, from looking at data and the information on OSF a few things are clear: participantid is the participant identifier variable, notetype is the condition identifier coding the experimental condition, and whichtalk identifies the TED talk participants saw (the mapping of talks to numbers is also given in the SPPS output). In a first step, we can transform the relevant indicator variables, participant and experimental condition variable, into factors for further analysis. Transforming a variable into a factor guarantees that none of the analyses incorrectly treats one of the factors (i.e., categorical variables) as a numerical variable (e.g., taking the mean of the numbers in the participant identifier column is not a reasonable statistical operation). However, instead of overwriting the existing variables, we create new variable with the same name as in our analysis of Urry et al. (2021), pid and condition. We also assign human understandable labels instead of using 1 and 2 for the condition codes. This will make it easier to understand the pattern of results. To do so we use factor() inside mutate() from the tidyverse in combination with the pipe operator %&gt;%. mo2014 &lt;- mo2014 %&gt;% filter(notetype != 3) %&gt;% mutate( pid = factor(participantid), condition = factor(notetype, levels = c(2, 1), labels = c(&quot;laptop&quot;, &quot;longhand&quot;)) ) glimpse(mo2014) #&gt; Rows: 99 #&gt; Columns: 24 #&gt; $ participantid &lt;dbl&gt; 103, 122, 142, 152, 172, 183, 193, 213, 228, ~ #&gt; $ notetype &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, ~ #&gt; $ whichtalk &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~ #&gt; $ Wcount &lt;dbl&gt; 148, 273, 124, 149, 182, 104, 178, 170, 289, ~ #&gt; $ threeG &lt;dbl&gt; 0.08219178, 0.05535055, 0.18032787, 0.0544217~ #&gt; $ factualindex &lt;dbl&gt; 2.499, 2.833, 2.333, 4.499, 4.999, 1.833, 3.4~ #&gt; $ conceptualindex &lt;dbl&gt; 2.0, 1.5, 2.0, 2.0, 2.0, 1.5, 2.0, 1.5, 2.0, ~ #&gt; $ factualraw &lt;dbl&gt; 6.0, 7.0, 5.0, 11.0, 12.0, 4.0, 7.0, 11.0, 12~ #&gt; $ conceptualraw &lt;dbl&gt; 4, 3, 4, 4, 4, 3, 4, 3, 4, 5, 3, 1, 4, 1, 2, ~ #&gt; $ perfectfactindexscore &lt;dbl&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ~ #&gt; $ perfectconceptindexscore &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ~ #&gt; $ perfectfactscore &lt;dbl&gt; 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 1~ #&gt; $ perfectconceptscore &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ~ #&gt; $ `filter_$` &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~ #&gt; $ ZFindexA &lt;dbl&gt; -0.17488644, 0.07568357, -0.29942123, 1.32553~ #&gt; $ ZCindexA &lt;dbl&gt; 0.52136737, 0.03238307, 0.52136737, 0.5213673~ #&gt; $ ZFrawA &lt;dbl&gt; 0.32848714, 0.68152314, -0.02454886, 2.093667~ #&gt; $ ZCrawA &lt;dbl&gt; 0.9199338, 0.2942131, 0.9199338, 0.9199338, 0~ #&gt; $ ZFindexW &lt;dbl&gt; -0.76333431, -0.47725123, -0.90551931, 0.9497~ #&gt; $ ZCindexW &lt;dbl&gt; 0.79471336, 0.06811829, 0.79471336, 0.7947133~ #&gt; $ ZFrawW &lt;dbl&gt; -0.6437995, -0.2476152, -1.0399839, 1.3371221~ #&gt; $ ZCrawW &lt;dbl&gt; 0.96478381, 0.17316633, 0.96478381, 0.9647838~ #&gt; $ pid &lt;fct&gt; 103, 122, 142, 152, 172, 183, 193, 213, 228, ~ #&gt; $ condition &lt;fct&gt; longhand, longhand, longhand, longhand, longh~ Looking at the data again reveals that the newly created variables are added to the end of the tibble. The next step is to calculate our dependent variable, the memory scores from 0 to 100 as used in the analysis of Urry et al. (2021). We can see that for the two question types, factual and conceptual, there are multiple measures. Each has an index score and a raw score as well as perfect variants for both types of scores. perfect here presumably means the maximal possible value that could be obtained for this score for this observation (i.e., row). The data also contains a number of \\(z\\)-transformed variants of the scores (variables Z), but we will ignore them here (the original paper used the z-scores, but as these are more difficult to interpret and the results are qualitatively the same, we ignore them here). We focus on the index score which gives participant a maximal of 1 point per question (this information is given in the paper/on OSF). Let us take a look at the first six observations for the relevant variables. mo2014 %&gt;% select(pid, condition, factualindex, conceptualindex, perfectfactindexscore, perfectconceptindexscore) %&gt;% head() #&gt; # A tibble: 6 x 6 #&gt; pid condition factualindex conceptualindex perfectfactinde~ perfectconcepti~ #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 103 longhand 2.50 2 7 3 #&gt; 2 122 longhand 2.83 1.5 7 3 #&gt; 3 142 longhand 2.33 2 7 3 #&gt; 4 152 longhand 4.50 2 7 3 #&gt; 5 172 longhand 5.00 2 7 3 #&gt; 6 183 longhand 1.83 1.5 7 3 We can see that the number of questions per question type and talk differs (as indicated by the difference in perfect indexscore values across rows), but the total number of items appears to always be ten (perfectfactindexscore + perfectconceptindexscore = 10 in each row). This also aligns with the list of items found on OSF which show that there are ten questions per talk with the number of factual and conceptual questions differing across talks. From this information we could calculate our memory scores. However, before moving on it makes sense to run a quick sanity check to see that indeed the number of questions per row is ten. To do this, we create a new variable with the sum of the two perfect index scores, using mutate() (which adds a variable to the existing data), and then see whether this sum is always equal to 10, using summarise() (which in this case reduces the data to one row). If the number of question per observation/row sums to ten, this should return TRUE. mo2014 %&gt;% mutate(sum_p_index = perfectfactindexscore + perfectconceptindexscore) %&gt;% summarise(check = all(sum_p_index == 10)) #&gt; # A tibble: 1 x 1 #&gt; check #&gt; &lt;lgl&gt; #&gt; 1 TRUE Fortunately, the check passes so we feel that our assumption about the meaning of the variables are supported so we could go ahead and calculate our memory score. When preparing data for analysis, or running an analysis, it is important to regularly include such sanity checks in ones analysis. Any analysis involves assumptions about the underlying data  for example, what a variable means, which values a variable can possibly take on, which observations are included in the data. Based on this assumption we calculate other variables from our data and perform our analysis. However, humans are fallible and data analysis experience shows that the assumptions are sometimes (regularly) false. Sometimes one has misunderstood (or misremembers) the meaning of a variable, there might have been some data entry error, or the data still includes some observations that should have been excluded (e.g., test runs from the researchers instead of participants). For example, in a study by Lewandowsky, Gignac, and Oberauer (2013) the age of one participants was recorded as 32,757 years and this error was only uncovered after the publication of the manuscript. Luckily for them the error did not affect the conclusion drawn from the data, but they had to publish a correction (Lewandowsky, Gignac, and Oberauer 2015). Publishing a correction is nothing dramatic (I have a few paper with published corrections because of errors discovered only after publication), but of course we would prefer not having to do so. And if the errors affect the conclusion substantially, sometimes a correction is not enough and a paper has to be retracted. Regular sanity or assumptions checks in ones analysis are on way to minimise the chance of errors in the final analysis. Based on the positive outcome of the sanity check we are now convinced we have understood the variables in the data and can now calculate our memory score from 0 to 100. For this, we divide each index score by the perfectindexscore and then multiply the results by 100. To simplify the coming analysis, we create a new tibble, mo2014a, that only retains the variables we really need for our analysis using select(). We then take another look at the first six rows of the data using head(). This shows that the data is now ready for our reanalysis. mo2014a &lt;- mo2014 %&gt;% mutate( factual = factualindex / perfectfactindexscore * 100, conceptual = conceptualindex / perfectconceptindexscore * 100 ) %&gt;% select(pid, condition, factual, conceptual) head(mo2014a) #&gt; # A tibble: 6 x 4 #&gt; pid condition factual conceptual #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 103 longhand 35.7 66.7 #&gt; 2 122 longhand 40.5 50 #&gt; 3 142 longhand 33.3 66.7 #&gt; 4 152 longhand 64.3 66.7 #&gt; 5 172 longhand 71.4 66.7 #&gt; 6 183 longhand 26.2 50 6.3.2 Descriptive Statistics Before performing an inferential statistical analysis of the data, we obtain some descriptive statistics. This will provide us with an overview over the data. In addition, the descriptive analysis is another way to check our data and minimise the chances of errors or problems. As a first thing, we want to calculate the number of participants per condition. For this, we again use some tidyverse functions and will explain the steps in more detail. We generally start our tidyverse analyses with the data, here our tibble mo2014a, followed by the pipe %&gt;%. The pipe pipes the tibble to the next function. When obtaining descriptives statistics we often want to get them conditional on a factor/categorical variable in our data. For example, now we want to calculate the number of observations per condition. This can be done by piping the tibble to the group_by() function and condition on the variable of interest, condition. The results of this is a grouped tibble which ensures that all following operations on this tibble are performed grouped (i.e., conditioned on) this grouping variable. We can now pipe this grouped tibble to the count() function to get the number of observations per note taking condition. mo2014a %&gt;% group_by(condition) %&gt;% count() #&gt; # A tibble: 2 x 2 #&gt; # Groups: condition [2] #&gt; condition n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 laptop 51 #&gt; 2 longhand 48 As another sanity check, we can compare this number to the values reported on the OSF for this data (the \\(N\\) by condition is not reported in the original paper). As the numbers match, this further increases our confidence in our data preparation. As the next descriptive statistic, we calculate the condition means for our DV of interest, conceptual memory scores. We also calculate the standard deviation to get an idea of the spread of the data. We again use piping and the tidyverse to get the result. But this time the final function in our pipe is summarise() which allows to calculate summary statistics. mo2014a %&gt;% group_by(condition) %&gt;% summarise( mean = mean(conceptual), sd = sd(conceptual) ) #&gt; # A tibble: 2 x 3 #&gt; condition mean sd #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 laptop 35 23.5 #&gt; 2 longhand 46.5 28.9 This shows that the conceptual memory is more than 10 points larger in the longhand compared to the laptop condition. We also see a difference in around 5 points in the SD. As a side note, we could have added another calculation into the summarise() call. For example, n = n() would have also calculated the number of participants per condition, as the previous code did. One important part of a descriptive analysis should always be a plot of the data. A plot of all data points is usually the best way to see if there is something wrong with the data. Above, we have used afex_plot() after having estimated a model with aov_car(), but we can also invoke ggplot2 directly. For this, we also pipe the data to ggplot() and then build the figure layer by layer. The important part is the mapping of variables in the data to aesthetics in the aes() function. We call this directly in the ggplot() call and mimic the other figures we have seen so far, mapping condition on the \\(x\\)-axis and the DV, conceptual memory, to the \\(y\\)-axis. As Figure 5.1, we begin with a violin plot (geom_violin()) with different quantiles. The violin plot shows the shape of the distribution. We combine this with the individual data points, whcih we show using geom_beeswarm() from the ggbeeswarm package (here we call the function without loading the package beforehand by using package::function()). We then add the mean (with standard error, which will be explained later) using stat_summary() in red. In this plot we see that the data already spans the full range in the \\(y\\)-axis, so we do not need to use coord_cartesian(ylim = c(0, 100)). mo2014a %&gt;% ggplot(aes(x = condition, y = conceptual)) + geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) + ggbeeswarm::geom_beeswarm() + stat_summary(colour = &quot;red&quot;) #&gt; No summary function supplied, defaulting to `mean_se()` Figure 6.2: Conceptual memory scores from Mueller and Oppenheimer (2014). This plot combines individual data points in black with means in red. Before looking at the figure, we see that we got a status message in the console, No summary function supplied, defaulting to `mean_se()`. This message is always shown when using stat_summary() without additional argument and can be safely ignored (i.e., it just indicates that the red point shows the mean and the red error bars show the standard error). The plot itself does not show anything unusual. The plot just reinforces the previous descriptive results: The mean memory score, but also the three displayed quantiles, are larger in the longhand than in the laptop condition. Taken together, the descriptive analysis suggests that there is nothing preventing us from running the inferential analysis. 6.3.3 Inferential Analysis The inferential analysis of the conceptual scores uses exactly the same call as our previous analysis, only with a new data set, mo2014a. mc_mo &lt;- aov_car(conceptual ~ condition + Error(pid), mo2014a) #&gt; Contrasts set to contr.sum for the following variables: condition mc_mo #&gt; Anova Table (Type 3 tests) #&gt; #&gt; Response: conceptual #&gt; Effect df MSE F ges p.value #&gt; 1 condition 1, 97 688.89 4.77 * .047 .031 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 Looking at the ANOVA table of the results shows that the \\(p\\)-value is smaller than .05; the analysis reveals a significant effect of condition. This indicates that this data provides evidence against the null hypothesis of no difference between the note taking conditions. Consequently, we would be justified in saying that the data provides evidence for a difference. To make it easy to detect a significant result, afex, like most statistical software tools, indicates a significant effect with \\(p &lt; .05\\) with one * next to the \\(F\\)-value (in case of \\(p &lt; .01\\) the indication is **, in case of \\(p &lt; .001\\) it is ***, and in case the effect is not significant, but \\(p &lt; .1\\), it is +). afex_plot(mc_mo, &quot;condition&quot;) Figure 6.3: afex_plot() figure for the conceptual memory scores from Mueller and Oppenheimer (2014, Experiment 2) that show a significant difference between the two note taking conditions. 6.4 Summary One reality of research is that a significant results is generally what researchers are looking for. If a results is significant we are happy, our experiment has worked and we can publish it. If it is not significant, we generally have problems publishing our results. References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
