[["index.html", "Introduction to Statistics for Experimental Psychology with R Overview 0.1 Acknowledgments 0.2 License and Attribution", " Introduction to Statistics for Experimental Psychology with R Henrik Singmann 2021-08-28 Overview At this point in time, the book is being written so only few chapters are already available. Chapter 1 provides an introduction to the role of statistics in the research process. It also answers the important question: Why do we need statistics? Chapter 2 provides an overview over important concepts and correct terminology we need to describe research designs (e.g., what is a variable? what is the difference between dependent and independent variables?). It also introduces the distinction between experimental and observational studies. Chapter 3 introduces the basic statistical approach. 0.1 Acknowledgments This project would not exist without the help and feedback provided by others: David Kellen, Lukasz Walasek, Stuart Rosen, Anna Krason 0.2 License and Attribution This book is licensed under the Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) license. This license allows you to share and adapt the work as long as you give appropriate attribution and do not use the materials for commercial purposes. Parts of this book uses other materials released under a compatible CC license. Where it does so, the source is clearly indicated. Please ensure to attribute the original source in case you re-use such materials. "],["role-of-statistics-in-the-research-process.html", "Chapter 1 Role of Statistics in the Research Process 1.1 The Research Process 1.2 Example: The Psychology of Loss Aversion 1.3 Epistemic Gaps: The Difference Between What we Want to Know and What we can Know 1.4 Summary 1.5 Conclusion", " Chapter 1 Role of Statistics in the Research Process In this book we are concerned with experimental psychology, in particular the statistical analysis of experiments in psychology and related disciplines such as language science, behavioural science, cognitive science, or neuroscience. The order expressed in the previous sentence  science first and statistical analysis second  is one of the overarching principles with which we will think about statistics. Whereas the goal here is to introduce the concepts and techniques required to perform a statistical analysis of (mostly experimental) data, the perspective taken here is that a statistical analysis can only be performed or understood within the scientific context it takes place in. One consequence of this perspective is that the start point of any statistical analysis needs to be a specific and clear research question. In the case in which both data collection and analysis is guided by such a research question, a statistical analysis is generally an indispensable part of the research process. As we will describe in more detail in this and the coming chapters, statistics is the tool that allows us to draw inferences that go beyond the data we have observed. This ability to generalise is what allows us to connect experimental results with the research questions and underlying theories. In sum, the statistical techniques introduced in this book can provide meaningful and scientifically helpful answers when the data is collected and analysed with a clear research question in mind. 1.1 The Research Process One way to describe the research process in psychology and related sciences is in terms of four interrelated steps: the research question, the operationalisation of the research question and the data collection, the statistical analysis of the data, and finally the communication of the results. Let us explain these steps in more detail. Any research should begin with a research question. For the disciplines considered here this is often a theory or hypothesis about human behaviour or the human mind. For example, a widely accepted idea in decision making and behavioural science is that people exhibit loss aversion  the displeasure resulting from losing £10 is stronger than the pleasure derived from winning £10. We will look at the example research question whether this is evidence for the idea of loss aversion in more detail in this chapter. What the example already shows is something that is common for research questions, it contains a general statement that involves not directly observable quantities (such as pleasure or displeasure). Not all research questions are as general as the question of whether there is loss aversion. In fact, many research questions are a lot smaller or more applied. For example, in many applied domains an important question is whether a specific intervention  such as a new therapeutical procedure or a workplace training  is better than no intervention or an already existing intervention. Sometimes researcher just want to find an answer to a specific issue that had come up in a previous study or to an open methodological issue. As a concrete example, consider the study by Hinze and Wiley (2011) that addresses a specific question from the research on the testing effect. The testing effect describes the now well established phenomenon that testing of newly learned materials (e.g., self-tests or quizzes) leads to better memory than additional studying (e.g., re-reading) (Roediger and Karpicke 2006). What Hinze and Wiley (2011) noticed was that at that time most research on the testing effect only used limited ways of implementing testing, such as multiple choice tests or open-ended questions. To see whether the phenomenon was more general, their research question was whether the testing effect also occurred for another type of testing, fill-in-the blank tests (i.e., sentences from the learned materials were shown with some words missing that needed to be filled in). The results showed that whereas the fill-in-the blank tests was more effective than mere re-reading (i.e., it showed a testing effect), it was not as effective as testing types that require more involved processing of the materials (e.g., open ended questions). The next step in the research process is the transformation of the research question into an empirical and statistical hypothesis, we will call this step operationalisation. That is, instead of talking about abstract ideas or research questions, we need to concretely decide which study to run. We need to find tasks or other measures (e.g., questionnaires) that allow us to collect data that addresses our research question. An important part of the operationalisation is the specification of relevant variables. We can understand variables as dimensions, features, or characteristics on which individuals or situations can differ. For example, relevant variables for loss aversion are the magnitude of a pontential loss or gain and the intensity of resulting pleasure or displeasure (e.g., from a questionnaire). For the testing effect, relevant variables are the type of additional learning (e.g. re-reading versus multiple choice testing) and the final memory performance. As we can see from the example of the testing effect, sometimes it appears difficult to separate research question and operationalisation. In this case, the research question is tied to a specific aspect of the operationalisation (i.e., how testing is implemented). However, even with a concrete research question there are always other aspects of the study (e.g., what is our measure of learning?) that require further operationalisation. As the example of loss aversion shows, some research questions are very general. Consequently, there are a multitude of possible studies that can be performed to investigate one research question. However, for any specific study a researcher needs to decide on one specific study design. What exactly are the tasks and measures we are using to investigate the question we are interested in? The traditional view of the research process  known as the hypothetico-deductive method  is that during the operationalisation step researchers derive an empirical prediction that tests their theory. That means, the theory coupled with the operationalisation predicts a specific outcome, the empirical prediction, that needs to occur if the theory is true. For example, as we will discuss in detail below, loss aversion predicts that people should be unwilling to gamble money if the chance of losing a specific amount of money is equal to the chance of winning the same amount of money (because losing that amount hurts more than winning that amount). According to the traditional view, if this predicted outcome were not to occur, we would learn that the theory is false. Furthermore, if the predicted outcome were to occur, this would not entail that the theory is necessarily true, but it would provide support for the theory. However, as we will see below (Section 1.3.1), in reality we generally learn less than the prescribed by the idealised form of the hypothetico-deductive method. Research practice often diverges from this idealised view of the research process (e.g., Haig 2014). Not in all cases there is one clear prediction or specific theory that is tested. Researchers might compare multiple theories with diverging predictions, only have a vague hypothesis instead of a fully fledged theory from which more than a single prediction follows, or they might simply be curious about what happens in a specific situation. For example, in the case of the study by Hinze and Wiley (2011), the research question was if the fill-in-the blank test would show the same testing effect as other testing procedures. None of the possible results, even the complete absence of a testing effect for this type of testing, would have provided evidence against testing effects in general. The goal of the research was not to confirm or disconfirm the testing effect. Instead, the goal was to test the generality and boundary conditions of the testing effect. Consequently, it does not seem appropriate to say operationalisation always involves making a specific empirical predictions. However, even in the absence of a specific empirical prediction, the operationalisation must result in an empirical hypothesis relating two or more of the variables that are part of the research design to the research question.1 The final step of the operationalisation is the data collection. Once the research question has been operationalised and the corresponding data is collected it is time for the statistical analysis. Generally, the statistical analysis answers one specific question: Does the data provide evidence for the empirical hypothesis derived from the research question? The remainder of the book will show in detail how to perform statistical analyses for common study designs and how to interpret the results in light of the research question and operationalisation. Once the data is sufficiently analysed, we have reached the final step of the research process, we need to communicate the results (this step is also known as dissemination). There are different forms of results communication depending on your goal and audience (e.g., scientific journal article, dissertation report, conference presentation, or a press release). Whereas the different forms differ in the amount of detail and background that is provided, they all need to provide a truthful and comprehensive account of the whole research process: What is the research question? How was it investigated (i.e., describe the operationalisation)? What are the results? What does this mean for the research question? Often, the difficult problem to solve during this step is to provide a comprehensive and truthful account but in a succinct manner. One important tool for doing so is through graphical means  results figures. Consequently, in this book we will discuss both how to present statistical results in a text and how to create appropriate results figures. Whereas this abstract overview leaves out some important things that are also part of research  such as where research questions come from  it shows three important things. The primacy of the research question. The research question determines the operationalisation and thus which data is collected. The research question also determines the statistical analysis, but indirectly; the research question determines the empirical hypothesis which is then tested in the analysis. The statistical analysis is not directly connected to the research question. The statistical analysis is performed on the operationalisation of the research question, but not on the research question itself. What this means is that the statistical analysis itself cannot directly inform us about the research question, or in other words, we cannot statistically test the research question. Instead, statistics can only tell us something about a specific operationalisation. Whether or not this allows strong inferences about the research question depends on the operationalisation. And as shown in the following example, an important part of the scientific discourse is to argue whether certain operationalisations allow one to address specific research questions. However, this is generally not a statistical question. Statistics is not the end goal of the research. Instead, the end goal is usually a written communication of the research. In most cases, the statistical analysis is an indispensable part of this communication that can provide evidence for or against a specific empirical hypothesis. However, to understand the full meaning and implications of a particular statistical result, it is important to know its context  the research question and its operationalisation. It is the task of the research to communicate this context when communicating the research. Without the context, the impact and meaning of a statistical result is severely limited. 1.2 Example: The Psychology of Loss Aversion To get a better understanding of the research process and the problems that can arise in it, let us consider a concrete example research question and how it can be investigated empirically in detail. Specifically, let us return to the example of loss aversion. Loss aversion is one of the assumptions underlying prospect theory (Kahneman and Tversky 1979), a mathematically formalised theory combining cognitive psychology with economic theory.2 The concise description of loss aversion is that losses loom larger than gains (Kahneman and Tversky 1979, 279). As described above, loss aversion means that the negative feeling associated with a loss of a certain amount is larger than the positive feeling associated with a gain of the same amount. For example, loss aversion predicts that the displeasure or pain from losing £10 is larger than the pleasure or joy from winning £10. We can see that loss aversion is a theoretical statement involving latent - that is, unobservable - quantities such as negative or positive feelings (i.e., displeasure versus pleasure). We can ask people how they feel, but we cannot easily observe feelings without asking people. In psychology we generally call unobservable theoretical concepts constructs. So how can we test whether people indeed show loss aversion if we cannot directly observe the constructs that form the core of it? One possibility for testing the hypothesis that individuals show loss aversion is hinted at above. We could either give people a certain amount, say £10, or take it away from them, and then ask them how they feel. This procedure runs into at least two problems. First, it is clearly ethically unacceptable to perform an experiment that consists of taking £10 away from our participants. Second, even if we were to overcome the ethical problems (e.g., by first giving participants an endowment and only taking money away from that endowment) there would still be the problem of how to measure the feelings associated with the two events. One way to avoid the ethical problem of taking away money from participants is to ask them to imagine how they would feel if they lost or gained a certain amount of money. Even though it is not certain whether this imagined feeling corresponds to the actual feeling the participants would have when actually losing or gaining this amount, this procedure is commonly used. For example, McGraw et al. (2010) asked their participants to imagine that they would play a single game with a 50% chance of losing $200 and a 50% chance of winning $200 (e.g., flipping a coin and if it comes up head you win $200 otherwise you lose $200). They then asked participants to imagine how they would feel for either of the two outcomes. However, they identified two different ways to ask this question, both of which are shown in Figure 1.1 below (McGraw et al. 2010). The first possibility, a bipolar scale, is shown in the upper part. On this response scale participants in the loss condition respond on the left side of the scale and participants in the gain condition respond on the right side of the scale. To compare the ratings, they measured participants responses as the absolute distance of the rating from the neutral point No Effect (i.e., Small Positive Effect would be treated as the same intensity as Small Negative Effect). The second possibility is shown in the lower part of Figure 1.1 and shows a unipolar intensity scale. On this scale, participants in both conditions provide their response on the same scale and only rate the intensity of their displeasure or pleasure. Figure 1.1: Example of two different scales for measuring feelings after a potential gain and loss. The upper part shows a bipolar scale in which losses receive a rating on the left side (of No Effect) and gains receive a rating on the right side. The lower part shows a unipolar scale in which the intensity for both losses and gains are given on the same scale. Image adapted from McGraw et al. (2010). Before reading on, take a moment and ask yourself if you believe the two scales shown in Figure 1.1 make a difference for whether or not participants show loss aversion. Or put differently, can you think of a reason why it matters for loss aversion, how we ask for participants feelings after a loss or gain? And if there were a difference, on which scale would you expect it to be more likely that loss aversion occurs? To investigate the question of whether there is loss aversion for both scales, McGraw et al. (2010) asked half of their participants to use the bipolar scale and the other half of their participants to use the unipolar scale to rate their feelings after the imagined loss and gain of $200. With this data in hand they then compared the feeling ratings for imagined losses and wins for each of the two conditions. Their results showed that it indeed matters which scale is used. For the bipolar scale there was no evidence for loss aversion. The feeling ratings for gains and losses were approximately equal at around 3.4 (where 1 = No Effect and 5 = Very Large Effect). However, when using the unipolar scale they found evidence for loss aversion. In the loss condition participants reported an on average stronger feeling (at around 3.6) compared to the gain condition (at around 3.1). McGraw et al. (2010) explain the results in terms of relative versus absolute feeling judgements. For the bipolar scale, people first judge the valence of the feeling  that is, whether it is good or bad  to determine the side on which they have to provide their response. Once this is done, they only then judge the intensity of the feeling. However, as the intensity judgement follows the valence judgement, they make this intensity judgement only by comparing against other feelings of the same valence. More specifically, McGraw et al. (2010) assume that in the loss condition the loss is only compared against other negative events and in the gain condition the gain is only compared with other positive events. In other words, for the bipolar scale participants only make a relative judgement of intensity these cannot compare for loss and gains. Consequently, they argue that the results from bipolar scale are not diagnostic for the question whether or not there is loss aversion.3 For the unipolar scale, McGraw et al. (2010) argue that the judgement of feeling intensity is not preceded by a valence judgement. Therefore, people use an absolute judgement of feeling, comparing it with both negative and positive feelings. Consequently, this data, which shows a pattern in line with loss aversion, is diagnostic for the question of whether or not there is loss aversion. Overall they conclude that their study provides evidence for loss aversion, because it appears when using the unipolar scale. What the results of McGraw et al. (2010) show is that seemingly minor differences in the operationalisation of a research question can have a tremendous effect on the results. In line with this, it is perhaps not too surprising that this operationalisation for investigating loss aversion  asking participants about their feelings  is not very common. Below we will discuss alternative operationalisations. And whereas McGraw et al. (2010) provide an explanation for the results, it is an explanation that might have been difficult to come up with before having seen the data. This inability to be able to predict what the effect of the response scale on the results are, has potential consequences that are further reaching. If we take their results seriously and generalise them to other domains, we might conclude that whenever we are interested in participants feelings it matters whether we use bipolar or unipolar scales. One could even go another step further and say that whenever we use subjective rating scales the type of the scale could have an unintended effect on the results. Because of this, it is often a good idea to try to find operationalisation of research questions that do not only involve subjective rating scales but other types of responses such as choices, response times, or more complex behaviour. 1.2.1 Evidence for Loss Aversion: Lotteries A different operationalisation for testing loss aversion is the comparison of choice patterns across different risky choices, lotteries, or gambles (these terms can be understood interchangeably here), a common experimental paradigm in decision making and behavioural economics. A lottery in this context consists of different options, each of which is associated with one or multiple outcomes, from which the participant has to choose one. The simplest type of lotteries are ones consisting of only one option. In this case, participants can only decide whether to accept or reject the lottery. For example, evidence for loss aversion can be found in the lotteries used by Battalio, Kagel, and Jiranyakul (1990). One of their lotteries (their Question 15) was: Would you play the following gamble: A 50% chance of losing $20 or a 50% chance of winning $20. Participants then had to decide whether to accept the lottery or reject the lottery.4 The results showed that 43% of participants accepted this lottery. So how does this result provide evidence for loss aversion? The evidence comes from the fact that participants are more likely to reject the lottery than to accept it (i.e., acceptance rate is below 50%). What is important about this lottery is that it is a symmetric 50-50 lottery. That is, the magnitude of the potential loss was equal to the magnitude of the potential gain and both possible outcomes appear with equal probability of 50%.5 Remember that loss aversion means disliking a loss more than liking a gain of the same magnitude. A symmetric lottery is exactly such a situation. The fact that participants generally dislike the symmetric lottery is therefore very much in line with loss aversion. One problem with the results of Battalio, Kagel, and Jiranyakul (1990) is that they only collected data from 35 participants which did not provide statistically compelling evidence for loss aversion. Whereas the data shows a descriptive pattern that is in line with loss aversion (i.e., below 50% acceptance of symmetric lotteries) this is not supported by a statistical analysis. More specifically, the statistical analysis does not provide support for the empirical prediction that the observed acceptance rate is below 50%. It does descriptively look like this, but the evidence in the data is not enough to surpass the statistical criterion we use for judging the evidence. More compelling evidence for loss aversion comes from a study by Brooks and Zank (2005). In their task, participants were asked to make a decision about more complex lotteries in which participants had to decide between two options. For example, one of the lotteries was the following: Which option do you prefer? A: A 25% chance of +£11, a 50% chance of £0, or a 25% chance of -£11. B: A 25% chance of +£10, a 50% chance of £0, or a 25% chance of -£10. We see that for each of the two options, we have two symmetric outcomes of the same magnitude. For both options A and B, there is a 25% chance of a loss and a 25% chance gain of the same magnitude (and with 50% probability participants neither lose nor gain anything). The difference between A and B is that for A, the magnitude of the potential loss and gain is larger by £1. The notion of loss aversion makes an interesting prediction in this case. Participants should increasingly dislike a symmetric lottery the larger the potential outcomes are. For our lottery this means that if we dislike losing £10 more than liking gaining £10, this difference between dislike and liking should be larger if the potential outcomes are £11. In terms of empirical prediction this means that participants should be more willing to choose option B than option A in the lottery above. In the study of Brooks and Zank (2005), each of 49 participants worked on around 50 lotteries that were all similar in structure to the lottery above. That is, for each of the two options there was a loss and gain of the same magnitude with the same probability (the third potential outcome was always smaller in magnitude than the loss/gain). As in the example lottery above, the difference between both options was always that the loss/gain magnitude for one option was £1 larger than for the other option. In line with the empirical prediction of loss aversion, participants chose the option with smaller magnitude of loss/gain in 63% of cases. And given the larger sample size in this study, the statistical analysis also supported the prediction that this 63% was larger than 50%. So does this all show that there is such a thing as loss aversion? Results such as those from Brooks and Zank (2005) certainly appear to support this theoretical idea (see also C. Camerer 2005). And this also makes intuitive sense. Most people (me included) feel that symmetric lotteries are not really attractive and become increasingly unattractive with increasing magnitude (who really thinks flipping a coin for the chance of losing or winning say £100.000 sounds like a good idea?). However, as in the previous example, the evidence for loss aversion discussed here hinges on a particular operationalisation of the theoretical idea. From these results, we have not really learned that the magnitude of a loss or gain is the psychologically relevant factor in the minds of people. The only thing we have learned is that people dislike certain lotteries or options in lotteries. Can we find an alternative for this data pattern that does not involve loss aversion? 1.2.2 Alternative Explanation: Loss Aversion or Loss Seeking? One clever alternative explanation for the why it looks like there is loss aversion was provided by Walasek and Stewart (2015). In their study, participants were also presented with 50-50 lotteries. An example of one of their lotteries is shown in Figure 1.2 below. As shown in the figure, each lottery involved mixed outcomes, that is both gains and losses (i.e., -$18 and +$20 in the example). Furthermore, each lottery consisted of only one option so participants only had to choose whether to accept and play a lottery or not. Finally, both possible outcomes always had a 50% probability of occurring. To make this logic clearer to participants, they were told that accepting the lottery shown in Figure 1.2 was equal to flipping a coin that has -$18 on one side and +$20 on the other side. Depending on which outcome comes out on top, their money would change accordingly. If participants rejected a lottery, they neither lost nor won any money. Figure 1.2: Screenshot of lottery task used to investigate loss aversion. This screenshot is from Walasek and Stewart (2015, Figure 1). As many experiments that fall within a cognitive domain, the study of Walasek and Stewart (2015) consisted of a series of similar trials in which participants had to do the same task (i.e., accept or reject the shown lottery). What differed across trials were the values of the two possible outcomes. For example, in one of the conditions of the experiment losses ranged from -$6 to -$20 in increments of -$2 (resulting in 8 different possible losses) and gains ranged from $12 to $40 in increments of $4 (resulting in 8 different possible gains). Across all trials for one participant in this condition, all possible losses were combined with all possible gains so that in total participants had to decide for \\(8 \\times 8 = 64\\) lotteries whether they accepted or rejected it. The 64 trials each participant of Walasek and Stewart (2015) worked on contain a subset that allowed us to directly address the question of whether there is evidence for loss aversion.6 Whereas most of the lotteries shown to participants were asymmetric  that is, the potential loss differed numerically from the potential gain (such as for the example in Figure 1.2)  a small subset of lotteries were symmetric, for these lotteries the amounts for the potential loss was equal to the amount of a potential gain. More specifically, the symmetric lotteries were -$12/+$12, -$16/+$16, and -$20/+$20. For the condition described above in which losses ranged up to -$20 and gains ranged up to +$40, the 191 participants accepted the symmetric lotteries only 21% of the time. 21% is descriptively below 50% indicating that participants indeed disliked these lotteries more than they liked them (i.e., they were overall more likely to reject than to accept the symmetric lotteries). Furthermore, a statistical analysis supported the empirical hypothesis (i.e., provided evidence that this results pattern generalises beyond the current data). As described above, this result makes sense in light of loss aversion. When losing a certain amount of money is worse than winning the same amount of money, one should reject a symmetric lottery in which one is equally likely to lose or to win a certain amount of money. The clever manipulation of Walasek and Stewart (2015) was that they included three further conditions in which they changed the range of possible outcomes. In addition to the -$20/+$40 condition discussed above, the 202 participants in the $-20/+$20 condition saw lotteries with losses ranging to -$20 and gains also ranging to +$20 only. Another group of 190 participants, the -$40/+$40 condition, saw lotteries with losses ranging to -$40 and gains also ranging to +$40. Finally, Walasek and Stewart (2015) also included a -$40/+$20 condition with 198 participants in which the losses ranged to -$40, but the gains only to +$20 (i.e., the complement to the -$20/+$40 condition). Importantly, in all conditions the number of possible outcomes for losses and gains was 8 (so the step size was either \\(\\pm\\)$2 or \\(\\pm\\)$4). Table 1.1 below shows the possible outcome for each condition Table 1.1: Possible outcomes for the lotteries in Experiment 1 of Walasek and Stewart (2015). In each condition, each participant saw 64 lotteries resulting from combining all possible gains with all possible losses in that condition. Condition -$20/+$40: Gains $12 $16 $20 $24 $28 $32 $36 $40 -$20/+$40: Losses -$6 -$8 -$10 -$12 -$14 -$16 -$18 -$20 -$20/+$20: Gains $6 $8 $10 $12 $14 $16 $18 $20 -$20/+$20: Losses -$6 -$8 -$10 -$12 -$14 -$16 -$18 -$20 -$40/+$40: Gains $12 $16 $20 $24 $28 $32 $36 $40 -$40/+$40: Losses -$12 -$16 -$20 -$24 -$28 -$32 -$36 -$40 -$40/+$20: Gains $6 $8 $10 $12 $14 $16 $18 $20 -$40/+$20: Losses -$12 -$16 -$20 -$24 -$28 -$32 -$36 -$40 As a consequence of this design, what changed across conditions was whether the symmetric lotteries were relatively good or relatively bad. To understand this, we need to look at the remaining asymmetric lotteries. In the -$20/+$40 condition discussed so far, there were more lotteries in which the possible gain was larger than the possible loss (e.g., a -$18/+$20 lottery) than lotteries for which the possible loss was larger than the gain (e.g., a -$20/+$18 lottery). Consequently, the symmetric lotteries were relatively bad (i.e., compared to the many lotteries in which the possible gain is larger than the possible loss). In the -$20/+$20 and -$40/+$40 conditions, the asymmetric lotteries were balanced. In half of the asymmetric lotteries the possible gain was larger than the possible loss, whereas for the other half the possible loss was larger than the possible gain. Consequently, the symmetric lotteries were neither relatively good nor relatively bad. Finally, in the -$40/+$20 condition the pattern was flipped with respect to the -$20/+$40 condition. There were only a few lotteries in which the possible gain was larger than the possible loss compared to the many lotteries for which the possible loss was larger than the gain. Consequently, the symmetric lotteries were relatively good. So does it matter whether the symmetric lotteries are relatively good or not? Indeed it does. As a reminder, people were unlikely to accept the symmetric lotteries in the -$20/+$40 condition in which the symmetric lotteries were relatively bad. Participants in this condition only accepted 21% of the symmetric lotteries. In the -$20/+$20 condition in which the symmetric lotteries were neither relatively good nor relatively bad, participants accepted 50% of the symmetric gambles. Similarly, in the -$40/+$40 condition participants accepted 42% of the symmetric gambles. Finally, in the -$40/+$20 condition in which the symmetric lotteries were relatively good, participants accepted 71% of the symmetric gambles. We can see that 71% is descriptively above 50%, indicating that participants here liked the symmetric lotteries more than they disliked them (i.e., they were overall more likely to accept than reject the lotteries). Furthermore, a statistical analysis supported the empirical hypothesis (i.e., provided evidence that this results pattern generalises beyond the current data). What these results show is that the choice pattern for lotteries are not always in line with the idea of loss aversion. Only in a context in which a symmetric lottery is relatively bad do we see evidence in line with the idea of loss aversion. If we are in a context in which a symmetric lottery is relatively good, we see the opposite pattern that one could term loss seeking. What does this mean for loss aversion? The original idea of Kahneman and Tversky (1979) that what matters is the magnitude of a loss or gain surely is not in line with the results of Walasek and Stewart (2015). Instead, Walasek and Stewart (2015) argue that what is relevant to determine the psychological impact of a gain or loss is the relative magnitude or rank of a possible outcome: Compared to other gains or losses I regularly experience, is this a large gain or large loss? Before moving on and linking this example of the psychology of loss aversion to the general goal of this book, let us answer one last question. If what matters is the rank (as suggested by Walasek and Stewart (2015)) and not the magnitude of an outcome (as proposed by Kahneman and Tversky (1979)), why do we see evidence for an effect of magnitude in other studies investigating loss aversion (e.g., Brooks and Zank 2005) that did not manipulate the context of gains and losses? An answer to this question is provided by Stewart, Chater, and Brown (2006). They argue (and also provide empirical evidence) that in our daily lives we experience more small losses (e.g., buying something at the bakery) and more larger gains (e.g., the monthly salary). As a consequence, for a gain and loss with the same magnitude the relative rank of the gain compared to all other gains is lower than the corresponding relative rank of the loss compared to all other losses. From this difference, we should generally observe a pattern consistent with loss aversion but for a different theoretical reason than proposed by Kahneman and Tversky (1979). 1.3 Epistemic Gaps: The Difference Between What we Want to Know and What we can Know The goal of this chapter is to provide an introduction to the role of statistics in the research process. Why do we need statistics and what can it tell us about the research questions we are interested in? However, so far we have not talked much about statistics. Instead we have introduced an abstract concept of the research process and exemplified it with the example of loss aversion. What this example has tried to show is that even though we can find statistical evidence that supports our empirical hypothesis, this does not mean we found an answer to our research question. The question of whether or not there is loss aversion is not a statistical question. The answer to the research question depends on whether we can rule out possible alternative explanations, such as that the magnitude does not matter, but the rank of a potential loss or gain. We nevertheless need statistics to help us determine whether our data provides support for a particular empirical hypothesis that operationalises our research question. To put this somewhat more bluntly. If you do not yet know about statistics in detail, the application of statistical methods can appear like a magical machinery that provides us with an answer to the research question we have. You throw your data in, turn the statistics machinery on, and get an answer to your research question out. Sadly, this image of statistics is false. The reason is that there are at least two epistemic gaps in the research process that prevent us from getting a straight answer to our research question. In this section we will introduce these gaps to ensure you can get a realistic image of the role of statistics in the research process. Before explaining what an epistemic gap is, we need to take a step back and think about science in general. There are at least two different parts that constitute a scientific discipline. Firstly, we have the substantive content, what the science is about. For example, psychology is concerned with the human mind and behaviour. So one part of psychology is theories about mind and behaviour (e.g., people exhibit loss aversion). But, just the theories is not enough. The problem is that there are many intuitive and good sounding but ultimately untrue theories (e.g., the idea that there are distinct learning styles; see Pashler et al. 2008). So the second part constituting a science is research; systematic investigations of our research questions that provide us with evidence. This evidence allows us to decide which theories to believe and which theories to discard. This perspective on science allows us to draw some conclusions about what this means for us as scientists. Firstly, it is important not to adopt theories too easily and early. As scientists we need to be natural sceptics. Instead of believing in a theory because it sounds compelling, we need to ask for the evidence first. We then need to evaluate the evidence and use this as the basis for the strength or degree of our belief. For example, if there are a few independent studies that support a certain theoretical position (as in the case of loss aversion) it seems appropriate to consider a certain theoretical position as a possibility but also consider alternative accounts as possible (as the rank based account of Walasek and Stewart 2015). If the evidence is weaker, say only studies from the proponents of a theory, we should be even more cautious in the degree of belief we assign to a theory. And only if the scientific evidence is overwhelming should we be willing to treat a theory as approximately true. For psychology, there are not many theories for which the majority of researchers would agree that the latter criterion is reached (besides maybe operant and classical conditioning). As a consequence, in many cases the best thing to do is to be honest and admit that the evidence is not sufficient to hold a strong theoretical position. For us scientists, this perspective also means that we need to be able to evaluate the evidence from the research in our domain of interest. Doing so requires substantive knowledge about our research domain, but also statistical knowledge that we will introduce in this book. Finally, it means that we need to be open to revise our beliefs in light of new evidence (as in the case of loss aversion and the results of Walasek and Stewart 2015). As the last two paragraphs show, science is ultimately about evidence. What evidence do we have to believe in the theories in our field? As scientists we hope that statistics is a tool that helps us answer the question about evidence. And to some degree it does, but not as much as we would hope. We need to be aware of the epistemic gaps. Let us explain now what this means. Epistemology is a branch of philosophy that is concerned with knowledge (e.g., what is knowledge, how do we know that we know) and justification (e.g., what is the reason I believe something) (for a philosophical overview see: Steup and Neta 2020) and epistemic is the corresponding adjective. Epistemology therefore is the field that deals with a philosophical questions at the core of science, which theories should I believe based on the available evidence. An epistemic gap describes the difference between what we want to know and what we can actually know.7 Ideally, we want to know whether a theory or hypothesis is true. As we have already seen in the example above, in many cases even carefully designed experiments cannot unambiguously answer this question. In the following, we will take a more abstract look at this question and discuss two general problems that further complicate this issue. What we can know is often quite different from what we would like to know. A competent application of statistics requires that one is aware of this problem and avoids over-interpreting the results from ones research. 1.3.1 Epistemic Gap 1: Underdetermination of Theory by Data Above we have provided an example of a theoretical (or basic) research question (Is there evidence for loss aversion?). We have seen that answering the research question requires careful thinking about the operationalisation; how exactly should we set up a study to test this question? And once we have decided on one operationalisation, we have seen that we can find alternative explanations that can explain the results without making the assumptions of the original research question. In other words, even though the operationalisation was carefully chosen, it could not unambiguously answer our research question. The fact that we could not compellingly answer the research questions despite employing a carefully chosen operationalisation is not a problem that is unique to our example. In contrast, an important insight from philosophy of science is that this is a problem of any empirical study. This issue is also known as underdetermination of theory by data or the Duhem-Quine thesis and always occurs if there is a difference between the research question and the corresponding operationalisation. And as we have seen in the examples, there essentially always is. There are different aspects or different angles with which we can look at and understand underdetermination. The first aspect has to do with the specification of the research question and its operationalisation. Research questions usually involve unobservable constructs  such as emotions (e.g., fear), memory, attention, comprehension, or learning  or vague phrases, such as works better or improves. An empirical investigation of such questions however requires a precise specification and operationalisation. By going from the research question to the concrete operationalisation, there is no guarantee that the operationalisation captures the intended meaning of the research question. For example, consider a test of a new therapeutical intervention compared with the standard or old treatment. Imagine we found that the new intervention decreased patient scores on a symptom questionnaire more strongly than the old treatment, but did not reduce the number of sick days due to the disorder. Should we interpret this in the way that the new treatment works better than the old treatment? In some sense it does, but in another it does not. The problem is that the nuances that result from operationalising a research question concretely do not always align with the broad way in which we like to think about research questions. At this point you might think this does not yet sound like such a big problem. We just need to define our research questions precisely enough and then we are able to learn something about our research question. Sadly, this is easier said than done. The first problem is that it is often impossible to precisely define our research question, because we have not yet found a way to precisely define the constructs that are involved in it (this is known as the problem of coordination, (Kellen et al. 2021)). And in most cases precisely defining these abstract constructs is not possible anyway. For example, if you have the hypothesis that a specific emotion, say fear, is related to some behavioural pattern, say aggression, you run into the problem that there is not a generally agreed upon definition of either of these constructs. There probably exist questionnaires for measuring fearfulness and aggressive tendencies, but these questionnaires do not represent the corresponding constructs or a definition of them. If you were to ask a sample of participants to fill out these questionnaires and found that the scores of the participants in these two questionnaires are related, it would not allow you to conclude that fearfulness and aggression are related. The only conclusion that would be allowed is that fearfulness as measured with the questionnaire is related to aggression as measured with the questionnaire. Of course, as scientists we would like to make the general conclusion that the constructs are related, but such an inference is not logically allowed. The general problem that we run into is the Duhem-Quine thesis; any empirical hypothesis that is tested in a study contains two parts: The theoretical prediction as well as a set of auxiliary assumptions that links the theoretical prediction with the data. To stay with our example, our theoretical prediction could be that fear and aggression are related. The auxiliary assumption are all additional assumptions that are needed to test this question empirically as decided on as part of the operationalisation: that the questionnaire is a valid measure of the constructs (which is a big assumption), that the data collection took place without any unforeseen problems, that we have tested enough participants to find an effect, that we use appropriate statistical procedures, etc. As can be seen, the list of auxiliary assumptions is somewhat limitless and difficult to enumerate fully. It also contains quite mundane assumptions such that we have to assume that the research actually took place and is not just made up by the researcher (for an exception, see the case of Diederik Stapel). The core of the Duhem-Quine thesis is that any empirical result does not pertain solely to the theoretical prediction of interest, but the union (or conjunction) of the theoretical prediction of interest with the auxiliary assumptions. If the results are in line with the empirical hypothesis, that only supports the theoretical prediction if all auxiliary assumptions are true. Likewise, if the results are not in line with the empirical hypothesis, this only provides evidence against the theoretical prediction if all auxiliary assumptions are true. However, testing whether all auxiliary assumptions are true cannot be done in the same study that tests the empirical hypothesis we set out to test (because we can always come up with more and more auxiliary assumptions not specifically tested). Consequently, any individual result on its own cannot provide conclusive evidence for or against a particular theoretical prediction, there can always be an alternative explanation that differs from the theory or hypothesis one has.8 That is what is meant by the underdetermination of theory by data. Whereas this issue might seem like a purely philosophical discussion, it is far from it. Most actual scientific discussions in the literature are about the auxiliary assumptions that are part of the operationalisation of a research question. For example, the argument for loss aversion as proposed by Kahneman and Tversky (1979) hinges on the auxiliary assumption that participants interpret the possible outcomes of the lotteries in terms of their magnitude or absolute value. As shown by Walasek and Stewart (2015), at least in some cases this auxiliary assumption does not appear to hold and participants instead interpret the relative value of the possible outcomes of the lotteries. It will be easy to find similar examples for the research area you are interested in. To sum this up, the problem of underdetermination and the first epistemic gap is that any particular result never uniquely supports or challenges one theoretical position or hypothesis. For any result that appears to support a theory there is another theory that makes the same prediction because an auxiliary hypothesis could be false and thus require a different theory. Likewise, for any result that seems to disagree with a theory, the theory can always be protected by claiming one of the auxiliary assumptions is incorrect. And this is also exactly what happens in real scientific discourse. As an example, when John Bargh, a prominent social psychologist from Yale, was confronted with results that disagreed with one of his most prominent findings (Doyen et al. 2012) he attacked (in a now deleted blog post that still can be found here) the incompetent or ill-informed researchers and claimed their study had many important differences from our procedure, all of which worked to eliminate the effect. As this section has shown, questioning the methods (i.e., the auxiliary assumptions) is a legitimate defence that protects ones theory. Of course, one can question the auxiliary assumptions of the original results that appeared to support the theory in the same way. In the case of Bargh, it appears that this is exactly what happened. Most other psychologists have stopped believing his original finding (e.g., Harris, Rohrer, and Pashler 2021). 1.3.2 Epistemic Gap 2: Signal versus Noise The first epistemic gap is that there is no strong logical link between the theories underlying our research questions and the operationalisation of the research question. Thus, in terms of the steps in our research process it concerns the relationship between step 1, the research question, and step 2, the operationalisation and data collection. The next epistemic gap concerns the relationship of steps 2 and step 3, the statistical analysis. As described above (1.1), the important task during the operationalisation is to transform the research questions into an empirical hypothesis. Ideally, this hypothesis comes in the form of an empirical prediction, describing which possible outcome would support our theoretical hypothesis (i.e., the outcome predicted by our theory). As part of this we should also clearly designate a possible outcome that, if it were to occur, would speak against our theoretical hypothesis. Once we have decided on this, we collect the data and then run the statistical analysis. The goal is that statistical analysis provides us with evidence with respect to the empirical hypothesis. Does the data support the empirical hypothesis or does it not? Before providing an overview of how this is done, let us go back to the example above, the study of Walasek and Stewart (2015). In contrast to the original formulation of loss aversion (Kahneman and Tversky 1979), which is based on the magnitude or absolute value of a gain or loss, the theoretical prediction of Walasek and Stewart (2015) was that what drives peoples behaviour is the relative value of a gain or loss. To test this, they presented participants with lotteries in different conditions in which the range of gains and losses differed. In one condition there were small losses and large gains and in another condition there were large losses and small gains (we ignore the other two conditions here). The hypothesis that follows from this design is that the relative attractiveness of the symmetric lotteries (in which magnitude of possible loss = magnitude of possible gain) differs in both conditions. In the small loss/large gains condition the symmetric lottery is relatively unattractive and in the large loss/small gains condition it is relatively attractive. The resulting empirical prediction is that participants should be less willing to accept the symmetric lotteries in the small loss/large gains condition than in the large loss/small gains condition. In line with this prediction, participants in the small loss/large gains condition accepted only 21% of the symmetric lotteries whereas participants in the large loss/small gains condition accepted 72% of the symmetric lotteries. From just looking at the bare numbers, the results appear to support the empirical prediction. Participants are roughly 50% less likely to accept the symmetric lotteries in the small loss/large gains condition than in the large loss/small gains condition. However, how can we be sure this particular difference is not a chance occurrence. Maybe we just got unlucky and the participants in the small loss/large gains condition are for some reasons generally less likely to accept any lotteries than the participants in the large loss/small gains condition. Maybe the former participants all had a horrible night of sleep and are in a really bad mood at the time of testing and therefore reject all gambles whereas this was not the case for the latter. If this were the case, the observed difference would not actually tell us anything about our research questions. The problem described in the previous paragraph is at the heart of the statistical approach described in this book. The core problem is that the responses we get from participants in experiments are inherently noisy. Human participants can do things for an unlimited number of reasons. Some of these reasons are related to our research question and its operationalisation and other reasons are not. For example, if participants read the lotteries carefully and think before they provide an answer, it is likely that the values of the possible outcome play a role in their answer. In this case, their responses are relevant for our research question. But what if participants are distracted by a message on their phone and do not read the problem fully? Or what if they intend to accept a lottery and accidentally reject it (i.e., press the wrong button)? We can also imagine that it matters if participants are relatively rich or relatively poor. For someone with a million in the bank, it might not really matter if they lose or win $16 so they might be inherently more likely to gamble on such a lottery than someone for whom this is more than the hourly wage. In all these cases, the values of the lotteries have a minor effect and thus the responses are more or less irrelevant for our research question. For the question of whether or not the results support our empirical hypothesis we therefore would like to distinguish between those responses that are relevant for our research question  we can call this the signal  and those responses that are generated more or less randomly and are irrelevant for our research question  we can call this the noise. If we had a procedure that could distinguish signal from noise we could then simply see whether the signal supports our empirical prediction. If it did, the data would provide support for the hypothesis and if not the data would not support the empirical hypothesis. Sadly, such a procedure does not and cannot exist (as we would then know why people do what they do, which is the reason we do research in the first place). In the absence of a procedure that can definitely separate the contribution of signal and noise, the statistical approach introduced here compares an estimate of the signal with an estimate of the noise. Let us assume for a moment the estimated signal supports our empirical prediction as in our example (i.e., we predict that participants are less likely to accept a lottery in the small loss/large gains condition and this is what the data shows). We then compare this estimated signal against the estimated noise. If the estimated signal is large given the estimated level of noise, we assume that the data supports the empirical prediction. If the estimated signal is not large given the estimated noise, we assume the data does not support our empirical prediction. So how can we estimate the signal and the noise? Estimating the signal is straight forward. We just use the observed difference between the conditions as our estimate of the signal. So for the example from Walasek and Stewart (2015) this would be the observed difference in accepting the symmetric lotteries between the two conditions which was roughly 50%. Estimating the noise is a bit more complicated and will be described in detail in later chapters. For now it is enough to understand that it is affected by two components: (1) The variability in responses within each condition and (2) the overall sample size (i.e., number of participants). If the variability within each condition becomes smaller (i.e., measurement becomes more precise) and the sample size stays the same, the levels of noise decreases. Likewise, if the sample size increases with a constant level of variability, the level of noise decreases. Another important question is what counts as large when comparing the estimated signal to the estimated noise. In the following chapters we will introduce a decision threshold for the signal to noise ratio to make this judgement.9 If the signal to noise ratio is above the threshold, we act as if there were a signal and change our beliefs. If it is not, we cannot make a decision. This decision threshold is chosen such that across many such decisions we control the rate of making false positive decisions. In particular, the decision threshold is chosen such that across many situations in which there is no signal, we only incorrectly assume that there is a signal in 5% of decisions. Taken together, the statistical procedures we are using attempt to answer the question whether there is a signal that supports the empirical hypothesis given that human data is inherently random and noisy. The problem in this is that the estimated signal  the observed difference between the conditions  is also affected by the noise. We never know if the observed difference is due to the signal we are interested in or just based on noise. To overcome this problem we compare the observed signal with the observed level of noise. If the observed level is large relative to the observed noise, we decide the data supports the presence of the signal. In other words, we never really know if the current data really supports our prediction or not, we just act as if it does. There always is a non-zero chance that the effect is due to the noise. Because we only have this one data set we are analysing, we cannot be 100% certain our estimate of the signal and the noise is fully accurate. However, as we will describe in detail later, across decisions that use this statistical decision procedure, it controls our rate of making false positive decisions (i.e., assume there is a signal when there is none). So as in the case for the first epistemic gap, the second epistemic gap also shows that we cannot really learn what we wanted to know, if the data supports the empirical hypothesis or not. If the signal is large relative to the noise, we have evidence that it does, but this evidence is never fully conclusive. The evidence might be strong, and we will later see how we can identify that, but there always should be some remaining doubt in the back of our head. Maybe we just got unlucky and the participants in our study responded in a way that made it look like there is a signal, but there isnt. With just one data set in hand, this cannot really be ruled out. 1.4 Summary In this chapter we have provided a conceptual overview of the research process in psychology and related disciplines. In this concept, the research always begins with a research question. What is it that I want to know? Often this research question stems from a particular theory that we want to test, but it can also be a purely applied hypothesis. The next important step is the operationalisation of this question followed by the data collection. That means we need to find appropriate tasks or measures and develop a study design with which we can test the empirical hypothesis following from our research question. As we have seen in the discussion of the first epistemic gap, the consequence of the separation of research question and operationalisation is that, strictly speaking, our study only lets us learn more about the tasks and measures we are using. Because of the problem of underdetermination of theory by data, even an apparently positive result does not allow us to infer that it supports our theoretical hypothesis. The core problem is that the empirical hypothesis is a combination of a theoretical hypothesis and auxiliary assumptions and we cannot rule out that one of the auxiliary assumptions is false. With the collected data in hand, the next step is to perform the statistical analysis. Here, we hope to find evidence that informs us about our empirical hypothesis. The procedure we will use in this book attempts to distinguish between the signal in the data, the part of the data relevant to our empirical hypothesis, and the noise, the randomness that is inherent in using human participants.10 However, the second epistemic gap entails that even with such a statistical procedure, we cannot find fully conclusive evidence. The problem is that we cannot estimate the true amount of noise in the data. Research participants have a myriad of potential reasons for why they show a certain behaviour and these reasons need not be related to our research question. With more precise measures and more participants, we can control this level of noise to some degree, but ultimately cannot be sure whether we did not just get unlucky and what we see is due to noise and not because of our hypothesis. The final step in the research process is the communication of our results. This step essentially combines all previous step. We need to communicate the research question, the operationalisation, the data collection process, and the results from the statistical analysis. Whereas the communication of the results is ultimately the goal of any research project, it is also the step during which we have to be mindful of the limits of our research. The biggest danger is that we forget the epistemic gaps that are inherent in any empirical research and oversell our results. We of course want that our research allows us to answer our (potentially big and broad) research questions, but we should be honest with ourselves and our audience and stick to the reality that we are primarily learning something about our operationalisation. When we get a statistical result that appears to support our empirical prediction we want to treat it as if it is true, but we should be clear that there always is a chance that our result might be a fluke. 1.4.1 Some Further Examples and Passing Thoughts Let us end this chapter with some more concrete examples from the literature that highlight some of the issues discussed here and tie it together with some thoughts on how to communicate research. To motivate ones research question, it is a good idea to start with the big picture. What are the real world issues and theoretical problems we want to address? Whereas this is a good idea, we should not mistake our operationalisation of the research question with this big picture. For example, in a recent paper on which I was a co-author (Baumann et al. 2020), we developed a new task, the ticket shopping task, to investigate sequential decision making problems (in this section, the details of the tasks are not really relevant so we will only mention the names, but do not describe tasks in detail). Whereas I feel we could make a good case that we could figure out to some degree what people do in this task, this does not mean that this is what people generally do in sequential decision making problems. Likewise, whereas there might be both a model-free and model-based reinforcement learning system in the brain, the popular two-step task (Daw et al. 2011) is unlikely to reveal all its mechanism or even prove its existence.11 In fact, there is good evidence participants in this task show many behaviours that are unrelated to the theoretical question of interest (e.g., Akam, Costa, and Dayan 2015). Another example comes from the domain of moral reasoning. Here, a prominent procedure is the use of the trolley problem and similar vignettes (Greene et al. 2001). Whereas these vignettes have revealed interesting results, it seems questionable to assume that participants responses represent the extent of their moral reasoning or that they are particularly predictive of participants real-world behaviour.12 Similar arguments can be made for many other experimental domains but also whenever particular questionnaires are used. It makes sense to look a bit critical at the argument we have made here. Surely, if we use a task such as the ticket shopping task or the two-step task we learn something about the underlying research question and theory? We surely do, the question that is difficult to answer is exactly what we learn. As a further example to illustrate this problem, let us consider the research on risk preferences in decision making. The idea of risk preferences is that some people might be more willing to take risks (e.g., when gambling or when choosing an investment) than others. There exist a number of different tasks to investigate risk preferences experimentally, such as the balloon analogue risk task [BART; Lejuez et al. (2002)] or the Columbia card task (Figner et al. 2009), as well as a number of different questionnaires. A large study with around 1500 participants who each performed eight different tasks and filled out twelve different questionnaires designed to measure risk preferences (Pedroni et al. 2017; Frey et al. 2017) could show that participants behaviour across tasks and questionnaires was surprisingly unrelated. Whereas participants who scored high on one questionnaire also scored high on other questionnaires (i.e., the different questionnaires shared a common risk trait), the scores on the questionnaires were largely unrelated to the behaviours in the different tasks. Furthermore, behaviours across the different risk tasks were unrelated to each other (i.e., a participant who was specifically risky in one task was not particularly risky in another task). In other words, even though the tasks and questionnaires all appear to measure risk preferences their failure to find a consistent pattern across participants suggests they fail to do so in a coherent manner. One might wonder if the fact that the questionnaires were related among each other represents some sort of silver lining here. I would not share this interpretation and instead attribute this to common-method variance. The important result is that the questionnaires were also unrelated to the behaviour in the tasks. This tells me that at this point we do not really understand what risk preferences are, how to measure them, or if they exist at all in the way they are conceptualised. To sum this up, it is important to keep in mind that the thing we learn something about in our research is primarily our operationalisation. If we want to make a case that we also learn something about the underlying research question, we have to make a good case for this and spell out which alternative explanations we rule out and which auxiliary assumptions we can take for granted. This usually requires considering other results than our own study. In short, do not confuse the task or measures with the theory or research question. When communicating the statistical results we also need to avoid overselling the results. As a general principle, we should report the results in a humble manner. To this end, we should avoid language that suggests a level of confidence that we cannot provide. This means, statistical results never prove or confirm our empirical hypothesis. Instead, they may support it or suggest certain interpretations. 1.5 Conclusion As scientists, we aim to answer interesting and important questions about the domain we are studying. The problem is that research cannot address the research questions directly. Instead, our research only addresses the operationalisation of the research question. Drawing inferences about the research question requires accepting a number of usually untested auxiliary assumptions. And even if the data appears to support our hypothesis and we are willing to accept the auxiliary assumptions, there always is the risk that we just got unlucky and interpret noise as if it were a signal. As a consequence, any individual study can only provide little evidence, especially for large and general research questions. Even worse, sometimes we only learn from our research that the chosen operationalisation is unable to answer our research question. In sum, definitive answers to our research questions need more than one study. Whereas this paints a less optimistic picture on what we can learn from our research than one would hope, it is important to stay realistic and humble. Many ideas, especially if they are our own, appear intuitive and compelling and we feel they must be right. But as scientists we need to stay sceptic and avoid the urge to believe in theories before we have overwhelming evidence that conclusively rules out possible alternative explanations (even those we havent yet thought about). The one thing that distinguishes science from non-scientific belief systems is that science is in principle based on solid evidence. And the overall strength of the evidence provided by our research depends on the whole research process of which statistics is one part. References "],["chapter-1-quiz.html", "Chapter 1: Quiz", " Chapter 1: Quiz Note: The pull-down menu for selecting an answer turns green after selecting the correct answer. Exercise 1.1 The start point for a statistical analysis should be? A data set with many variables that allows testing many different hypotheses A clearly specified research question The believe that my hypothesis is probably true Answer: 1 2 3 Exercise 1.2 Which of the following is NOT one of the steps in the research process? Research Question Operationalisation and data collection Statistical analysis Responding to emails Communication of the results Answer: 1 2 3 4 5 Exercise 1.3 What is NOT part of the operationalisation? Hypothetico-deductive method Specifying which tasks or questionnaires will be used Transformation of a research question into empirical and statistical hypothesis Deciding for which variables data will be collected Answer: 1 2 3 4 Exercise 1.4 What is the main reason why we need statistics? To prove/confirm a theory To test whether the data we collected provide evidence for the empirical hypothesis To publish papers in high-impact journals Answer: 1 2 3 Exercise 1.5 What is an epistemic gap? The difference between what we want to know and what we can know A specific outcome predicted by a theory A mathematically formalized cognitive theory Answer: 1 2 3 Exercise 1.6 Underdetermination of theory by data suggests that: Every specific hypothesis or theory is false or will eventually shown to be false We need hypotheses to answer research questions No single data sets unambiguously supports a specific hypothesis or theory Answer: 1 2 3 Exercise 1.7 When can a researcher assume that the data support the empirical prediction? When the data descriptively supports the empirical prediction When there are no epistemic gaps When the estimated signal is large given the estimated level of noise When the estimated level of noise is smaller than a pre-specified threshold Answer: 1 2 3 4 Exercise 1.8 What can we conclude if a statistical analysis provides strong support for our empirical hypothesis? Our research question is true or probably true Our empirical hypothesis is true or probably true It is unlikely that an alternative explanation could explain the results None of the above Answer: 1 2 3 4 "],["research-designs.html", "Chapter 2 Data and Research Designs 2.1 Empirical Evidence and Data 2.2 Data Types 2.3 Measurement 2.4 Independent and Dependent Variables 2.5 Experimental versus Observational Variables 2.6 Summary", " Chapter 2 Data and Research Designs In the previous chapter (Chapter 1) we have provided an overview of the research process in psychology and related disciplines. The goal of this overview was to show that statistics is just one part of the full research endeavour and usually not the end goal. We also highlighted that an answer to our research questions requires not only the results from the statistical analysis, but the context in which this result was generated. More specifically, we argued that the most important part of the research process is usually the operationalisation of the research question: What are our measures? What is the task participants have to do? What is our study design? The goal of this chapter is to provide us with the necessary conceptual knowledge and the right terminology to answer these questions for our research. 2.1 Empirical Evidence and Data In this book we are concerned with empirical research. What this means is that we are generally not interested in research questions for which an answer or proof can be found purely through thinking hard, such as in mathematics or philosophy. Instead, we are only interested in research question for which the evidence comes in the form of observations or experiences, empirical evidence for short. As we have discussed in the previous chapter, that does not mean that our theories cannot include unobservable entities such as mental states (e.g., fear, enjoyment, attention). However, if our theories include such unobservable entities, these must be causally responsible for something that is observable (e.g., behaviour). This way, we can still test the theories (e.g., if our theory predicts that fear should lead to aggression, but we can induce fear without it leading to aggression, we learn that our theory must be wrong). The fact that we are interested in empirical research means that the ultimate arbiter of whether or not we should believe in a theory is empirical evidence. It does not matter how elegant or intuitive a theory is. If the observed behaviour of people disagrees with a theory, it is wrong. It also means that theories that are so vague that there is no possible empirical evidence that would disprove them are not part of the empirical sciences (i.e., they are not empirical theories). This criterion is also known as falsifiability and was introduced by the philosopher Karl Popper in the 1930s. For example, there is a never ending discussion of whether Freudian psychoanalysis is in principle falsifiable or not. Whereas Karl Popper was very strong in his belief that it is not (which would render psychoanalysis non-scientific), proponents of Freudian psychoanalysis naturally see this rather differently.13 Empirical evidence comes in at least two different forms, either as anecdotes or as data. Whereas an anecdote typically refers to a single person, data usually contains information about multiple persons. However, anecdotes and data differ on more dimensions than just the number of observations, as summarised in the following aphorism: The plural of anecdote is not data. Anecdotes are unsystematic observation, typically in the form of stories (e.g., the friend of a friend), that somehow address our research questions. The problem with anecdotes is that they are generally difficult to verify and to investigate further. This makes it impossible to rule out possible alternative explanations for the relationship between the anecdote and the research question. And as we have seen in the previous chapter, one of the main criteria for deciding whether an observation provides evidence for a theoretical claim is whether we can rule out plausible alternative explanations. In sum, anecdotes surely matter when coming up with good hypotheses or ideas what to study, but for mature sciences anecdotes should only play a minor evidentiary role in deciding which claims to believe. Data are systematic observations that are collected for a specific purpose, such as answering a research question or bookkeeping. Data generally consists of observations on multiple variables. Each variable corresponds to a specific set of possible outcomes or states of affair, where each possible outcome corresponds to one value of the variable. An observation is the smallest unit of data corresponding to one set of values on at least one of the variables. As an example of data, consider again the study by Walasek and Stewart (2015) discussed in the previous chapter (1). The task of participants was to accept or reject 50-50 lotteries (for an example, see Figure 1.2) and each participant had to do this for 64 trials. Table 2.1 below shows six observations each from two different participants from this study. Observations are shown in rows and variables are shown as columns. This tabular representation of the data with observations in rows and variables in columns is common and will be used throughout the book. The way the data is shown here is exactly the format in which the data was kindly provided by Lukasz Walasek (i.e., no variables added or removed). So presumably, this is exactly the format that he used to analyse the data (of course, this table omits several thousand observations, but the format of the data is the same for the not shown observations). Table 2.1: Selected observations of the data from Walasek &amp; Stewart (2015, Exp. 1a). The [] indicates that this is only part of the whole data set and some observations are not shown. subno loss gain response condition resp 8 6 6 accept 20.2 1 8 6 8 accept 20.2 1 8 6 10 accept 20.2 1 8 6 12 accept 20.2 1 8 6 14 accept 20.2 1 8 6 16 accept 20.2 1 [] 369 6 12 reject 40.2 0 369 6 16 reject 40.2 0 369 6 20 reject 40.2 0 369 6 24 accept 40.2 1 369 6 28 accept 40.2 1 369 6 32 accept 40.2 1 In total we can see six different variables in this data set. Let us discuss these in turn. The first variable, subno (we generally use a monospace font to refer to variable names as they appear in a data set), is the participant identifier or subject number (because actual individuals take part in research and not passive subjects, the term participant is now preferred to subject). This variable should be part of any data set to uniquely identify to which participant (or more generally unit of observation) a specific observation belongs. Here, we see that it only takes numbers. It is not uncommon to only use numbers for the participant identifier variable, but it can also be a combination of numbers and letters, or a (ideally anonymous) name. The second and third variables, loss and gain, specify the possible outcomes of the lotteries for each trial. For example, the second observation/row shows a lottery in which the potential loss was $6 and the potential gain was $8. Based on these two columns, we can see that the observations are ordered by the combination of loss and gain. This means the order of observations in the data does not reflect the actual order of trials in which participants saw them (as this order was random). All values of the two variables are numbers with the lowest possible loss/gain being 6 and the largest possible loss/gain being 40, depending on the condition a participant is in. Column four shows the response of the participant to the lottery, either accept or reject. The sixth variable, resp, is a numeric version of the response. Here, an accept decision is represented by a 1 and a reject decision by a 0. So these two variables carry the same information, but in different formats that have different benefits. The response variable makes it easy to understand what participants response was (i.e., it is clear which value corresponds to which possible response, accept or reject, in the study). The resp variable makes it easy to perform calculation on the results given it uses a numerical code to represent the same information. For example, because reject is mapped onto 0 and accept is mapped onto 1, we could take the mean over all observations to get the overall acceptance rate across all gambles (the mean of the whole data is 0.38 which corresponds to an acceptance rate of 38%).14 However, if we only had the resp variable, we would additionally need the information what actual response 0 and 1 correspond to. Finally, variable five informs us in which condition each participant is in. Remember from the previous chapter that the experiment varied the range of gains and losses across participants resulting in four conditions in total: a condition with loss and gains ranging up to -$20/+$40, a -$20/+$20 condition, a -$40/+$40 condition, and a -$40/+$20. This information is here provided in form of a decimal number with the values before the decimal point referring to the range of gains and the values after the decimal to the range of losses without the trailing 0 (i.e., in the opposite order to how we have referred to the conditions so far). Thus, participant with subno 8 is in the -$20/+$20 condition and participant 369 in the -$20/+$40 condition (if we were showing a participant in the -$40 loss condition, there lowest possible loss outcome would be 12 and not 6 as for the two participants shown here). The example from Walasek and Stewart (2015) hopefully clarifies the abstract definition of a variable that was provided above. For each variable, we have a set of possible outcomes that is defined by our research design. For example, for the participant identifier subno, this set encompasses all possible participants that can take part in the study. For loss and gain, this set contains all potential losses and potential gains that occur in the lotteries. For response there are the two possible outcomes, to accept or to reject a lottery. We then define values for each possible outcome. In the case of subno we assign a different number to every participant from which we collect data. For loss and gain, the values correspond to the magnitude of the potential loss and gain in US dollars (the currency that was used in the experiment). For response, we do not use a numeric code but use a word to represent the two outcomes. There are a few things of note in the example data shown in Table 2.1. Every observation in this data is complete; for each observation we have values on each variable and no missing data. Whereas this is common in experimental research, it is not always the case for other types of research. Whereas missing data is not something we will discuss in detail in this book, it is important to be aware that it can happen and to think about what to do in this case (sadly, there is no general solution). We have multiple observations per participant, 64 to be precise. These 64 observations are given in different rows. We call this data format  in which the data from each participant potentially spans multiple rows, one row per observation (i.e., 64 rows per participant in the present case)  the long format. This long format contrasts with a wide format that is also commonly found in the social sciences. In the wide format, the data of one participant only spans a single row. In case a participant has multiple observations, these are given in different columns. For the procedures introduced in this book, we generally want the data to be in the long format. (If each participant provides only one observation, there is no difference between the long and the wide format.) The variables differ in whether they contain numbers (all variables but response) or no numbers (only response). 2.2 Data Types Let us discuss the last point from above in more detail. A common intuition is to think about numbers when thinking about data. As we have seen in the example data, this is not necessary. Data does not have to be numbers. The response variable shows that we can use other values, such as words or phrases, to represent values of variables. However, the conception of data primarily as numbers is also not completely false. For example, the statistical analyses introduced in this book need to represent all variables in terms of numbers. Fortunately for us, the tools we will be using will generally convert data not using numbers into numeric data when necessary. This means we will use the type of data representation that makes it easiest to understand what the data stands for. 2.2.1 Numerical Versus Categorical Variables An important issue that arises when thinking about data as numbers is that numbers can mean different things. One possibility is that numbers represent numerical information; that is, they represent a measurement, magnitude, or count of something. However, we can also use number in a broader sense in which they only serve as a label (e.g., numbers on football jerseys or telephone numbers). In this case, numbers only represent categorical information; each observation falls into one of a set of mutually exclusive categories. The meaning of the numbers has important consequences of how we use them. If numbers do not represent numerical information most mathematical operations do not make sense. For example, it does not make much sense to calculate the average of two telephone numbers. Let us exemplify this with some variables from the example data. Let us begin with the loss/gain variable pair (we can consider them together, because the type of information is the same, the only difference is whether the number refers to a potential loss or a potential gain). For these variables, the meaning of numbers corresponds to the common understanding of numbers as a magnitude of something. In particular, the magnitude of a potential loss and potential gain. We could understand these variables as measuring the magnitude of the potential loss and potential gain of a lottery. The larger the number the larger potential loss and gain. In fact, the number exactly represents the potential loss and potential gain (i.e., the measurement of the potential loss and gain is perfectly accurate). Because the numbers of the variables represent numeric information we can treat the variable as a numeric variable in a statistical analysis. More specifically, because performing mathematical operations, such as addition or calculating the average of the numbers, is meaningful for this variable we can treat it as a numerical variable. For example, we could calculate the average potential loss/gain for a participant and it would be useful information (i.e., we could interpret the average in a meaningful way, for example by comparing with the average loss/gain in a different condition). As a second example, let us consider the subno variable. Here, the numbers do not really measure the magnitude of something. Participant number 16 is not twice participant 8. From just looking at the variable, we also do not know what it means. As described above, one just needs to assign numbers somehow to each participant. For example, one could assign number 1 to the first participant that participates in the experiment, number 2 to the second one that participates, and so forth. Alternatively, one could also assign number 1 to the first participant invited, number 2 to the second one invited, and so forth. Another possibility is to specify the maximum number of participants one can collect, say 500 participants, and then just assign a unique random number from 1 to 500 to every participant that participates (e.g., by drawing them from a pool of all numbers without replacement). Importantly, we do not need to know which of these procedures was used. The only reason we have the subno variables is so we know to which participant a particular observation belongs to. The numbers in subno only serve the purpose as a label identifying the participant. Instead of numbers, we could also use non-numeric labels, such as random strings of letters, as the participant variable. Consequently, it does not make much sense to perform any mathematical operation on the subno variable. For example, the average participant number does not provide any useful information. For the purposes of this book, this distinction between these two data types is central: Can we treat a variable as a numerical variable or a categorical variable? The statistical methods introduced in the following chapters can only deal with these two types of variables (and categorical variables can generally also only serve the role of an explanatory variable and not as an outcome variable). So how can we identify whether a variable is numerical or categorical? Usually it is easy to identify categorical variables among the variables that have numbers. Whenever the numbers represent a label, a variable is usually a categorical variable. For example, in addition to the subno variable, the numbers in the condition variable only serve as a label to identify the condition. We cannot interpret the numbers of the condition variable shown in Table 2.1 as actually representing a numerical value of either 20.2 or 40.2. Instead, each of the four possible values of the variable, 20.2, 20.4, 40.2 and 40.4, refers to one of their four conditions of the experiment, with loss and gains ranging up to either -$20/+$20, -$40/+$20, -$20/+$40, and -$40/+$40 (note again that the value after the decimal point is the range of the potential loss and the value before the decimal point the range of the potential gains). And non-numeric variables in which the values are labels, such as for the response variable, are also clearly categorical variables. More difficult is the decision for the resp variable. Clearly, the two possible values of the response variable, the accept or reject decision, are response categories or labels. However, when transforming it into a variable with numbers 1 and 0, we can perform meaningful mathematical operations on it. As discussed above, the mean of the variable can be interpreted as the average accept proportion. More generally, any binary categorical variable (i.e., a categorical variable of two categories) can be seen as a special case where treating it as a numerical variable can in certain situations be meaningful. However, whether or not it is meaningful depends on the situation. In general it is best to explicitly treat a variable as categorical unless one is sure treating it as numerical is meaningful. To sum this up, for the statistical purposes of this book we distinguish numerical variables and categorical variables. Numerical variables hold numerical information such as magnitudes of something or the degree with which something holds. For categorical variables the values of the variable serve as labels designating membership in one of a number of mutually exclusive categories. When categorical variables are part of an experimental design, we will later also call them factors. 2.2.2 Assumptions of Numerical Variables What the case of the resp variable, the numerical representation of a binary categorical variable, shows is that the decision of whether something is a numerical or categorical variable can depend on the situation. To help with this decision, it is helpful to know what exactly is entailed by treating a variable as numerical. For the statistical methods used here, when we treat a variable as numerical we assume it represents continuous numerical information. What this means is that we assume that: A certain difference or interval has the same meaning anywhere on the scale. For example, a difference of 1 unit of the variable means the same whether we add it to 10 or 20. We can see that this holds for the loss/gain variable pair but we will later discuss examples where this is a questionable assumption. A corollary to this assumption is that calculating the mean for our variable must be meaningful in itself. If we cannot interpret the mean, a variable cannot be treated as numeric. Our variable can in principle take on any real-valued (i.e., decimal) number. That is, even though we might have only used discrete values for our variables, such as for the loss/gain variable pair only a subset of the whole numbers between 6 and 40 (see Table 1.1), our statistical method assumes the in-between values are possible and in principle meaningful. Let us consider a few example variables to see how well they fulfil the two assumptions for a numerical variable. For the loss/gain variable pair, the two assumptions are fully satisfied. However, the loss/gain variable pair is not actually an outcome that was measured in an experiment. Instead, this variable pair was part of the design of an experiment. So maybe it is not the most relevant variable for this question. The numerical outcome variable in this data set is resp. Clearly, resp does not fulfil the assumptions as it only has two discrete outcomes, the values 0 and 1. However, we can calculate and interpret the mean (as the average proportion accepted). We could also assume that a specific difference, say a 0.1 (or 10%) difference, means the same whether it happens at an acceptance rate of 50% or an acceptance rate of 85%.15 So whereas the assumptions are violated they are also partially fulfilled. So whether we can interpret the results from an analysis depends on the exact context and circumstances. For example, if our statistical analysis would lead to results or predictions beyond the probability range of 0 to 1, this would be clearly problematic as the results would not be meaningful. In other words, we would have learned very little meaningful about our data from such a statistical analysis. A very popular variable in psychology and related sciences is subjective rating scales (also known as Likert scales). For example, we have discussed the study of McGraw et al. (2010) where participants in one condition where asked to rate the intensity of their emotional reaction to a potential loss or potential gain on a response scale ranging from 1 = no Effect to 5 = Very Large Effect (see Figure 1.1, unipolar intensity scale). Does this variable represent a numerical variable? Clearly, a value of 5 represents an emotional reaction that is larger than a value of 1. What this means is that the variable does represent a magnitude, but does it fulfil the assumptions spelled out above? We can also take the average of the scale and interpret it in a meaningful way. Specifically, in the loss condition participants reported an on average stronger feeling with an average value of around 3.6 compared to the gain condition, where the average value was around 3.1. However, it is questionable whether a difference of 1 means the same everywhere across the scale. More specifically, is the difference between No Effect and Small Effect (i.e., the difference between 1 and 2) the same as the difference between Moderate Effect and Substantial Effect (i.e., the difference between 3 and 4)? Numerically it is, but whether this also holds psychologically is a question that is difficult to answer. Like most researchers McGraw et al. (2010) have treated this variable as a numerical variable so have made this assumption (which is also implicit in the process of calculating the average). The validity of their conclusions rests to some degree on whether or not we believe making this assumption makes sense. Let us generalise the conclusion of the previous paragraph and answer the question what it means that the two points above represent the assumptions for treating a variable as a numerical variable. Can we only treat a variable as a numeric variable in a statistical model if it perfectly meets the assumptions? In an ideal statistical world the answer would be yes, but the reality of data analysis always differs from the ideal. Many of the variables that we regularly encounter in our research (e.g., rating scales) violate the two assumptions to some degree and we still need to include them as numerical variables in our model (because treating them as categorical does not help us in answering our research questions). Whenever the assumptions are to some degree violated this can be interpreted as another instance of an epistemic gap (or as an instance of the first epistemic desk, Section 1.3.1). The fact that the assumptions are violated opens the possibility for an alternative explanation of the results that differs from our hypothesis. In other words, if the assumptions are perfectly met the evidence provided by our statistical analysis is stronger than when the assumptions are only partially met. The problem is that once we have numbers and treat them as a numerical variable, the computer treats all the numbers the same way (i.e., assuming they are a continuous numerical variable), the numbers dont remember where they came from (Lord 1953)16. Only we  the researchers  know where the numbers came from and need to take this into account when interpreting statistics. We can also interpret this insight in terms of the concepts introduced in the previous chapter. The numbers are part of the operationalisation; we establish a procedure that maps real world entities (above we have called these possible outcomes or states of affairs) onto values of the variables (which are in many cases numbers). The numbers that emerge from this procedure are related to our research question, but they are not identical with our research question. Any inference from the statistical results based on these numbers requires many auxiliary assumptions, one which is that we assume that numerical variables are continuous. And as we can never be sure if all the auxiliary assumptions are true, we have to be careful and humble with the conclusions we draw from our research. 2.3 Measurement So far we have categorized different variables as they appear in a data sets and how we can integrate them into a statistical analysis. Here, we take a step back and consider in a principled manner how the variables are created. The question we are considering is what does the measurement process by which we assign values to events allow us to infer from the variable. 2.3.1 Measurement Scales The discussion above is about variables consisting of numbers and the meaning of the numbers. One way to interpret this discussion is in terms of the first epistemic gap introduced in the previous chapter, the difference between our research question and the operationalisation of the research question (Section 1.3.1). If we apply this distinction to the issue of this section, the meaning of variables, we can understand this as the distinction between the magnitude of a latent construct, which is called an attribute in this context, a variable is supposed to represent (e.g., strength of an emotional intensity or personal risk preference) and the measurement of that attribute through the operationalisation (i.e., the application of a procedure that assigns a number for that attribute to an observation). An important theoretical contribution to this distinction within the context of psychology comes from Stevens (1946). He assumed that we can distinguish four different types of measurement operationalisation, which he called measurement scales, with respect to which type of relationship between attributes they reveal. These four different measurement scales are, nominal, ordinal, interval, and ratio scale. If a scale is constructed by assigning labels to different attribute values, it is called a nominal scale and can be understood as equivalent to what we have termed a categorical variable. For a nominal scale, the values of the attributes do not exhibit any quantitative relationship among each other. In addition to the examples discussed above, many demographic variables can be understood to be on a nominal scale such as gender (e.g., male, female, non-binary, or other) or handedness (right-handed, left-handed, or ambidextrous). If a scale is constructed through rank orderings of attributes it is called an ordinal scale. As a consequence, we can order attributes along a dimension but cannot make any further quantitative distinctions. A common example of an ordinal scale is the final result of a sports competition with first place, second place, and so on. The important aspect of an ordinal scale is that differences between values on the ordinal scale do not need to correspond to the same difference in the attributes that is measured with the ordinal scale. If we stay within the sports competition example, the difference between the first and the second place in terms of performance does not need to be the same as the difference between the second and the third place. For example, in the 2020 Olympics 100 m women sprints final the difference between the first place (Elaine Thompson-Herah) and the second place (Shelly-Ann Fraser-Pryce) was 0.13 seconds, whereas the difference between the second and third place (Shericka Jackson) was only 0.02 seconds. In this case the same difference on the ordinal rank scale (i.e., one rank difference), does not correspond to the same difference in the underlying attribute (i.e., performance, time needed for sprinting 100 m). We can also understand some demographic characteristics can be understood to be on an ordinal scale, such as education levels (e.g., some primary or secondary school education, compulsory education up to age 16, college, or higher education or professional &amp; vocational equivalents). An interval scale results from an operationalisation that also maintains that differences, or intervals, between the values of the attributes on the scale need to have the same meaning across the scale. Thus, an interval scale can also be understood as fulfilling the requirements of a numerical variable. The typical example of an interval scale is temperature measured in either degrees Celsius (°C) or Fahrenheit (°F). Within each temperature scale, a 1 degree difference has the same meaning independent of the current temperature. Furthermore, both scales can be transformed into each other. For interval scales it also makes sense to calculate the mean (e.g., the mean temperature), but calculating ratios does not make sense. For example, saying the 40 °C is double the temperature of 20 °C is not a really meaningful statement (e.g., because 20 °C = 68 °F and 40 °C = 104 °F and \\(2 \\times 68 \\neq 104\\)). The final scale type, ratio scale, results from an operationalisation that in addition to maintaining the meaning of differences across the scale also contains a true zero point of the attribute. To stay within the typical example of temperature scale, whereas the zero point of degrees Celsius and Fahrenheit is arbitrary and they represent only interval scales, the zero point of the Kelvin scale, 0 K, is the lowest possible temperature making the Kelvin scale a ratio scale. Many physical scales are on a ratio scale such as length or time. For example, it makes sense to say that a sprinter who took 20 seconds for the 100 m sprint took twice the time as a sprinter who only took 10 seconds because 0 seconds is the true zero point of no time. Whereas Stevens four measurement scale widely popular in psychology and related disciplines thanks to their prominence in most introductions and textbooks, their actual scientific contribution needs to be considered critically (following Michell 1997, 2002, 1999). What Stevens proposes attempts to bridge the epistemic gap between the attribute and its measurement. According to this position, if we have established an interval or ratio scale, we have learned that the underlying attribute exhibits an interval or ratio structure. Unfortunately, because of the problem of underdetermination discussed before, learning about the actual structure of a theoretical construct or attribute is not that easy. Only because the numbers look like a numerical variable, this does not mean that the underlying attribute behaves like a numerical variable. Discussing the problem in detail is beyond the scope of the present chapter, but the crux of the matter is the issue we have already highlighted in the previous chapter. The operationalisation used to measure a certain latent construct does not represent the latent construct. More technically, the definition of what constitutes measurement that allows to infer the structure of an attribute from its operationalisation differs from the definition given by Stevens (i.e., the application of a procedure that assigns a number of an attribute to an observation). Instead, a proper quantitative measurement that is analogous to the measurement of physical quantities (e.g., length) needs to fulfils a number of assumption, such as expressed in the theory of conjoint measurement, that very few psychological or behavioural measures have been shown to fulfil. Put more bluntly, Stevens attempt to bridge the epistemic gap is logically incorrect, to establish what he wants to establish we need stronger theories that define the theoretical constructs more rigorously (Michell 1999). Even though Stevens idea that we can measure latent constructs through interval or even ratio scales is incorrect, his distinction is nevertheless helpful as it again allows us to understand the limits of what we can learn from data. What we can take away from the present discussion is that the common measurement approach that is taken in psychology and related fields is generally only able to establish ordinal relationships. For example, subjective ratings scales, but also choices among lotteries, only represent ordinal relationships in terms of the underlying latent attributes. Nevertheless, we generally treat data from such variables as numerical variables in statistical analyses. This again reinforces the point made before that we cannot interpret the results from statistical analyses as directly answering our research questions. Our statistical analysis generally makes assumptions about the nature of the underlying attribute or construct that we cannot verify. However, this does not mean that all hope is lost. As Stevens (1946) already mentioned when introducing ordinal scales, treating them as numeric is not always pointless (p. 679): In numerous instances it leads to fruitful results. We just have to be mindful that measurement in psychology is generally not the same as measurement in physics when interpreting the results. We primarily learn something about our operationalisations and not directly about our research questions. 2.3.2 Reliability and Validity The main message of the previous section is that measurement of mind and behaviour is not as straightforward as measuring physical attributes such as length. Nevertheless, we should aim to use measures (i.e., operationalisations that provide measurements) that are of high quality. Two concepts that are important for judging the quality of measurement are reliability and validity, which we will introduce now. Reliability refers to the consistency of a measure. One intuitive way to understand reliability is as the consistency of a measure across different measurement occasions under consistent conditions. Reliability is also inversely related to noise in the measurement process. A measure has a high reliability if repeated applications to the same conditions lead to very similar outcomes (i.e., the level of noise in the measurement process is low). A measure has low reliability if repeated applications to the same conditions lead to widely different outcomes (i.e., the level of noise in the measurement process is high). For example, consider a regular bathroom scale. We expect such a scale to have a very high reliability; we should get pretty much the exact same results if we step on it several times in a row, as long as we do not change our weight in between (e.g., by drinking something). Validity refers to the ability of a measure to measure what it is supposed to measure. Within the concepts introduced within this book, validity thus refers to the ability of a measure to bridge the epistemic gap between the operationalisation of a construct and the actual (i.e., true) value of the construct. Thus, validity can be seen as one way to conceptualise the question of how strongly a measurement operationalisation corresponds to the measurement of a physical quantity. Given the difficulties in defining or even establishing the constructs researchers are interested in, establishing whether or not a measure has a high or low validity is generally difficult. One way to visualise both reliability and validity is given in Figure 2.1 below. Here, each panel represents one operationalisation or measure and each shot on the target represents one measurement with that measure. We can see that reliable measures have low levels of noise (i.e., low level of dispersion of the shots) around one mean value. In this figure, validity is visualised as a bias with respect to the centre of the target. Valid measures are centred on the target whereas invalid measures are centred around an off-target value. What this figure highlights is that reliability and validity are in principle distinct qualities. Only because a measure is reliable, it does not mean it is valid. And likewise, a valid measure does not have to be reliable. Figure 2.1: Visualisation of reliability and validity as shots on a target. Figure is taken from Statistical Thinking for the 21st Century by Russell A. Poldrack (Figure 2.1). Whereas the visualisation in Figure 2.1 should provide a good first intuition about reliability and validity, the conceptualisation of validity as a bias is not the only way to think about it. To stay within the metaphor provided by the figure, an invalid measurement could also be one that aims for the floor instead of the target. Or even something completely missing the task such as shooting darts when the goal is to shoot bullets. The problem with validity is that sometimes we dont know how to better define or measure what we are measuring so determining validity is not always easy. The previous paragraph already points to an important distinction between reliability and validity. Usually there is a way to quantify reliability, but quantifying validity is only possible for specific interpretations of validity. The most common ways to quantify the reliability of a measure are: Split-half reliability or internal consistency refer to the reliability estimate that results from splitting a measure into sub-measures and comparing the scores across sub-measures (e.g., by calculating a score for all odd items and comparing them with the score of all even items). Test-retest reliability refers to the consistency of applying the same measure at different time points. When using a measure such as a questionnaire, the difficulty in calculating the test-retest reliability is the possibility of memory consistency effects or temporal instability. Memory consistency effects refer to the observation that participants often prefer to be self consistent with their previous answers if they remember them thus potentially artificially increasing the reliability. Temporal instability on the other hand can result in artificially lower reliabilities if the measured construct is temporally unstable (such as mood). Inter-rater reliability refers to the agreement of different raters for the same event or situation. Inter-rater reliability can usually only be calculated for measures that are not self assessments or subjective scores. Prominent examples of situations in which one can calculate inter-rater reliabilities are medical diagnoses (i.e., by comparing the diagnosis across multiple doctors) or essay marks (when marked by multiple independent markers). To quantify validity we need to have an external criterion that also measures the construct of interest. When we have such a criterion, we can compare the value on the criterion with the value on our measure which gives us an estimate of the criterion validity of the measure. For example, in the previous chapter we had discussed that there exist different tasks or questionnaires for measuring an individuals risk preferences. Consider we also had access to a persons financial history showing to what degree they invest their money into relatively high risk (e.g., stock options), medium risk (e.g., individual stocks), or lower risk (e.g., index funds) financial instruments. A risk preference measure with high criterion validity would be one for which participants that have a high score on the measure invest more of their actual money in high risk investments. Other types of validity are non quantifiable. Construct validity is usually considered the most important type of validity as it refers to all empirical and theoretical support that provides evidence that a measure measures what it is supposed to measure. Face validity refers to the degree with which a test appears to measure what it is supposed to measure. Considerations of both reliability and validity should be incorporated into the overall assessment of the results when judging the empirical evidence provided by a given study. 2.4 Independent and Dependent Variables In addition to distinguishing the type of information variables can contain, we can also distinguish the different roles variables can play in the research process. Remember, when discussing the operationalisation step of the research process we specified that we need to identify relevant variables that we hope can address the research question as well as specify an empirical hypothesis involving at least two variables. As we will see here, we can assign two different roles to these at least two different variables. Usually exactly one variable is the variable for which we are interested in the results, in psychology we call this variable the dependent variable. Synonyms for dependent variable that are common in the statistical literature are response variable, outcome variable, or criterion and we can see that their meaning points in the same direction as dependent variable. The dependent variable is the main outcome of our study, the variable which we are primarily interested in measuring. The other variable(s) are called independent variable(s) in psychology, because we believe the values of the dependent variable depend on the values of the independent variable(s). A popular synonym for independent variables in the statistical literature is covariates, as the dependent variable is assumed to covary with the independent variables.17 Above we have also used the term explanatory variable to describe independent variables. Loosely speaking we can describe the distinction such that a study is about the effect of the independent variable on the dependent variable.18 Let us show this distinction in the loss aversion study of Walasek and Stewart (2015) the data of which is shown in Table 2.1 above. Their research question was that what matters for peoples preference for symmetric 50-50 lotteries is not the absolute value of the potential loss and gain but the relative rank of the lotteries compared to other lotteries. The operationalisation of this research question involved the manipulation of the range of lotteries in one variable, condition, and measuring participants responses to symmetric lotteries in the response\\resp variable pair. Here the distinction between both variable types is relatively straight forward. We are interested in the effect of condition on response, which makes condition the independent variable and response variable the dependent variable. In general, the distinction between independent and dependent variable is easy to understand in an experiment, such as the study by Walasek and Stewart (2015). In an experiment, independent variables are manipulated, such as the case for condition. Manipulated means we assign participants to the different conditions (compared to measuring in which condition a participant is). Not all studies are or can be experiments in which the independent variable is manipulated. For example, a common research question is the effect of a demographic variable on an outcome. However, demographic variables cannot really be manipulated or assigned to participants. For example, we might be interested in studying the effect of parental wealth on childrens educational attainment. Whereas manipulating parental wealth is in principle possible, a more common approach is to measure this variable as well as childrens educational attainment. Nevertheless, we can still make the distinction between the independent variable, parental wealth, and dependent variable, educational attainment. Not all variables in an experiment neatly fall within the distinction of independent and dependent variables. For example, let us go back to the six variables shown in Table 2.1 that make up the full data collected in the study by Walasek and Stewart (2015). As discussed above, the response\\resp variable pair is the dependent variable and condition the independent variable. This leaves us with three further variables that need to be classified. Let us begin with the loss/gain variable pair that determines the possible outcomes of a lottery shown to participants. Clearly these are also manipulated. More specifically, the condition determines exactly which lotteries and therefore which values of the loss/gain variable pair a participant works on. Thus, the loss/gain variable pair are also independent variables that jointly determine the independent variable condition (i.e., if we did not have the condition variable in this data set we could determine it from the loss/gain variable pair). This means that determining the independent variables in this study depends on the perspective one takes. If we only focus on the main research question and symmetric lotteries then condition is the independent variable. However, if we also look at the lotteries individually, then condition and the loss/gain variable pair are the independent variables. Table 2.1 contains one more variable, subno, the participant identifier variable. It might seem a bit surprising to think about this variable in terms of independent and dependent variable, but we should be able to classify this variable somehow. Clearly, subno is not a dependent variable. We are not interested in the values or results of the subno variable. However, it also seems not clearly an independent variable. We do not have any specific expectations or ideas of how different subjects affect the response variable. To help us with the classification let us consider again why we have the subno variable in the data in the first place; because we collect data from multiple participants and need to identify to which participant an observation belongs. The follow-up question to this is why do we collect data from multiple participants? As discussed in the previous chapter (Section 1.3.2), participants are a source of noise in our experiment as different participants can do what they do for a multitude of reasons. If we only had the data from one participant, we could not distinguish between the idiosyncratic noise of that participant and the signal we are interested in. By collecting data from multiple participants, we try to control for the noise by averaging over it with the hope that what remains is the signal. The overarching idea is that noise has an unsystematic effect on the results; some participants may be more likely to show a particular behaviour whereas other participants may be less likely to show that behaviour, but on average the noise cancels out. In sum, we collect data from multiple participants to control for noise that is inevitable when dealing with real people. Thus, we could say subno is a control variable. We could even be more specific and say it is a control variable and an independent variable, because we use it to control noise at the level of the design. In other situations we might measure a variable for control purposes in which case we could call it a control variable and a dependent variable. To sum this section up: Jointly, the dependent and independent variables are the key components of the operationalisation of a research question. They are the central concepts that make up the study design and link the practical reality of the research (i.e., how the research actually takes place and what is measured) with the research question. It is not wrong to say that a study is defined primarily by its dependent and independent variables. When designing ones own study, making it clear what the dependent and independent variables are is maybe the most important decision after having decided on a research question. Likewise, when reading a scientific article describing a study, understanding clearly what the independent and dependent variables are is central to understanding the study. Therefore, whenever thinking and talking about any research, make sure to be clear what the dependent and independent variables are. In experimental research this often boils down to asking: What was the task of the participants? Other variables that are part of a study usually serve some control purpose and can be denoted control variables. 2.5 Experimental versus Observational Variables As we have seen when discussing the distinction between independent and dependent variables, we can distinguish different types of independent variables or research designs, namely experimental and non-experimental independent variables. Here we will adopt the common terminology and use observational variable to describe non-experimental independent variables. If a study solely consists of experimental variables (we drop the independent part from now on because experimental or observational variables are always independent variables), we can call it an experimental study or experiment for short. If a study solely consists of observational variables, we can call it an observational study. If a study contains both experimental and observational variables, there is no agreed upon name and depending on which variable is more relevant to the research question, researchers tend to use either experimental or observational study. However, as experimental variables provide a number of evidential benefits that will be discussed here, there is a tendency to call a study an experiment even if it also contains observational variables. Depending on the actual situation and the inferences drawn this can be seen as a stretch. An experimental variable is one that is in control of and can be manipulated by the researcher. What this means is that the values of the variables can be assigned to participants by the researcher. For example, in the study of Walasek and Stewart (2015) the researchers assigned each participant to be in one of the four conditions corresponding to a different range of potential losses and gains. Likewise, in the previous chapter we briefly introduced a study of Hinze and Wiley (2011) on the generality of the testing effect. After an initial reading of a piece of text, participants were assigned to one of three experimental conditions: a control condition in which they could re-read the materials, a testing condition using open-ended questions, and a testing condition using a fill-in-the blank text. In both of these cases, the researchers decided which condition a participant was part of. The important part of an experiment variable is not only that participants can be assigned to different conditions, but how they are assigned. More specifically, for an experimental variable the assignment needs to be performed randomly; we say participants are randomised to the available conditions. One way to understand random assignment is that before the experiment takes place, the probability to be in any of the experimental conditions needs to be the same for every participant.19 For example, random assignment means that for every participant in the study of Walasek and Stewart (2015), the probability to be in any of the four conditions is 0.25 (i.e., 1/4). For every participant in the study of Hinze and Wiley (2011), the probability to be in any of the three conditions is approximately 0.33 (i.e., 1/3). You can imagine randomisation as an actual physical process that produces a random outcome, such as the toss of a coin or throw of a dice. For example, for the study of Hinze and Wiley (2011) we could imagine that for every participant that takes part in the experiment, the researcher (or research assistant) throws a regular six-sided dice. If the dice lands on 1 or 2 the participant is assigned to the re-reading condition, if the dice lands on 3 or 4 the participant is assigned to the open-ended question condition, and if the dice lands on 5 or 6 the participant is assigned to the fill-in-the-blank condition. Alternatively, if we pre-specify the sample size and want to ensure that every group is of approximately the same size, we could use a different approach. We could prepare as many sheets of paper as the number of participants we want to collect. On each sheet we write one condition so that among all sheets, each condition appears equally often. Then, we shuffle all sheets in a bowl to randomise their order. When performing the experiment, we take one sheet out of the bowl (without putting it back) for every participant and assign the participant to the condition written on the sheet. Nowadays randomisation is mostly done through a computer using so-called random number generators. An observational variable is a variable that is not in control of the researcher and we cannot randomly assign participants to condition. In other words, whenever randomisation is impossible an independent variable is an observational variable. Above we talked about demographic characteristics, in particular parental wealth, as an independent variable. As already described there, demographic variables generally cannot be randomly assigned but are a mostly immutable part of a person. Consequently, demographic characteristics (e.g., age, gender) are generally observational variables. The same is true for many other psychological characteristics of a person such as personality traits (e.g., extraversion) or abilities (e.g., intelligence quotient). In the vast majority of cases such variables are observational variables. At this point you might wonder why an experiment necessarily entails randomisation of the independent variable. What is the benefit of an experimental over an observational variable? The reason for this is that only randomisation allows drawing causal inferences from a study. Only with an experimental  that is randomised  independent variable can we say that the independent variable is the cause of the dependent variable. Without randomisation (i.e., when dealing with an observational independent variable) such an inference is not permitted. Remember that above we said one way to think about the distinction between dependent and independent variables is that in a study we want to learn about the effect of the independent variable on the dependent variable. We did not specify what we mean with effect, but we said this way to think about dependent and independent variable only holds loosely. With effect we meant a causal relationship, the independent variable is the cause of the dependent variable. In the absence of such a cause-effect relationship, it seems wrong to speak of an effect of the independent variable. And we can now see the reason why we said this only holds loosely, it only holds if the independent variable is an experimental variable, but not if it is an observational variable. 2.5.1 Epistemic Gap 3: Causal Inference and Confounding Variables The reason only an experimental variable allows a causal inference is due to another epistemic gap, the possible influence of confounding variables when dealing with observational data. A causal inference means we learn that the independent variable and nothing else is responsible for the effect observed on the dependent variable. A causal inference is only possible if plausible alternative explanations for the observed effect on the dependent variable that do not involve the independent variable can be ruled out. In the context of a causal inference, such an alternative explanation is known as a confounding variable or confounder for short. If we randomly assign participants to conditions we can theoretically rule out confounders as an alternative explanation. However, in the case of an observational variable we cannot; there may be other reasons, the confounders, that are related to the observational variable that are responsible for the observed effect. Similar to the first epistemic gap, the inference that the independent variable is the cause of the effect in the dependent variable is underdetermined for an observational variable, but not underdetermined for an experimental variable.20 Let us exemplify this problem with a new example. Imagine you want to investigate the effectiveness of a novel drug against a control treatment (i.e., old drug) for the treatment of a viral infection in a hospital setting. The independent variable is the treatment (control treatment versus novel drug) and the dependent variable is viral load (i.e., whether or not the virus can still be detected in the system). Let us imagine that the results show that the new drug is more effective than the control treatment. That is, participants show a lower viral load (i.e., are less sick) after the novel treatment than the control treatment. The question we are trying to answer now is whether it matters for the inference we can draw from this study if the assignment to treatment condition is random or not. Let us begin by considering a non-random assignment of the independent variable. For example, one way to implement non-random random assignment could be to use the novel drug in one hospital and the control treatment in another hospital. If we were to run such a study, would this allow us to conclude that a difference in the dependent variable is due to the differences in treatment? This inference would only be allowed if the two hospitals were identical. If there were any systematic differences between the hospitals, say patients in the hospital with the control treatment are on average older than patients in the hospital with the new treatment (e.g., because they are in different areas with different population characteristics), then this difference could be responsible for the difference in the dependent variable. The systematic difference in age between the hospitals plays the role of a confounder. Age is responsible for the choice of hospital patients go to, because patients near the hospital in which the control treatment is administered are on average older than patients near the hospital in which the new treatment is administered. Age is also responsible for the difference on the dependent variable, because older patients are less likely to recover and thus have a higher viral load at the end of the study than young patients. In a situation in which a confounder is present, we cannot infer that the treatment is the cause of the observed effect. We again have the logical structure indicative of an underdetermination: The two conditions differ in terms of two characteristics, treatment status and age, so either of them (or both) can be responsible for the differences in the dependent variable. We cannot be sure which one of the two possible causes it is. Let us now consider a situation in which participants are randomly assigned to the two treatment conditions. Here, we again have data from two hospitals but participants in each hospital are randomly assigned to the treatment conditions. For example, for each new patient that shows the relevant symptoms the doctor administering the treatment takes a pre-randomised envelope that contains either the old drug or the novel drug.21 Because the assignments to conditions is random we would expect that confounders such as age are balanced across the two condition. Every participant that comes to any of the two hospitals has the same chance to either get the control treatment or the novel drug, independent of their age or other characteristics. Consequently, as long as randomness did not introduce an accidental confounding (see Section 1.3.2) we can attribute the effect on the dependent variable to the independent variable. In summary, the difference between an experimental and an observational variable is the degree with which you can rule out possible alternative explanations. For an experimental variable for which participants are randomly assigned to conditions we know that in theory all confounding variables should be balanced. So as long as the randomisation did proceed as planned, one can be certain that the only alternative explanation for the effect of the independent variable on the dependent variable is random chance. We always might get unlucky and a confounding variable just happens to be unbalanced in our data set. However, with larger sample sizes and larger effects the chance alternative explanation becomes increasingly unlikely. So for experimental variables the only epistemic gap when wanting to judge whether the independent variable is responsible for the effect of the independent variable on the dependent variable is the effect of random chance or noise (see Section 1.3.2). For example, in the main study that lead to the approval of the Biontech Covid-19 vaccine (Polack et al. 2020), over 40,000 participants were randomly assigned (i.e., more than 20,000 participants per condition) to either receive the real vaccine or a placebo (i.e., a saline injection without active ingredients). Among the participants that received the vaccine only 8 participants developed Covid-19 and among the participants that received the placebo 162 participants developed Covid-19. Whereas from these results we cannot definitely rule out that there is some confounding variable that explains the difference in contracting Covid-19 it seems extremely unlikely. Participants for this trial were recruited from six different countries (e.g., USA, Turkey, Brasil) and were diverse in their demographic characteristics (e.g., sex, ethnicity, age, weight), but these characteristics were extreme similar for both conditions (Polack et al. 2020, Table 1). For an observational variable for which participants are not randomly assigned to condition we do not know whether there is a potential confounding variable. One way to address this problem is to measure known confounding variables and show that they are not responsible for the difference in the dependent variable. But even when we are able to control or measure a large number of possible confounding variables, we can never be certain that there is not another unobserved confounding variable that is responsible for the effect. So for observational variables we always have to deal with the two epistemic gaps when wanting to judge whether the independent variable is responsible for the effect of the independent variable on the dependent variable, the problem of possible confounders plus random chance or noise. To end this section, let us come back to the example of our trial testing a new drug in a hospital setting. After the lengthy discussion on observational versus experimental variables you can hopefully see that the idea of only administering the new drug in one hospital and the control treatment in another hospital is a bad idea. Without proper randomisation of participants to treatments the inference that the drug is responsible for the effect on the viral load seems very weak thanks to the possible influence of confounders. You might even go so far to wonder who would ever run such a study without proper randomisation or believe the corresponding results. Sadly, a study that pretty much did exactly what we have sketched above  administering the novel drug only in one hospital and the control treatment in another hospital, with patients systematically differing between hospitals  played a very unfortunate role during the Covid-19 pandemic. In particular, the first study to suggest that Hydroxychloroquine was effective against Covid-19, the study by Gautret et al. (2020), had exactly this problem.22 Whereas critics where quick to point out this and other problems with the study (Bik 2020; Rosendaal 2020; Sayare 2020), the damage was done. The then current US president Donald Trump praised Hydroxychloroquine as a wonder cure of Covid-19. It required much scientific effort and follow-up studies, using resources that could have potentially been used more productively elsewhere, to show that it is not (for a full timeline of events see Sattui et al. 2020). The problem in this case was that whereas medical and statistical experts could immediately see the problems with the study, the general public could not. And once a false claim that appears to be scientific (i.e., in the media reported along the lines of researchers have shown that ) is established in the public discourse, it is often difficult to combat it. In general it seems that discussing the empirical evidence provided by a particular scientific study is either beyond the expertise available to the mass media or they are unwilling to invest the time commitment to do so. 2.5.2 Is Causal Inference from Observational Data Possible at All? What the previous section argues is that causal inference is generally only possible from experimental independent variables. With observational variables there can always be a confounder that is responsible for the effect instead of the independent variable. However, many interesting research questions cannot be investigated with experiments but only through observational variables. As we have discussed above, demographic variables or other immutable features of individuals, such as personality traits, are observational variables by definition. Likewise, many variables relating to lifestyle choices, such as dietary or exercise habits, might in principle be amenable to experimental manipulations, but in reality it seems difficult to impossible or completely unethical to run corresponding experiments. Does this mean we cannot draw causal inferences for such research questions? I believe the honest and realistic answer is that in the vast majority of cases we cannot. In my eyes a fair assessment of the situation is that, causal inference from observational data is literally the most difficult problem in the empirical sciences. Importantly, causal inference from observational data is not primarily a statistical problem. We have introduced the problem that confounders pose as an epistemic gap. And as for the other epistemic gaps, overcoming this epistemic gap requires diverse and conceptually strong evidence. There are statistical methods that can assist in providing such evidence, but they cannot provide the type of compelling evidence that is needed. The problem is that even if the observational data strongly suggests something, there always is the possibility that a confounder was missed or not adequately taken into account. As an example of the problem, let us consider the case of vitamin supplements, specifically vitamin C and E supplements (Lawlor et al. 2004; Woodside et al. 2005; Mozaffarian, Rosenberg, and Uauy 2018). Early evidence from large observational studies in the 90s with tens of thousands of participants suggested that taking vitamin C and E supplements reduces the chance of getting cancer and cardiovascular diseases to a considerable degree. Based on these positive results, large scale experiments (i.e., also with tens of thousands of participants) followed in which participants were randomly assigned to either take vitamin supplements or a placebo (i.e., sugar pill without vitamins) and monitored for several years. By and large, these experiments could not replicate the positive effects found in the observational studies. Unless an individual is susceptible to a vitamin deficiency, vitamin supplements do not appear to have a measurable health benefit. The probable reason for the difference between the observational studies and the experiments is likely due to an insufficient adjustment of socioeconomic status as a confounder. As often found, participants that came from a better socioeconomic background where more healthy (i.e., less likely to develop cancer and cardiovascular diseases) but they were also more likely to take vitamin pills (because they believed them to be helpful). Whereas the observational studies measured and tried to account for differences in socioeconomic status between participants who already take vitamin supplements and who do not, they only did so partially [e.g., they did not account for differences in the socioeconomic status of the parents which led to developmental differences that also affected the probability of developing cancer and cardiovascular diseases as well as the probability of taking vitamin pills; Lawlor et al. (2004)]. What this example shows is that even in a situation in which the confounder is in principle known (i.e., socioeconomic status) and the observational data sets were large (&gt; 10K participants), causal inference from observational data was not possible. Even after attempting to account for confounding, the observational data suggested a relationship that turned out to be spurious. The apparent problem was that accurately measuring the influence of the confounder was not possible, before knowing that the observed relationship is in fact spurious. Only an experiment was able to reveal that there was no effect of vitamin supplements. This example suggests that for many research questions and data sets common in psychology and related disciplines causal inference from observational data is equally difficult or even impossible. Especially as data sets are often considerably smaller and less is known about the causal relationship existing in a domain (i.e., which variables could act as confounders). As a consequence of the problem with observational data, the current book primarily focuses on experimental data sets and, where non-experimental variables are considered, their limitations will be discussed. Whereas focussing on experimental data restricts the type of research questions that can be investigated, it at least eliminates one of the three epistemic gaps introduced here. This also means that applying the methods introduced here to observational data sets will require additional care when trying to draw justified conclusions and is not recommended. To repeat what we have said above, the question of whether an effect found in observational data reflects a causal relationship is not a statistical question. So the statistical tools introduced here cannot provide an answer to the question of whether a relationship in observational data is causal. For researchers interested in analysing observational data, some good introductory literature that attempt to approach the problem of confounders in a principled manner are Rohrer (2018), McElreath (2020), Hernán and Robins (2021), and Shadish, Cook, and Campbell (2002). Note that, given the additional epistemic gap that needs to be bridged, these methods are more advanced than the methods introduced here (i.e., require technical and mathematical knowledge going beyond what is required here). After all, drawing causal inferences from observational data is literally the most difficult problem in the empirical sciences. 2.5.3 Internal versus External Validity Above we have talked about validity in the context of measurement. In this context, the question of validity is the question if a measure measures what it is supposed to measure (e.g., when a risk attitudes questionnaire really measures risk attitudes it has a high validity). However, the term validity is also used in the context of experimental versus observational studies. In this context, the two relevant types of validity are internal validity and external validity which do not refer to a specific measure, but are used to describe complete studies or research designs. We provide a brief introduction to these two terms here, for more see Shadish, Cook, and Campbell (2002).23 Internal validity refers to the internal structure of a study and reflects the degree with which the study provides evidence for the causal relationship between independent and dependent variable. This means that generally speaking, internal validity is high if a study is an experiment (i.e., independent variable is randomised) and internal validity is low if a study in an observational study.24 Within the terminology of the epistemic gaps introduced in this book, internal validity is related to the third epistemic gap. Only when we can be sure there are no possible confounders is the internal validity high, and we can only be sure of this in case we have an experimentally manipulated independent variable. External validity refers to the degree with which the results of a study generalise to different settings, such as different situations, people, stimuli, and times. Within the terminology of the epistemic gaps introduced in this book, external validity is related to the first epistemic gap, the underdetermination of theory by data. The degree with which we can be sure that our results really address our research question and are not confined to the specifics of our operationalisation we can be sure that the results generalise to other situations. In other words, if we only learn that the causal link holds in the very specific circumstances that are tested within our study, but do not actually hold in the general terms in which our research question is formalised, external validity is low. For example, the study by Hinze and Wiley (2011) introduced in Chapter 1 directly addressed the external validity by seeing whether the testing effects also holds for a different operationalisation of testing. 2.6 Summary In this chapter we have introduced a number of important concepts that allow us to describe studies and research designs. We have begun by highlighting that as empirical scientists the ultimate arbiter for whether or not to believe in a theoretical position or hypothesis is empirical evidence. This evidence should come from systematically collected data sets and not anecdotes. Data sets that can be used to address our research questions consist of independent variable(s) and usually one dependent variable. The distinction between both is that we assume the dependent variable depends on the independent variable. If the independent variable is an experimental variable for which participants are randomised onto conditions we can even infer that the independent variable is causally responsible for the effect of the dependent variable. If the independent variable is solely an observational variable, we generally cannot make such a causal judgement. The reason for why observational variables do not allow causal inferences lies in the third epistemic gap introduced here. For an observational variable, there can always be a different confounding variable that is responsible for both the effect on the independent variable and the dependent variable. Together the three epistemic gaps put clear limits on what we can learn from empirical data in psychology and related disciplines. The first epistemic gap, underdetermination of theory by data, is the difference between the research question and the operationalisation of the research question. Whereas the operationalisation attempts to address the research question they are usually not the same. The second epistemic gap, signal versus noise, concerns the relationship between the operationalisation and the statistical analysis. Even if the statistical analysis appears to provide support for the empirical hypothesis, we cannot be 100% sure of that. There always is the chance that the observed outcome just occurred by chance  that is noise  and does not represent a genuine signal in the data. Finally, the third epistemic gap, confounding variables, is always present when dealing with observational independent variables. As just summarised, in the absence of randomisation we can never really be sure if the independent variable and not a confounding variable is the reason for the observed effect. Thus, we can reiterate the message with which we ended the previous chapter. If we interpret statistical results, we need to be careful and humble with the conclusions we draw. We have also introduced the different data types we can deal with in a statistical analysis. Independent variables can be both numerical and categorical variables. If an independent variable is categorical, we generally call it an experimental factor, or just factor. Dependent variables can generally only be numerical variables, unless it is a binary numerical variable that we are treating as numerical. We have also argued that most genuine psychological variables we collect, such as responses on rating scales, are only on an ordinal scale and do not satisfy the assumptions of being a numerical variable. However, in our analyses we nevertheless treat them as numerical. This violation of a statistical assumption places further limits on the inferences that are permitted from our study. In line with this, we have argued that measurement in psychology is a generally difficult problem and simply assuming our measures provide more information than they actually provide is another inferential problem we have to deal with. References "],["standard1.html", "Chapter 3 The Standard Approach for One Independent Variable 3.1 Example Data: Note Taking Experiment 3.2 The Logic of Inferential Statistics 3.3 The Basic Statistical Model 3.4 Estimating the Statistical Model in R 3.5 Summary", " Chapter 3 The Standard Approach for One Independent Variable In this chapter we are introducing the standard statistical approach for analysing experimental data with one independent variable (i.e., one factor). The simple case for this is a study comparing two experimental conditions on one dependent variable. We will exemplify the standard approach for this design using a recent and straightforward experiment. 3.1 Example Data: Note Taking Experiment Heather Urry and 87 of her undergraduate and graduate students (Urry et al. 2021) (yes, all 87 students are co-authors!) compared the effectiveness of taking notes on a laptop versus longhand (i.e., pen and paper) for learning from lectures. 142 participants (which differed from the 88 authors) first viewed one of several 15 minutes lectures (TED talks) during which they were asked to take notes either on a laptop or with pen and paper. As this was a proper experiment, participants were randomly assigned to either the laptop (\\(N = 68\\)) or longhand condition (\\(N = 74\\)). After a 30 minutes delay, participants were quizzed on the content of the lecture. The answers from each participant were then independently rated from several raters (which agreed very strongly with each other) using a standardised scoring key resulting in one memory score per participant representing the percentage of information remembered ranging from 0 (= no memory) to 100 (= perfect memory).25 Figure 3.1 below shows the memory scores across both note taking conditions. Figure 3.1: Distribution of memory scores from Urry et al. (2021) across the two note taking conditions. In Figure 3.1, each black point shows the memory score of one participant so the full distribution of the data is visible. The shape of the distribution is also shown via a violin plot (i.e., the black outline around the points) to which we have added three lines representing three summary statistics of the data. From top to bottom these lines are the 75% quantile, the 50% quantile (i.e., the median), and the 25% quantile. The red points show the mean and the associated error bars show the standard error of the mean26. We see that the two means are quite similar, although the mean in the laptop condition is slightly larger, by 2.0 points (mean laptop = 68.2, mean longhand = 66.2). 3.2 The Logic of Inferential Statistics The previous paragraph provide us with descriptive statistics describing the results in the experiment by Urry et al. (2021): There is a memory difference of 2.0 on the scale from 0 to 100 between the laptop and the longhand condition for the sample of 142 participants. However, as researchers we are usually not primarily interested what happens in our sample. What we would like to know if our results generalises to the population from which this sample is drawn. In this case, we would like to know whether there is a memory difference between note taking with a laptop or in longhand format for students (as this is roughly the population the sample is drawn from). Going beyond the the present sample is the goal of inferential statistics. There are different inferential statistical approaches, and we are focussing on the most popular one, null hypothesis significance testing (NHST). We will describe NHST more thoroughly in the next chapter, but will already introduce its main ideas here. For NHST, the question of generalising from sample to population is about two different possible true states of the world: There either is no difference in conditions means in the population (i.e., there only is a mean difference in our sample due to chance) or there is a difference in the condition means in the population. To decide between these possibilities, we set up a statistical model for the data. This statistical model allows us to assess if there is no difference in the population  we call this possible state of the world the null hypothesis. More specifically, the statistical model allows us to test how compatible the data is with the null hypothesis of no difference. The test of the null hypothesis proceeds as follows: (1) We assume that the state of the world in which there is no difference in the population means is true. (2) Based on this assumption we calculate how likely it is to observe a difference as large as the one we have observed in our sample. (3) If the probability of observing a difference as large as the one we have is very small, we take this as evidence that the null hypothesis of no difference is not true  we reject the null hypothesis. (4) We act as if there were a difference in the population. In this case (i.e., we reject the null hypothesis and act as if there is a difference), we say our experimental manipulation has an effect. As is clear from this description, the logic underlying inferential statistics using NHST is not trivial. To make it clearer, let us apply the logic to the example data. We want to know whether the observed mean memory difference between note taking with a laptop and note taking in long hand format in our sample generalises to the population of students that take note in either of these formats. To do so, we set up a statistical model for our data. This model allows us to test how compatible our data is with the null hypothesis that there is no mean memory difference in the population. Specifically, the model allows us to calculate the probability of obtaining a memory difference as large as the one we have observed assuming the there is no mean memory difference in the population. If our data is incompatible with the null hypothesis (i.e., it is unlikely to obtain a memory difference as large as the one we have observed if the null hypothesis were true), we reject the null hypothesis of no mean memory difference in the population. Because we reject the null hypothesis we then act as if there was a mean memory difference in the population. In other words, we then act as if the type of note taking had an effect on memory after lectures in the population. Even though the logic of NHST is not necessarily intuitive, it is clearly helpful for researchers. After running an experiment we really would like to know if the difference observed in our experiment (i.e., in our sample) is meaningful in the sense that it generalises to the population from which we have sampled our participants. And in almost every actual experiment there is some mean difference between the condition (i.e., it is extreme unlikely that both conditions have exactly the same mean). Thus, we pretty much always face this question. NHST allows us to test whether the observed difference is compatible with a world in which there is no difference. If this is unlikely, we decide (i.e., act as if) there were a difference. What we can see from spelling out the logic in detail is that there are quite a few inferential steps we have to make to get to what we want. We design experiments with the goal in mind to find a difference between the different experimental conditions. However, we then do not test this directly. Instead, we test the compatibility of the data with the converse of what we are actually interested in  the null hypothesis of no effect. If this test fails (i.e., shows that the data is likely incompatible with the null hypothesis) we then make two inferential steps. First we reject the null hypothesis and then we act as if there were a difference. Both of these inferential steps are not necessitated logically. What this means is that inferences based on NHST alone are never extremely strong. NHST is the de facto standard procedure for inferential statistics across empirical sciences (i.e., not only in psychology and related disciplines). Understanding the logic of NHST will enable you to understand the majority of empirical papers and will also allow you to apply inferential statistics in your own research. Nevertheless, there exist a long list of popular criticisms of NHST (e.g., Rozeboom 1960; Meehl 1978; Cohen 1994; Nickerson 2000; Wagenmakers 2007). We will discuss these criticisms in more detail in later chapters, but for now it is important to realise that NHST does not allow to test, or prove, whether there is a mean difference in the population. The only thing NHST calculates is a probability of how compatible the data is with the null hypothesis. If this probability is low that does not necessarily mean that there is a difference. Likewise, if this probability is high that does not necessarily mean there is no difference. All inferences we draw based on NHST results are probabilistic in itself (i.e., can be false). So the most important rule when interpreting the results from NHST is to be humble. NHST never proves or confirms anything. Instead NHST results suggest or indicate certain interpretations. If we do not over-interpret results, but stay instead stay humble in our interpretations, we are unlikely to fall prey to the common (and often justifiable) criticisms of the NHST framework. 3.3 The Basic Statistical Model To apply inferential statistics in the NHST framework to our data, we begin by setting up a statistical model to the data. A statistical model attempts to explain (or predict) the observed values of the dependent variable (DV) from the independent variable (IV).27 In the experimental context this means predicting our observed outcome, the DV, from the experimental manipulation, the IV. The basic statistical model partitions the observed DV into three parts that: the overall mean, which for reasons that will become clear later is called the intercept, the effect of the IV, and the part of the data that cannot be explained by the model, the residuals. When summing these three parts together, they result in the observed value. In mathematical form we can express this as \\[\\begin{equation} \\text{DV} = \\underbrace{\\text{intercept}}_{\\text{overall mean}} + \\text{IV-effect} + \\text{residual}. \\tag{3.1} \\end{equation}\\] (For those not used to reading mathematical expressions, the point at the end of the equation is simply a full stop that ends the sentence and has no mathematical meaning.) As someone without a mathematics background myself, I know that equations in a text are often more intimidating than immediately useful. Consequently, before moving on it makes sense to go through this equation in more detail. Furthermore, all statistical analyses discussed in this book are applications of Equation (3.1). This equation forms the foundation for the statistical analysis of experimental data and thus understanding it will unlock all analyses discussed in this book. Consequently, it makes sense to spend more time on it. Let us consider the the variables in Equation (3.1) in more detail. When doing so, we also consider how many different possible values each variable can take on.28 The following Figure, a variant of Figure 3.1, shows the elements graphically and we explain them in the text just below. Figure 3.2: Data from Urry et al. (2021) showing the overall mean (intercept, blue dotted line), the condition specific effects (difference between dashed red lines for the condition means and the blue line), and the residuals (grey lines from condition means to data points). \\(\\text{DV}\\): The dependent variable, DV, are the observed values, one for each observation/participant. For the example data this are all the 142 black data points shown in Figure 3.2. Thus, our statistical model tries to explain the individually observed values. \\(\\text{intercept}\\): The intercept represents the overall mean. Consequently, we only have one intercept (i.e., the intercept is the same for each observation). In experimental designs we define this as the mean of all condition means. For the example data the intercept is (68.2 + 66.2) / 2 = 67.2 and is shown as a blue dotted line in Figure 3.2. \\(\\text{IV-effect}\\): The IV-effect represents the effect of our independent variable which we define as the difference between the condition means and the intercept (i.e., the deviation of the condition means from the intercept). Thus, we always have as many different IV-effects as we have conditions. For the example data with only two conditions, we only have two different IV-effects, both of which with the same magnitude and only differ in sign, 1.0 for the laptop condition and -1.0 for the longhand condition. If we add these values to the intercept, we get the condition means. As we will discuss further below, this is the most relevant part for answering the statistical question of interest. In Figure 3.2, the red dashed line (and the red points) show the condition means, thus the condition effects are the differences between the blue line and the red lines. \\(\\text{residual}\\): The residuals are the idiosyncratic aspects of the data that are left unexplained by the statistical model. As the model only predicts the condition means (i.e., intercepts plus independent variable), these are the deviations of the individual observations from the condition means. Thus, as for the DV, we have as many residuals as we have values of the DV. In Figure 3.2, the residuals are shown as grey lines from the condition means to each data point. This is all the information (or variability in the data) our model cannot explain. 3.3.1 Model Predictions A simplification of Equation (3.1) that makes it clearer what the statistical model predicts is obtained if we ignore the residuals for a moment. As a reminder, the residuals are the part of the data that remains unexplained. In other words, these represent all the idiosyncratic parts of the data independent of our manipulation (e.g., some participants have better memory than others independent of how they took notes). What remains from our statistical model if we ignore all idiosyncratic aspects are only the predictions based on our IV. In the case of experimental data, the IV is the experimental condition. Thus, what a statistical model actually predicts is the means of the experimental conditions. We can again formalise this as \\[\\begin{equation} \\hat{\\text{DV}} = \\text{intercept} + \\text{IV-effect}. \\tag{3.2} \\end{equation}\\] Here, the hat symbol (\\(\\hat{}\\)) means predicted value. Thus in contrast to the actual DV above, we only have the predicted DV in this equation. When performing statistical analyses it sometimes help to remind oneself that all a standard statistical model predicts are the condition means. We generally do not make predictions about individual participants or consider other factors that are not part of the model. We only predict, and are interested in, the condition means. 3.3.2 Statistical Model for the Example Data Let us take a look at the first six participants and their values for all the variables in the basic statistical model to get a better understanding of Equation (3.1). pid condition overall  intercept iv_effect prediction residual 1 laptop 65.8 67.2 1 68.2 -2.4 2 longhand 75.8 67.2 -1 66.2 9.6 4 longhand 50.0 67.2 -1 66.2 -16.2 5 laptop 89.0 67.2 1 68.2 20.8 6 longhand 75.6 67.2 -1 66.2 9.4 8 longhand 83.3 67.2 -1 66.2 17.1 The first three columns show the data. pid is the participant identifier (id) column. As it is often the case for real data, some ids are missing (here 3 and 7) for various reasons (e.g., potential participants were interested in the study and received an id, but then did not finish or start the experiment) so the first 6 rows already go up to pid = 8. condition tells us in which note taking condition a participant was and overall is their memory score on the scale from 0 to 100 which serves as the DV in the statistical model (i.e., the left-hand side in Equation (3.1)). The four right most columns contain the values of the variables on the right-hand side of Equation (3.1), the intercept, the iv_effect, and the residual. In addition, the prediction column shows the left-hand side of Equation (3.2). As described above, every observation (i.e., row) has a idiosyncratic DV and residual. We also see that all values share one intercept, and the IV-effect is condition specific. As a consequence, the prediction column (which is the sum of intercept and iv-effect) also has two values, one for each condition. Finally, we can see that the sum of the three values on the right-hand side of Equation (3.1) equals the observed value of the DV. For example, consider pid = 4. If we enter the values into Equation (3.1) we have \\[ 50.0 = 67.2 + (-1) + (-16.2). \\] From this example data we can also understand better what the residuals mean, they are the difference between the observed value and the predicted value, \\(\\text{residual} = \\text{DV} - \\hat{\\text{DV}}\\). Consider again pid = 4. Here we have \\[ -16.2 = 50- 66.2. \\] We can also see how the residual captures the idiosyncratic aspects of our data that cannot be explained by the condition means. For example, some participants  such as pid = 5 and pid = 8  have large positive residuals indicating that they have good memory independent of their note taking condition. Likewise, pid = 4 has a large negative residual indicating comparatively worse memory (again independent of the note taking condition). 3.3.3 Understanding the Statistical Model Now that we have described the parts of the statistical model we are almost ready to fit the model and interpret the output. Before doing so it makes sense to look at all the parts again individually and try to understand why we set up the statistical model in the way we do. Remember, our goal is to evaluate whether there is an effect of the experimental manipulation (i.e., a difference between the two note taking conditions) in the population from which the data is sampled. To do so, we set up a model that partitions the observed data into three parts, the intercept representing the overall mean, the condition specific effect (IV-effect) representing the difference of the condition means from the intercept, and the residuals representing the idiosyncratic part not explained by the model. The reason for doing so is that it allows us to zoom in on what matters for our statistical question, the condition specific effect. To answer the question if there is a difference between the conditions in the population, we can now focus on this part of the model. The overall level of performance captured in the intercept and the residuals can (for now) be ignored for this question. Consequently, the statistical test reported below is a statistical test of the condition effect. Thus, the reason for setting up the statistical model in this way is to make it easy to get an answer to the question that interests us: Is there an effect of the note taking manipulation/conditions on memory? To answer this question we only need to consider the condition effect. 3.4 Estimating the Statistical Model in R 3.4.1 Package and Data Setup For the statistical analyses reported in this book we generally use the afex package (Singmann et al. 2021) in combination with the emmeans package (Lenth 2021). afex stands for analysis of factorial experiments and simplifies many of the things we want to do (full disclaimer: I am the main developer of afex). Most analyses can also be performed with different functions, but it is often easiest to use afex functions as they are developed particularly for cognitive and behavioural researchers working with experimental data. More specifically, afex functions provide the expected results for experimental data sets out-of-the-box without the need to change any settings (which is not true for the corresponding non-afex functions). emmeans stands for estimated marginal means and is the package we use once a statistical model is estimated to further investigate the results. afex and emmeans are fully integrated with each other which allows to test practically any hypotheses of interest with a combination of these two packages in a straight forward manner. We already introduce the interplay of these two packages here, and the next chapters will showcase the full power of this combination. We also regular use functions from the tidyverse package (e.g., for plotting). tidyverse is a collection of packages developed mainly by RStudio and their head data scientist Hadley Wickham. A full introduction of the tidyverse is beyond the scope of the present book, interested readers are encouraged to read the introductory book, Wickham and Grolemund (2017), which is also available for free online. We begin the analysis by loading the three packages first (use install.packages(c(\"afex\", \"emmeans\", \"tidyverse\")) in case they are not yet installed). We also change the default ggplot2 theme using theme_set() to a nicer one. library(&quot;afex&quot;) library(&quot;emmeans&quot;) library(&quot;tidyverse&quot;) theme_set(theme_bw(base_size = 15) + theme(legend.position=&quot;bottom&quot;, panel.grid.major.x = element_blank())) The next step would be loading in the data. This is made easy here as the data from Urry et al. (2021) is part of afex, under the name laptop_urry. So we can load it with the data() function. We then also get an overview of the variables in this data set using str(), which returns the structure of a data.frame. data(&quot;laptop_urry&quot;) str(laptop_urry) #&gt; &#39;data.frame&#39;: 142 obs. of 6 variables: #&gt; $ pid : Factor w/ 142 levels &quot;1&quot;,&quot;2&quot;,&quot;4&quot;,&quot;5&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ condition : Factor w/ 2 levels &quot;laptop&quot;,&quot;longhand&quot;: 1 2 2 1 2 2 1 2 2 1 ... #&gt; $ talk : Factor w/ 5 levels &quot;algorithms&quot;,&quot;ideas&quot;,..: 4 4 2 5 1 3 5 2 5 4 ... #&gt; $ overall : num 65.8 75.8 50 89 75.6 ... #&gt; $ factual : num 61.7 68.3 33.3 85.7 69.2 ... #&gt; $ conceptual: num 70 83.3 66.7 92.3 82.1 ... The str function shows six variables, three of which we have already mentioned above: pid: participant identifier, a factor with 142 levels, one for each participant. condition: factor identifying which note taking condition a participant belongs to, with two levels, laptop and longhand. talk: A factor identifying which TED talk a participant saw, with 5 level. overall: Numeric variable with participants overall memory performance on a scale from 0 (= no memory) to 100 (= perfect memory). This variable is called overall because it is the average of two separate memory performance scores given below. factual: Numeric variable with participants memory score for factual questions (ignored in this chapter). conceptual: Numeric variable with participants memory score for conceptual questions (analysed in the next chapter). 3.4.2 Estimating the Statistical Model For estimating a basic statistical model using afex we can use the aov_car() function. The next code snippet show how to do so for the example data, when saving the output in object res1 . res1 &lt;- aov_car(overall ~ condition + Error(pid), laptop_urry) #&gt; Contrasts set to contr.sum for the following variables: condition The first argument to aov_car() is a formula specifying the statistical model, overall ~ condition + Error(pid). The second argument identifies the data.frame containing the data (i.e., all the variables appearing in the formula), laptop_urry. We can also see that calling aov_car() produces a status message informing us that contrasts are set to contr.sum for the IVs in the model. This message is only shown for information purposes and can be safely ignored (we want contr.sum as contrasts for our variables, but as this is not the default R behaviour a message is shown). A formula in R is defined by the presence of the tilde-operator ~ and the main way for specifying statistical models. It allows specifying statistical models in a similar way to the mathematical formulation, specifically the prediction equation of the statistical model, Equation (3.2). Therefore, a formula provides a comparatively intuitive approach for specifying a statistical model. On the left hand side of the ~ we have the dependent variable, overall. On the right hand side we have the variables we want to use to predict the dependent variable. In the present case, the right-hand side consists of two parts concatenated by a +, the independent variable condition and an Error() term with the participant identifier variable pid. Thus, there are two difference between the formula used here and the prediction Equation (3.2), the formula misses an explicit intercept and we have specified an Error() term that is missing in Equation (3.2). Let us address these two difference in turn. The intercept is not actually missing from this equation, but implicitly included. More specifically, an intercept is specified using a 1 in a formula. However, unless an intercept is explicitly suppressed  which can be done by including 0 in the formula (and which should only be done if there are very good statistical reason to do so; i.e., it makes very rarely sense)  it is always assumed to be part of the models. Consequently, including it explicitly produces equivalent results. The following code shows this by comparing the previous result without explicit intercept, res1 with an aov_car call with explicit intercept using the all.equal() function. This function can be used to compare arbitrary R objects and only returns TRUE if they are equal. res1b &lt;- aov_car(overall ~ 1 + condition + Error(pid), laptop_urry) #&gt; Contrasts set to contr.sum for the following variables: condition all.equal(res1, res1b) #&gt; [1] TRUE The Error() term is a mandatory part of the model formula when using aov_car() and is used to specify the participant identifier variable (i.e., pid in this case). For a simple example as the present one that seems unnecessary, but later in the book we will see why the requirement of the Error() term is useful. Before looking at the results, let us quickly explain why the function for specifying models is called aov_car(). A regular statistical model such as the ones considered here that solely includes factors (i.e., categorical variables) as independent variables is also known as analysis of variance, which is usually shortened to ANOVA.29 The basic R function for ANOVA models is simply called aov(). However, aov() does not in all cases return the expected results for all types of ANOVA models considered in this book (i.e., in some situations aov() can return results that would be considered inappropriate, even when used carfeully). An alternative to aov() is the Anova() function from package car (Fox and Weisberg 2019) (where car stands for the book title, Companion to Applied Regression). Anova() always returns the expected and appropriate ANOVA results when used correctly. However, calling Anova() requires at least two function calls and can become tricky with more complicated models discussed in later chapters. aov_car() combines the simplicity of model specification of the aov() function with the appropriate statistical results from the Anova() function from the car package (i.e., aov_car() calls Anova() internally). 3.4.3 Interpreting the Results We can now look at the results of our statistical model. For this, we simply call the object that contains the results res1 (we would get the same output when calling print(res1) or nice(res1)). res1 #&gt; Anova Table (Type 3 tests) #&gt; #&gt; Response: overall #&gt; Effect df MSE F ges p.value #&gt; 1 condition 1, 140 269.66 0.52 .004 .471 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 The default aov_car() output is an Anova Table we will see throughout the book. We can also see that the results table contains Type 3 tests, but we will ignore this for now. The only other option, Type 2 tests, produces the same results for the example data. We will get back to the meaning of type of test in later chapters when it makes a difference and ignore this part until then. The next line of the results table is only reference information. We see that the response variable, which we also know as DV, is overall, just as we intended. We then get a table of effects, which in this case only has one row, the effect of condition. This row contains all the information for our null hypothesis significance test (NHST) for the condition effect. The most important column in this output is the last column, p.value, or \\(p\\)-value. The \\(p\\)-value in this column is the main results of NHST and allows us to judge the compatibility of the data with the null hypothesis. It is the probability of obtaining a difference as extreme as observed when assuming that the null hypothesis of no difference is true. We see that in this case the \\(p\\)-value is not small, it is .47. Thus, the data are not incompatible with the null hypothesis and does not suggest that there is a memory difference between note taking with a laptop or in longhand format during lectures. In general, researchers have adopted a significance level of .05. This means that if a \\(p\\)-value is smaller than .05 we treat this as evidence that the data is incompatible with the null hypothesis. In this case we would say the result is significant. However, as in our case the result is not smaller than .05 the result is not significant (I would avoid saying insignificant if the \\(p\\)-value is larger than .05, as significant is a technical term here). Thus, in the present case we do not reject the null hypothesis. The present data therefore do not provide evidence that the observed difference between the two modes of note taking generalises from the sample to the population according to NHST. There are two further important columns whose results generally need to be reported, df, which stands for degrees of freedom (or df), and F. Understanding these columns in detail is beyond the scope of the present chapter, so we will only introduce them briefly. There are two degrees of freedom reported here, the first value, 1, is the numerator degree of freedom. It is always given by number of conditions minus 1. In the present case, we have two conditions, laptop and longhand, so the numerator df are 2 - 1 = 1. The second value is the denominator df, which are generally given by number of participants minus numerator df minus 1. Here we have 142 participants and therefore 142 - 1 - 1 = 140. In general, the larger the denominator df (i.e., the more participants we have) the better we can detect incompatibility with the null hypothesis (i.e., the easier it is to get small \\(p\\)-values). The \\(F\\)-value is a value expressing the observed incompatibility of the data with the null hypothesis. If \\(F \\leq 1\\), the data are compatible with the null hypothesis. If \\(F &gt; 1\\) the data are to some degree incompatible with the null hypothesis, with larger values indicating more incompatibility. The \\(p\\)-value is calculated from df and \\(F\\)-value. Consequently, the results are usually reported in the following way: \\(F(1, 140) = 0.52\\), \\(p = .471\\). The next column that is important is ges which stands for generalised eta-squared, using the mathematical notation with Greek letters, \\(\\eta^2_G\\). \\(\\eta^2_G\\) is a standardised effect size that tells us something about the absolute magnitude of the observed effect (Olejnik and Algina 2003; Bakeman 2005). More specifically, \\(\\eta^2_G\\) is supposed to be a measure of the proportion of variance in the DV that can be accounted for by a specific factor or IV in the model. For example, in the present case the condition effect is supposed to explain 0.4% of the variance in performance. In general, we should avoid standardised effect sizes such as \\(\\eta^2_G\\) and instead report simple effect sizes. A simple effect size is expressed in units of our measured DV. For example, throughout this chapter we have mentioned that the observed difference in memory performance between both note taking conditions is 2.0 on the scale from 0 to 100. Here, the difference of 2.0 is a simple effect size. We will have to say more about effect sizes later, but as some journal editors or publishing guidelines require standardised effect sizes (which is statistically not a reasonable recommendation in my eyes) the default output contains it. Finally, the default output contains the MSE column, which stands for mean squared errors. This column is mainly included for historical reasons. Traditionally, ANOVA models could relatively easily be calculated by hand or by calculator based on different variance terms (hence the name, analysis of variance). One of this term is the mean squared error from which, in combination with the residual squared error, the \\(F\\)-value can be calculated. In my undergrad studies I still learned to calculate ANOVA by hand, but this seems rather unnecessary nowadays. Hence, we will simply ignore this column. Interested reader can find a detailed explanation about the meaning of MSE for example in Howell (2013) or Baguley (2012). One thing we note in the results table is that it does not contain any information about the intercept. However, as discussed above, the intercept is included in the model. The reason for omitting the intercept from the default output is that it is generally not of primary interest. In experimental research usually the main interest is in the effect of our independent variables, the effect of the experimental manipulation. The statistical model that separates the intercept (i.e., overall mean) from the condition effect allows to zoom in on the relevant part. In line with this, the default output of aov_car does the same. Later chapters will show how we can also get information about the intercept. Estimating a statistical model with aov_car() provides us with the inferential statistical results, the null hypothesis tests for the IV-effects shown above. To get these, we just need to call the object containing the results at the R prompt (e.g., calling res1 in the present case). However, we can use the results object also for others parts of the statistical analyses, for data visualisation and follow-up analyses. 3.4.4 Data Visualisation For data visualisation we can use the afex function afex_plot() which is built on top of the ggplot2 package. afex_plot() requires an estimated model object (e.g., as returned from aov_car()) and specifying which factors of the model we want to plot. In the present case, we only have one factor, condition, so we can only choose this one. Importantly, all factors passed to afex_plot() need to be passed as character strings (i.e., enclosed with \"...\"). afex_plot(res1, &quot;condition&quot;) Figure 3.3: afex_plot() figure for data from Urry et al. (2021) This simple call to afex_plot() produces already a rather good looking results figure combining the individual-level data points (in the background in grey) with the condition means (in black). Individual data points in the background that have the same or very similar values are displaced on the x-axis so they do not lie on top of each other. This is achieved through package ggbeeswarm (which needs to be installed once: install.packages(\"ggbeeswarm\")). The plot also per default shows 95% confidence intervals of the means, which we will explain in detail in a later chapter. As afex_plot() returns a ggplot2 plot object, we can manipulate the plot to make it nicer. p1 &lt;- afex_plot(res1, &quot;condition&quot;) p1 + labs(x = &quot;note taking condition&quot;, y = &quot;memory performance (0 - 100)&quot;) + coord_cartesian(ylim = c(0, 100)) + geom_line(aes(group = 1)) For example, in the code snippet above we first save the plot object as p1 and then call a number of ggplot2 function on this plot object to alter the plot appearance (in ggplot2 graphical elements are added to a plot using +). Function labs() is used to change the axis labels, coord_cartesian() changes the extent of the y-axis (i.e., the plot now show the full possible range of memory performance score), and geom_line(aes(group = 1) adds a line connecting the two means. This figure could now be used in a results report or manuscript as is. 3.4.5 Follow-Up Analysis Follow-up analysis refers to an inspection of the predicted condition means and their relationships. In the case of a single independent variable with two levels (e.g., laptop versus longhand) their is not much to investigate in this regard. We can nevertheless show the general procedure. For follow-up analyses we generally begin with function emmeans() from package emmeans (Lenth 2021). Function emmeans() then returns the estimated marginal means, which is a slightly complicated way of saying condition means, plus additional statistical information. Similarly to afex_plot(), emmeans() requires an estimated model object as well as the specification of a factor in the model for which we want to get the condition means: emmeans(res1, &quot;condition&quot;) #&gt; condition emmean SE df lower.CL upper.CL #&gt; laptop 68.2 1.99 140 64.3 72.1 #&gt; longhand 66.2 1.91 140 62.4 70.0 #&gt; #&gt; Confidence level used: 0.95 For now we only focus on the estimates means in column emmean and ignore the additional inferential statistical information in columns SE to upper.CL. We can see that the reported means match the means given in the text at the very beginning of the chapter, 3.1. The power of emmeans is not only to provide the condition means, but it also allows us to perform calculation on the condition means. For example, in the case of a factor with two levels we can easily calculate the difference between the condition means as our simple effect size. For this, we can save the object returned by emmeans() and then call the pairs() function on this object which gives us all pairwise comparisons of conditions means of which there is only one in the present case (we would get the same results by combining both calls into one: pairs(emmeans(res1, \"condition\"))): em1 &lt;- emmeans(res1, &quot;condition&quot;) pairs(em1) #&gt; contrast estimate SE df t.ratio p.value #&gt; laptop - longhand 1.99 2.76 140 0.722 0.4715 The output shows a mean difference of 1.99 which slightly differs from the 2.0 reported above, which is slightly concerning. However, the results reported above are rounded to one decimal only. If we do so for the present results, we also get an estimated difference of 2.0 (we will not explain this code in detail here): em1 %&gt;% pairs() %&gt;% as.data.frame() %&gt;% format(digits = 1, nsmall = 1) #&gt; contrast estimate SE df t.ratio p.value #&gt; 1 laptop - longhand 2.0 2.8 140.0 0.7 0.5 3.5 Summary The goal of this chapter was to introduce the standard statistical approach for analysing experimental data with one independent variable with two levels  an experiment with two conditions. Practically every time when we run such an experiment, we observe that there is some mean difference in the dependent variable between the two conditions. For our example data by Urry et al. (2021) there was a memory difference of 2.0 points between the two note taking conditions (laptop versus longhand) on the response scale from 0 to 100. The important statistical question we then have is whether there is any evidence suggesting that the observed difference in our sample generalises to the population. The sample are the participants in our experiment and the population refers to all possible participants that could have been sampled. For Urry et al. (2021) this population could be loosely described as students taking notes or maybe more precisely undergraduate students at research intensive (R1) US universities. The question we would like to get a statistical answer to is: Should we believe that there generally is a memory difference between note taking with a laptop versus longhand? To answer this question we need inferential statistics. The inferential statistical approach we are using is called null hypothesis significance testing or NHST. However, NHST does not directly address the question whether there is evidence for a difference in the population. Instead, NHST tests the compatibility of the data with the null hypothesis  the assumption that there is no difference between the condition in the population. The most important result from NHST is the \\(p\\)-value. The \\(p\\)-value is a measure of the compatibility of the data with the null hypothesis; it is the probability of obtaining a results as extreme as observed assuming the null hypothesis is true. If the \\(p\\)-value is smaller than .05 we reject the null hypothesis that there is no difference. In this case we decide that there is evidence for a difference (although this does not follow with logical necessity). To apply NHST to the data we set up a statistical model that observed partitions the data into three parts (Equation (3.1)): the intercept representing the overall mean, the effect of the independent variable (i.e., the difference of the condition means from the intercept), and the residuals representing the idiosyncratic aspects not explained by the other parts of the model. This partitioning allows us to zoom in on the part of the data that we are interested in, the effect of our independent variable, the experimental manipulation. To estimate a statistical model to the data we used function aov_car() from the afex package. aov_car() allows us to specify the statistical model using a formula of the form dv ~ iv + Error(pid) (where pid refers to the variable in the data with the participant identifier) mimicking the mathematical specification of the statistical model. The default output returns an ANOVA table which provides a null hypothesis significance test for our iv, the independent variable. The returned table is called an ANOVA table because statistical models that only contain factors are called analysis of variance or ANOVA. In the present case, the statistical model only has a single factor, note taking condition, with two levels, laptop versus longhand. In the returned ANOVA table, we do not only have the \\(p\\)-value for our experimental factor, but additional inferential statistical information such as the degrees of freedom, df, and the \\(F\\)-value. We can also use the object returned from aov_car() for plotting using function afex_plot(). This function produces a plot combining the individual-level data points with the condition means. This provides a comprehensive display of the data of the experiment. As the function returns a ggplot2 object, this plot can be be easily modified to create a figure that can be used in a results report. We can also use the object returned from aov_car for follow-up analyses using emmeans. With emmeans we can easily obtain the condition means (or estimated marginal means) on the dependent variable. Based on these condition means we can calculate the observed effect size (i.e., the mean difference). Applying the statistical model to the data from Urry et al. (2021) showed a non significant difference, \\(F(1, 140) = 0.52\\), \\(p = .471\\). This suggests that there is no difference in memory performance after watching a talk and taking notes with either a laptop or in longhand format. References "],["case-study-1-more-results-from-note-taking-studies.html", "Chapter 4 Case Study 1: More Results from Note Taking Studies 4.1 Conceptual Memory Data from Urry et al. (2021) 4.2 Why are Experiments Replicated? 4.3 Conceptual Memory Data from Mueller and Oppenheimer (2014) 4.4 Summary", " Chapter 4 Case Study 1: More Results from Note Taking Studies In this chapter we will apply what we have learned in the previous chapter - how to analyse experimental data with one experimental manipulation and two conditions. For this, we will again take a look at the data from Urry et al. (2021). Additionally, we will analyse data from Mueller and Oppenheimer (2014). This study was the first published study investigating the question of note taking with a laptop or in longhand format and was the basis on which Urry et al. (2021) planned their study. For the data of Mueller and Oppenheimer (2014) we will perform a full analysis starting with reading in the data. So in addition to performing the statistical hypothesis test, we will calculate some descriptive statistics. We start the analysis in this chapter in the same way as in the previous chapter, by loading the three packages we generally use, afex, emmeans, and tidyverse, and set a nicer ggplot2 theme. Before doing so it is probably a good idea to restart R (unless, of course, you are just starting R). In RStudio this can be conveniently done through the menu by clicking on Session and then Restart R. In other R environments you might need to restart the program. The benefit of restarting R is that it should create a blank R session in which no packages are loaded and no objects exist in the workspace. Only such a blank sessions ensures that, once we have obtained a set of results, we can recreate them later using the same code. That is, a blank R session avoids any potential problems due to analyses performed in a previous session that are still lingering. Restarting R should generally be done when starting a new analysis or after one is completely done with an analysis. In the latter case, it makes sense to restart R and the rerun all code one has saved in ones script to ensure that all results replicate based on only the code in the script (and do not require some additional code not saved). library(&quot;afex&quot;) library(&quot;emmeans&quot;) library(&quot;tidyverse&quot;) theme_set(theme_bw(base_size = 15) + theme(legend.position=&quot;bottom&quot;, panel.grid.major.x = element_blank())) 4.1 Conceptual Memory Data from Urry et al. (2021) As a quick reminder, Urry et al. (2021) showed their participants short lectures (TED talks) on video during which participants were allowed to take notes. One group of participants, the laptop condition, could take notes on a laptop, whereas the participants in the longhand condition could take notes with pen and paper. After the lecture participants were quizzed on two aspects of the content of the lecture, factual questions and conceptual questions. In the previous chapter we have analysed the overall memory score which was the average of the performance for the factual questions and the conceptual questions. Here, we are only concerned with the memory performance for conceptual questions. We begin our analysis by loading in the data (which is part of afex can be loaded with data()) and getting an overview of the variables using str(): data(&quot;laptop_urry&quot;) str(laptop_urry) #&gt; &#39;data.frame&#39;: 142 obs. of 6 variables: #&gt; $ pid : Factor w/ 142 levels &quot;1&quot;,&quot;2&quot;,&quot;4&quot;,&quot;5&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ condition : Factor w/ 2 levels &quot;laptop&quot;,&quot;longhand&quot;: 1 2 2 1 2 2 1 2 2 1 ... #&gt; $ talk : Factor w/ 5 levels &quot;algorithms&quot;,&quot;ideas&quot;,..: 4 4 2 5 1 3 5 2 5 4 ... #&gt; $ overall : num 65.8 75.8 50 89 75.6 ... #&gt; $ factual : num 61.7 68.3 33.3 85.7 69.2 ... #&gt; $ conceptual: num 70 83.3 66.7 92.3 82.1 ... As before, we have the participants identifier variable in pid and the note taking condition in variable condition. We can also guess that the conceptual memory scores are in the aptly name variable conceptual (if we were unsure about this, we could also check the documentation of the data at ?laptop_urry). Usually, once the data is sufficiently prepared (i.e., we have performed some sanity checks and identified DV and IV), the first step in an analysis should be plotting the data. This could be done using ggplot2 directly. However, in cases such as the present one where it is very clear which statistical model we are going to estimate it is often a bit less effort to plot the data with afex_plot(). Thus, we start by estimating the statistical model for the conceptual memory performance of the data from Urry et al. (2021) and save the estimated model object as mc_urry. For this, we again use aov_car() on the laptopt_urry data and specify the model using the formula interface. The DV we are considering here is conceptual, our IV is condition, and the participant identifier is pid. Consequently, the formula is conceptual ~ condition + Error(pid). Then, before looking at the inferential statistical results, we use this model object to plot the data using afex_plot. mc_urry &lt;- aov_car(conceptual ~ condition + Error(pid), laptop_urry) #&gt; Contrasts set to contr.sum for the following variables: condition afex_plot(mc_urry, &quot;condition&quot;) Figure 4.1: Conceptual memory scores from Urry et al. (2021) across note taking conditions The goal behind beginning with plotting the data is that it allows to see whether the data looks alright. That is, we check whether there are any features that stand out such as clear outliers or an unusual pattern in the data. If this were the case, we would try to figure out if we can find a reason for this issue or how we deal with it. But, as the data looks alright, we continue and consider the results of the significance test: mc_urry #&gt; Anova Table (Type 3 tests) #&gt; #&gt; Response: conceptual #&gt; Effect df MSE F ges p.value #&gt; 1 condition 1, 140 441.76 1.00 .007 .319 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 The ANOVA table reveals that the significance test for the effect of condition is not significant with \\(p = .319\\). Thus, in line with the finding that there is no evidence for a difference in overall memory performance, there also is no evidence for a difference in memory for conceptual information. We can also again use emmeans to see the condition means (or estimated marginal means). In line with Figure 4.1 (as afex_plot internally also uses emmeans it shows exactly the same means in graphical form), the memory score in the laptop condition is descriptively around 3.5 points higher than the score in the longhand condition. emmeans(mc_urry, &quot;condition&quot;) #&gt; condition emmean SE df lower.CL upper.CL #&gt; laptop 73.5 2.55 140 68.5 78.6 #&gt; longhand 70.0 2.44 140 65.2 74.8 #&gt; #&gt; Confidence level used: 0.95 Before moving to the next data set, let us consider how we could report this analysis in a research report. We could for example write: As shown in Figure 4.1, participants conceptual memory scores (on a scale from 0 to 100) are descriptively slightly larger in the laptop condition compared to the longhand condition. We analysed these scores with an ANOVA with one factor, note taking condition, with two levels (laptop vs. longhand). The effect of note taking condition was not significant, \\(F(1, 140) = 1.00\\), \\(p = .319\\). This indicates that the data does not provide evidence for a difference in memory for conceptual information based on how notes are taken during lectures. 4.2 Why are Experiments Replicated? The experiment by Urry et al. (2021) was not the first experiment investigating the effect of note taking during lectures on memory. In contrast, their study was a replication of Mueller and Oppenheimer (2014). A replication is the act of rerunning an existing study to see if one can obtain (or replicate) the results of the previous study. As we have discussed before, inferences from NHST are never conclusive as they are probabilistic and require multiple inferential steps. Replications are one of the most important tools in science for overcoming at least the probabilistic uncertainties associated with the inferences we draw from experimental data. For example consider that several independent but otherwise as similar as possible experiments  that is, replications of the same experiment  all obtain a significant result (i.e., indicate that the data are incompatible with the null hypothesis). Such a pattern would dramatically increase our confidence that the null hypothesis is likely false. In addition to the gain in confidence for specific results, there are good practical reasons for replicating an existing experiment. For example, when beginning to work on a new topic it is generally a good idea to replicate the experiment on which one wants to build on. If one already has problems replicating what exists that shows that the topic is maybe not as simple as portrayed in the literature. Another excellent reason for performing a replication is if one simply does not believe an existing result. Remember, one of the key components of the scientific method is scepticism (at least according to Wikipedia). And if a results is difficult to believe, the reasonable sceptical position to take is to require more evidence. A replication is one way (if not the best way) to produce such evidence. Not believing existing experiments also does not imply that one questions the integrity of the researchers who did the experiment. There are many completely harmless reasons why a study might not replicate. For example, researchers might have just obtained a significant results by chance (which happens in 5% of cases, as discussed in the next chapters). Sadly, replicating existing experiments and publishing the results, is still not the norm in psychology and related disciplines. Quite to the contrary, the situation is so dire that many fields are currently considered to be in a replication crisis. For example, a large scale effort to replicate 100 studies in psychology (Open Science Collaboration 2015) showed that less than 50% could be replicated successfully. Similarly sobering results have since been observed across the social sciences (C. F. Camerer et al. 2018, 2016; Klein et al. 2018). Much has been written about this problem and this is not the right place to rehash all arguments. The best summary of the situation is the book by Chris Chambers (Chambers 2017). The important thing is to realise that science is a cumulative endeavour. Every new experiment builds on existing research. If the existing research has never been replicated, our confidence in this research has to be somewhat low. This questions the foundations of any new work that builds up on this non-replicated research. To move forward we researchers need to replicate work that is important for our research, value replications done by others (especially if it is of our work), and let findings that do not replicate fade into obscurity. 4.3 Conceptual Memory Data from Mueller and Oppenheimer (2014) As Urry et al. (2021) is a direct replication of Mueller and Oppenheimer (2014), the design is the same and uses the same materials (i.e., the same TED talks and same questions). Participants watched short lecture videos (projected onto a screen) and could take notes either on a laptop or with longhand format. 30 minutes after the lecture they were asked factual and conceptual questions about the lectures. Their answers were coded by the first author. As in the previous analysis, we will transform the answers to a memory index from 0 (= no memory) to 100 (= perfect memory). In line with the analysis of Urry et al. (2021) above, we are only interested in the conceptual memory here. The experiment by Urry et al. (2021) was a direct replication of Experiment 1 of Mueller and Oppenheimer (2014). Here we focus on Experiment 2 by Mueller and Oppenheimer (2014), which is also a direct replication of their Experiment 1 and only included an additional experimental manipulation which we will ignore here. The reason for focussing on their Experiment 2 instead of Experiment 1 is that the data of Experiment 2 come out a bit more interesting (feel free to rerun the analysis reported here for their Experiment 1 to see what I mean). However, to not provide an incomplete picture for the research question of whether the mode of taking note during lectures affects memory, we will consider the overall evidence (i.e., all 3 experiments of Mueller and Oppenheimer, the experiment of Urry et al., and further data) at the end of this chapter. Luckily for us, the data from Mueller and Oppenheimer (2014), including the data from their Experiment 2, is available online on the Open Science Framework (OSF). The OSF is one of the most visible developments resulting from the replication crisis. It is a free website that allows researchers to share their data and other materials associated with their research. Before the replication crisis and the OSF it was very rare to get access to the data underlying published studies. Nowadays many researchers depose their (anonymised) data for published studies on the OSF and include the links to the data in their papers. This allows other researcher, such as us, to reanalyse existing data and ensure that the reported results can be reproduced.30 4.3.1 Preparing the Data To get into the habit of downloading data from OSF and reanalysing them, this is what we are going to do now. The file we need is called Study 2 abbreviated data.csv and can be found at the following OSF link: https://osf.io/t43ua/ Please go ahead and download it now and put it in a folder so you can access it. I have already done so and copied it into folder data. We then use the tidyverse function read_csv(), which always returns a tibble (the tidyverse version of a data.frame), to read in the data, as object mo2014. Then we use the glimpse() function (also a tiydverse function) to get an overview of the data (it is very similar to str() but less verbose for tibbles). mo2014 &lt;- read_csv(&quot;data/Study 2 abbreviated data.csv&quot;) glimpse(mo2014) #&gt; Rows: 153 #&gt; Columns: 22 #&gt; $ participantid &lt;dbl&gt; 103, 122, 142, 152, 172, 183, 193, 213, 228, ~ #&gt; $ notetype &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, ~ #&gt; $ whichtalk &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~ #&gt; $ Wcount &lt;dbl&gt; 148, 273, 124, 149, 182, 104, 178, 170, 289, ~ #&gt; $ threeG &lt;dbl&gt; 0.08219178, 0.05535055, 0.18032787, 0.0544217~ #&gt; $ factualindex &lt;dbl&gt; 2.499, 2.833, 2.333, 4.499, 4.999, 1.833, 3.4~ #&gt; $ conceptualindex &lt;dbl&gt; 2.0, 1.5, 2.0, 2.0, 2.0, 1.5, 2.0, 1.5, 2.0, ~ #&gt; $ factualraw &lt;dbl&gt; 6, 7, 5, 11, 12, 4, 7, 11, 12, 11, 7, 6, 8, 5~ #&gt; $ conceptualraw &lt;dbl&gt; 4, 3, 4, 4, 4, 3, 4, 3, 4, 5, 3, 1, 4, 1, 2, ~ #&gt; $ perfectfactindexscore &lt;dbl&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ~ #&gt; $ perfectconceptindexscore &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ~ #&gt; $ perfectfactscore &lt;dbl&gt; 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 1~ #&gt; $ perfectconceptscore &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ~ #&gt; $ `filter_$` &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~ #&gt; $ ZFindexA &lt;dbl&gt; -0.17488644, 0.07568357, -0.29942123, 1.32553~ #&gt; $ ZCindexA &lt;dbl&gt; 0.52136737, 0.03238307, 0.52136737, 0.5213673~ #&gt; $ ZFrawA &lt;dbl&gt; 0.32848714, 0.68152314, -0.02454886, 2.093667~ #&gt; $ ZCrawA &lt;dbl&gt; 0.9199338, 0.2942131, 0.9199338, 0.9199338, 0~ #&gt; $ ZFindexW &lt;dbl&gt; -0.76333431, -0.47725123, -0.90551931, 0.9497~ #&gt; $ ZCindexW &lt;dbl&gt; 0.79471336, 0.06811829, 0.79471336, 0.7947133~ #&gt; $ ZFrawW &lt;dbl&gt; -0.6437995, -0.2476152, -1.0399839, 1.3371221~ #&gt; $ ZCrawW &lt;dbl&gt; 0.9647838, 0.1731663, 0.9647838, 0.9647838, 0~ We can see data from 153 participants on 22 columns. Many of the columns have names that are not immediately clear. This is not uncommon. An important task when getting any new data set is trying to figure out what the variables mean. One usually also has to do this for the data for the own experiments. For example, software for running experiments often collects more variables than needed for analysis. Consequently, the first analysis step is usually to figure out what is needed and what not. Before doing so however, we note that 153 participants is not the final number of participants reported by Mueller and Oppenheimer (2014). Instead, they removed two participants before the analysis resulting. Studying their OSF repository in detail (in particular the published SPPS script with output Output and Syntax - Study 2.doc) shows that participants with number 194 and 237 needs to be removed (the same information can be found in variable filter_$ in the current data set). This file also shows that the two relevant notetype conditions are 1 = longhand and 2 = laptop and we will remove notetype == 3 before analysis. Before moving on, we filter our data and remove these observations. For this we use filter() from the tidyverse in combination with the pipe operator %&gt;% (i.e., we pipe our tibble to filter() and only retain those rows that we want). Importantly, we overwrite mo2014 with the filtered tibble as we do not need the filtered out observations any more (to use them, we woul dhave to read the data in again). We then see how many participants remain using nrow() (which returns the number of rows in the data). mo2014 &lt;- mo2014 %&gt;% filter(notetype != 3, participantid != 194, participantid != 237) nrow(mo2014) #&gt; [1] 99 The reported 99 participants matches the 99 participants reported on OSF for the two conditions, laptop versus longhand. Sadly, the OSF does not include a codebook describing all variables for this particular data set (only for an earlier version of the data set with variables that only overlaps to some degree with the present one). However, from looking at data and the information on OSF a few things are clear: participantid is the participant identifier variable, notetype is the condition identifier coding the experimental condition, and whichtalk identifies the TED talk participants saw (the mapping of talks to numbers is also given in the SPPS output). In a first step, we can transform the relevant indicator variables, participant and experimental condition variable, into factors for further analysis. Transforming a variable into a factor guarantees that none of the analyses incorrectly treats one of the factors (i.e., categorical variables) as a numerical variable (e.g., taking the mean of the numbers in the participant identifier column is not a reasonable statistical operation). However, instead of overwriting the existing variables, we create new variable with the same name as in our analysis of Urry et al. (2021), pid and condition. We also assign human understandable labels instead of using 1 and 2 for the condition codes. This will make it easier to understand the pattern of results. To do so we use factor() inside mutate() from the tidyverse in combination with the pipe operator %&gt;%. mo2014 &lt;- mo2014 %&gt;% filter(notetype != 3) %&gt;% mutate( pid = factor(participantid), condition = factor(notetype, levels = c(2, 1), labels = c(&quot;laptop&quot;, &quot;longhand&quot;)) ) glimpse(mo2014) #&gt; Rows: 99 #&gt; Columns: 24 #&gt; $ participantid &lt;dbl&gt; 103, 122, 142, 152, 172, 183, 193, 213, 228, ~ #&gt; $ notetype &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, ~ #&gt; $ whichtalk &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~ #&gt; $ Wcount &lt;dbl&gt; 148, 273, 124, 149, 182, 104, 178, 170, 289, ~ #&gt; $ threeG &lt;dbl&gt; 0.08219178, 0.05535055, 0.18032787, 0.0544217~ #&gt; $ factualindex &lt;dbl&gt; 2.499, 2.833, 2.333, 4.499, 4.999, 1.833, 3.4~ #&gt; $ conceptualindex &lt;dbl&gt; 2.0, 1.5, 2.0, 2.0, 2.0, 1.5, 2.0, 1.5, 2.0, ~ #&gt; $ factualraw &lt;dbl&gt; 6.0, 7.0, 5.0, 11.0, 12.0, 4.0, 7.0, 11.0, 12~ #&gt; $ conceptualraw &lt;dbl&gt; 4, 3, 4, 4, 4, 3, 4, 3, 4, 5, 3, 1, 4, 1, 2, ~ #&gt; $ perfectfactindexscore &lt;dbl&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, ~ #&gt; $ perfectconceptindexscore &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ~ #&gt; $ perfectfactscore &lt;dbl&gt; 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 1~ #&gt; $ perfectconceptscore &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, ~ #&gt; $ `filter_$` &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~ #&gt; $ ZFindexA &lt;dbl&gt; -0.17488644, 0.07568357, -0.29942123, 1.32553~ #&gt; $ ZCindexA &lt;dbl&gt; 0.52136737, 0.03238307, 0.52136737, 0.5213673~ #&gt; $ ZFrawA &lt;dbl&gt; 0.32848714, 0.68152314, -0.02454886, 2.093667~ #&gt; $ ZCrawA &lt;dbl&gt; 0.9199338, 0.2942131, 0.9199338, 0.9199338, 0~ #&gt; $ ZFindexW &lt;dbl&gt; -0.76333431, -0.47725123, -0.90551931, 0.9497~ #&gt; $ ZCindexW &lt;dbl&gt; 0.79471336, 0.06811829, 0.79471336, 0.7947133~ #&gt; $ ZFrawW &lt;dbl&gt; -0.6437995, -0.2476152, -1.0399839, 1.3371221~ #&gt; $ ZCrawW &lt;dbl&gt; 0.96478381, 0.17316633, 0.96478381, 0.9647838~ #&gt; $ pid &lt;fct&gt; 103, 122, 142, 152, 172, 183, 193, 213, 228, ~ #&gt; $ condition &lt;fct&gt; longhand, longhand, longhand, longhand, longh~ Looking at the data again reveals that the newly created variables are added to the end of the tibble. The next step is to calculate our dependent variable, the memory scores from 0 to 100 as used in the analysis of Urry et al. (2021). We can see that for the two question types, factual and conceptual, there are multiple measures. Each has an index score and a raw score as well as perfect variants for both types of scores. perfect here presumably means the maximal possible value that could be obtained for this score for this observation (i.e., row). The data also contains a number of \\(z\\)-transformed variants of the scores (variables Z), but we will ignore them here (the original paper used the z-scores, but as these are more difficult to interpret and the results are qualitatively the same, we ignore them here). We focus on the index score which gives participant a maximal of 1 point per question (this information is given in the paper/on OSF). Let us take a look at the first six observations for the relevant variables. mo2014 %&gt;% select(pid, condition, factualindex, conceptualindex, perfectfactindexscore, perfectconceptindexscore) %&gt;% head() #&gt; # A tibble: 6 x 6 #&gt; pid condition factualindex conceptualindex perfectfactinde~ perfectconcepti~ #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 103 longhand 2.50 2 7 3 #&gt; 2 122 longhand 2.83 1.5 7 3 #&gt; 3 142 longhand 2.33 2 7 3 #&gt; 4 152 longhand 4.50 2 7 3 #&gt; 5 172 longhand 5.00 2 7 3 #&gt; 6 183 longhand 1.83 1.5 7 3 We can see that the number of questions per question type and talk differs (as indicated by the difference in perfect indexscore values across rows), but the total number of items appears to always be ten (perfectfactindexscore + perfectconceptindexscore = 10 in each row). This also aligns with the list of items found on OSF which show that there are ten questions per talk with the number of factual and conceptual questions differing across talks. From this information we could calculate our memory scores. However, before moving on it makes sense to run a quick sanity check to see that indeed the number of questions per row is ten. To do this, we create a new variable with the sum of the two perfect index scores, using mutate() (which adds a variable to the existing data), and then see whether this sum is always equal to 10, using summarise() (which in this case reduces the data to one row). If the number of question per observation/row sums to ten, this should return TRUE. mo2014 %&gt;% mutate(sum_p_index = perfectfactindexscore + perfectconceptindexscore) %&gt;% summarise(check = all(sum_p_index == 10)) #&gt; # A tibble: 1 x 1 #&gt; check #&gt; &lt;lgl&gt; #&gt; 1 TRUE Fortunately, the check passes so we feel that our assumption about the meaning of the variables are supported so we could go ahead and calculate our memory score. When preparing data for analysis, or running an analysis, it is important to regularly include such sanity checks in ones analysis. Any analysis involves assumptions about the underlying data  for example, what a variable means, which values a variable can possibly take on, which observations are included in the data. Based on this assumption we calculate other variables from our data and perform our analysis. However, humans are fallible and data analysis experience shows that the assumptions are sometimes (regularly) false. Sometimes one has misunderstood (or misremembers) the meaning of a variable, there might have been some data entry error, or the data still includes some observations that should have been excluded (e.g., test runs from the researchers instead of participants). For example, in a study by Lewandowsky, Gignac, and Oberauer (2013) the age of one participants was recorded as 32,757 years and this error was only uncovered after the publication of the manuscript. Luckily for them the error did not affect the conclusion drawn from the data, but they had to publish a correction (Lewandowsky, Gignac, and Oberauer 2015). Publishing a correction is nothing dramatic (I have a few paper with published corrections because of errors discovered only after publication), but of course we would prefer not having to do so. And if the errors affect the conclusion substantially, sometimes a correction is not enough and a paper has to be retracted. Regular sanity or assumptions checks in ones analysis are on way to minimise the chance of errors in the final analysis. Based on the positive outcome of the sanity check we are now convinced we have understood the variables in the data and can now calculate our memory score from 0 to 100. For this, we divide each index score by the perfectindexscore and then multiply the results by 100. To simplify the coming analysis, we create a new tibble, mo2014a, that only retains the variables we really need for our analysis using select(). We then take another look at the first six rows of the data using head(). This shows that the data is now ready for our reanalysis. mo2014a &lt;- mo2014 %&gt;% mutate( factual = factualindex / perfectfactindexscore * 100, conceptual = conceptualindex / perfectconceptindexscore * 100 ) %&gt;% select(pid, condition, factual, conceptual) head(mo2014a) #&gt; # A tibble: 6 x 4 #&gt; pid condition factual conceptual #&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 103 longhand 35.7 66.7 #&gt; 2 122 longhand 40.5 50 #&gt; 3 142 longhand 33.3 66.7 #&gt; 4 152 longhand 64.3 66.7 #&gt; 5 172 longhand 71.4 66.7 #&gt; 6 183 longhand 26.2 50 4.3.2 Descriptive Statistics Before performing an inferential statistical analysis of the data, we obtain some descriptive statistics. This will provide us with an overview over the data. In addition, the descriptive analysis is another way to check our data and minimise the chances of errors or problems. As a first thing, we want to calculate the number of participants per condition. For this, we again use some tidyverse functions and will explain the steps in more detail. We generally start our tidyverse analyses with the data, here our tibble mo2014a, followed by the pipe %&gt;%. The pipe pipes the tibble to the next function. When obtaining descriptives statistics we often want to get them conditional on a factor/categorical variable in our data. For example, now we want to calculate the number of observations per condition. This can be done by piping the tibble to the group_by() function and condition on the variable of interest, condition. The results of this is a grouped tibble which ensures that all following operations on this tibble are performed grouped (i.e., conditioned on) this grouping variable. We can now pipe this grouped tibble to the count() function to get the number of observations per note taking condition. mo2014a %&gt;% group_by(condition) %&gt;% count() #&gt; # A tibble: 2 x 2 #&gt; # Groups: condition [2] #&gt; condition n #&gt; &lt;fct&gt; &lt;int&gt; #&gt; 1 laptop 51 #&gt; 2 longhand 48 As another sanity check, we can compare this number to the values reported on the OSF for this data (the \\(N\\) by condition is not reported in the original paper). As the numbers match, this further increases our confidence in our data preparation. As the next descriptive statistic, we calculate the condition means for our DV of interest, conceptual memory scores. We also calculate the standard deviation to get an idea of the spread of the data. We again use piping and the tidyverse to get the result. But this time the final function in our pipe is summarise() which allows to calculate summary statistics. mo2014a %&gt;% group_by(condition) %&gt;% summarise( mean = mean(conceptual), sd = sd(conceptual) ) #&gt; # A tibble: 2 x 3 #&gt; condition mean sd #&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 laptop 35 23.5 #&gt; 2 longhand 46.5 28.9 This shows that the conceptual memory is more than 10 points larger in the longhand compared to the laptop condition. We also see a difference in around 5 points in the SD. As a side note, we could have added another calculation into the summarise() call. For example, n = n() would have also calculated the number of participants per condition, as the previous code did. One important part of a descriptive analysis should always be a plot of the data. A plot of all data points is usually the best way to see if there is something wrong with the data. Above, we have used afex_plot() after having estimated a model with aov_car(), but we can also invoke ggplot2 directly. For this, we also pipe the data to ggplot() and then build the figure layer by layer. The important part is the mapping of variables in the data to aesthetics in the aes() function. We call this directly in the ggplot() call and mimic the other figures we have seen so far, mapping condition on the \\(x\\)-axis and the DV, conceptual memory, to the \\(y\\)-axis. As Figure 3.1, we begin with a violin plot (geom_violin()) with different quantiles. The violin plot shows the shape of the distribution. We combine this with the individual data points, whcih we show using geom_beeswarm() from the ggbeeswarm package (here we call the function without loading the package beforehand by using package::function()). We then add the mean (with standard error, which will be explained later) using stat_summary() in red. In this plot we see that the data already spans the full range in the \\(y\\)-axis, so we do not need to use coord_cartesian(ylim = c(0, 100)). mo2014a %&gt;% ggplot(aes(x = condition, y = conceptual)) + geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) + ggbeeswarm::geom_beeswarm() + stat_summary(colour = &quot;red&quot;) #&gt; No summary function supplied, defaulting to `mean_se()` Figure 4.2: Conceptual memory scores from Mueller and Oppenheimer (2014). This plot combines individual data points in black with means in red. Before looking at the figure, we see that we got a status message in the console, No summary function supplied, defaulting to `mean_se()`. This message is always shown when using stat_summary() without additional argument and can be safely ignored (i.e., it just indicates that the red point shows the mean and the red error bars show the standard error). The plot itself does not show anything unusual. The plot just reinforces the previous descriptive results: The mean memory score, but also the three displayed quantiles, are larger in the longhand than in the laptop condition. Taken together, the descriptive analysis suggests that there is nothing preventing us from running the inferential analysis. 4.3.3 Inferential Analysis The inferential analysis of the conceptual scores uses exactly the same call as our previous analysis, only with a new data set, mo2014a. mc_mo &lt;- aov_car(conceptual ~ condition + Error(pid), mo2014a) #&gt; Contrasts set to contr.sum for the following variables: condition mc_mo #&gt; Anova Table (Type 3 tests) #&gt; #&gt; Response: conceptual #&gt; Effect df MSE F ges p.value #&gt; 1 condition 1, 97 688.89 4.77 * .047 .031 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 Looking at the ANOVA table of the results shows that the \\(p\\)-value is smaller than .05; the analysis reveals a significant effect of condition. This indicates that this data provides evidence against the null hypothesis of no difference between the note taking conditions. Consequently, we would be justified in saying that the data provides evidence for a difference. To make it easy to detect a significant result, afex, like most statistical software tools, indicates a significant effect with \\(p &lt; .05\\) with one * next to the \\(F\\)-value (in case of \\(p &lt; .01\\) the indication is **, in case of \\(p &lt; .001\\) it is ***, and in case the effect is not significant, but \\(p &lt; .1\\), it is +). afex_plot(mc_mo, &quot;condition&quot;) Figure 4.3: afex_plot() figure for the conceptual memory scores from Mueller and Oppenheimer (2014, Experiment 2) that show a significant difference between the two note taking conditions. 4.4 Summary One reality of research is that a significant results is generally what researchers are looking for. If a results is significant we are happy, our experiment has worked and we can publish it. If it is not significant, we generally have problems publishing our results. References "],["references.html", "References", " References "]]
