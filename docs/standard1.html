<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 The Standard Approach for One Independent Variable | Introduction to Statistics for Experimental Psychology with R</title>
  <meta name="description" content="Statistics for Experimental Psychology with R using the afex package." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 The Standard Approach for One Independent Variable | Introduction to Statistics for Experimental Psychology with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Statistics for Experimental Psychology with R using the afex package." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 The Standard Approach for One Independent Variable | Introduction to Statistics for Experimental Psychology with R" />
  
  <meta name="twitter:description" content="Statistics for Experimental Psychology with R using the afex package." />
  

<meta name="author" content="Henrik Singmann" />


<meta name="date" content="2021-08-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="role-of-statistics-in-the-research-process.html"/>
<link rel="next" href="case-study-1-more-results-from-note-taking-studies.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistics for Experimental Psychology</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Overview</a></li>
<li class="chapter" data-level="2" data-path="role-of-statistics-in-the-research-process.html"><a href="role-of-statistics-in-the-research-process.html"><i class="fa fa-check"></i><b>2</b> Role of Statistics in the Research Process</a>
<ul>
<li class="chapter" data-level="2.1" data-path="role-of-statistics-in-the-research-process.html"><a href="role-of-statistics-in-the-research-process.html#the-research-process"><i class="fa fa-check"></i><b>2.1</b> The Research Process</a></li>
<li class="chapter" data-level="2.2" data-path="role-of-statistics-in-the-research-process.html"><a href="role-of-statistics-in-the-research-process.html#example-i-testing-a-new-therapy"><i class="fa fa-check"></i><b>2.2</b> Example I: Testing a New Therapy</a></li>
<li class="chapter" data-level="2.3" data-path="role-of-statistics-in-the-research-process.html"><a href="role-of-statistics-in-the-research-process.html#example-ii-the-psychology-of-loss-aversion"><i class="fa fa-check"></i><b>2.3</b> Example II: The Psychology of Loss Aversion</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="role-of-statistics-in-the-research-process.html"><a href="role-of-statistics-in-the-research-process.html#evidence-for-loss-aversion-the-reflection-effect"><i class="fa fa-check"></i><b>2.3.1</b> Evidence for Loss Aversion: The Reflection Effect</a></li>
<li class="chapter" data-level="2.3.2" data-path="role-of-statistics-in-the-research-process.html"><a href="role-of-statistics-in-the-research-process.html#alternative-explanation-loss-aversion-or-loss-seeking"><i class="fa fa-check"></i><b>2.3.2</b> Alternative Explanation: Loss Aversion or Loss Seeking?</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="role-of-statistics-in-the-research-process.html"><a href="role-of-statistics-in-the-research-process.html#epistemic-gaps-the-difference-between-what-we-want-to-know-and-what-we-can-know"><i class="fa fa-check"></i><b>2.4</b> Epistemic Gaps: The Difference Between What we Want to Know and What we can Know</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="role-of-statistics-in-the-research-process.html"><a href="role-of-statistics-in-the-research-process.html#epistemic-gap-1-underdetermination-of-theory-by-data"><i class="fa fa-check"></i><b>2.4.1</b> Epistemic Gap 1: Underdetermination of Theory by Data</a></li>
<li class="chapter" data-level="2.4.2" data-path="role-of-statistics-in-the-research-process.html"><a href="role-of-statistics-in-the-research-process.html#epistemic-gap-2-signal-and-noise"><i class="fa fa-check"></i><b>2.4.2</b> Epistemic Gap 2: Signal and Noise</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="role-of-statistics-in-the-research-process.html"><a href="role-of-statistics-in-the-research-process.html#summary"><i class="fa fa-check"></i><b>2.5</b> Summary</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="role-of-statistics-in-the-research-process.html"><a href="role-of-statistics-in-the-research-process.html#what-shouldnt-and-what-should-we-do"><i class="fa fa-check"></i><b>2.5.1</b> What Shouldn’t and What Should we do?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="standard1.html"><a href="standard1.html"><i class="fa fa-check"></i><b>3</b> The Standard Approach for One Independent Variable</a>
<ul>
<li class="chapter" data-level="3.1" data-path="standard1.html"><a href="standard1.html#ex:urry"><i class="fa fa-check"></i><b>3.1</b> Example Data: Note Taking Experiment</a></li>
<li class="chapter" data-level="3.2" data-path="standard1.html"><a href="standard1.html#the-logic-of-inferential-statistics"><i class="fa fa-check"></i><b>3.2</b> The Logic of Inferential Statistics</a></li>
<li class="chapter" data-level="3.3" data-path="standard1.html"><a href="standard1.html#the-basic-statistical-model"><i class="fa fa-check"></i><b>3.3</b> The Basic Statistical Model</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="standard1.html"><a href="standard1.html#model-predictions"><i class="fa fa-check"></i><b>3.3.1</b> Model Predictions</a></li>
<li class="chapter" data-level="3.3.2" data-path="standard1.html"><a href="standard1.html#statistical-model-for-the-example-data"><i class="fa fa-check"></i><b>3.3.2</b> Statistical Model for the Example Data</a></li>
<li class="chapter" data-level="3.3.3" data-path="standard1.html"><a href="standard1.html#understanding-the-statistical-model"><i class="fa fa-check"></i><b>3.3.3</b> Understanding the Statistical Model</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="standard1.html"><a href="standard1.html#estimating-the-statistical-model-in-r"><i class="fa fa-check"></i><b>3.4</b> Estimating the Statistical Model in R</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="standard1.html"><a href="standard1.html#package-and-data-setup"><i class="fa fa-check"></i><b>3.4.1</b> Package and Data Setup</a></li>
<li class="chapter" data-level="3.4.2" data-path="standard1.html"><a href="standard1.html#estimating-the-statistical-model"><i class="fa fa-check"></i><b>3.4.2</b> Estimating the Statistical Model</a></li>
<li class="chapter" data-level="3.4.3" data-path="standard1.html"><a href="standard1.html#interpreting-the-results"><i class="fa fa-check"></i><b>3.4.3</b> Interpreting the Results</a></li>
<li class="chapter" data-level="3.4.4" data-path="standard1.html"><a href="standard1.html#data-visualisation"><i class="fa fa-check"></i><b>3.4.4</b> Data Visualisation</a></li>
<li class="chapter" data-level="3.4.5" data-path="standard1.html"><a href="standard1.html#follow-up-analysis"><i class="fa fa-check"></i><b>3.4.5</b> Follow-Up Analysis</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="standard1.html"><a href="standard1.html#summary-1"><i class="fa fa-check"></i><b>3.5</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="case-study-1-more-results-from-note-taking-studies.html"><a href="case-study-1-more-results-from-note-taking-studies.html"><i class="fa fa-check"></i><b>4</b> Case Study 1: More Results from Note Taking Studies</a>
<ul>
<li class="chapter" data-level="4.1" data-path="case-study-1-more-results-from-note-taking-studies.html"><a href="case-study-1-more-results-from-note-taking-studies.html#conceptual-memory-data-from-urry-et-al.-2021"><i class="fa fa-check"></i><b>4.1</b> Conceptual Memory Data from Urry et al. (2021)</a></li>
<li class="chapter" data-level="4.2" data-path="case-study-1-more-results-from-note-taking-studies.html"><a href="case-study-1-more-results-from-note-taking-studies.html#why-are-experiments-replicated"><i class="fa fa-check"></i><b>4.2</b> Why are Experiments Replicated?</a></li>
<li class="chapter" data-level="4.3" data-path="case-study-1-more-results-from-note-taking-studies.html"><a href="case-study-1-more-results-from-note-taking-studies.html#conceptual-memory-data-from-mueller-and-oppenheimer-2014"><i class="fa fa-check"></i><b>4.3</b> Conceptual Memory Data from Mueller and Oppenheimer (2014)</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="case-study-1-more-results-from-note-taking-studies.html"><a href="case-study-1-more-results-from-note-taking-studies.html#preparing-the-data"><i class="fa fa-check"></i><b>4.3.1</b> Preparing the Data</a></li>
<li class="chapter" data-level="4.3.2" data-path="case-study-1-more-results-from-note-taking-studies.html"><a href="case-study-1-more-results-from-note-taking-studies.html#descriptive-statistics"><i class="fa fa-check"></i><b>4.3.2</b> Descriptive Statistics</a></li>
<li class="chapter" data-level="4.3.3" data-path="case-study-1-more-results-from-note-taking-studies.html"><a href="case-study-1-more-results-from-note-taking-studies.html#inferential-analysis"><i class="fa fa-check"></i><b>4.3.3</b> Inferential Analysis</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="case-study-1-more-results-from-note-taking-studies.html"><a href="case-study-1-more-results-from-note-taking-studies.html#summary-2"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Statistics for Experimental Psychology with <code>R</code></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="standard1" class="section level1" number="3">
<h1><span class="header-section-number">Chapter 3</span> The Standard Approach for One Independent Variable</h1>
<p>In this chapter we are introducing the standard statistical approach for analysing experimental data with one independent variable (i.e., one factor). The simple case for this is a study comparing two experimental conditions on one dependent variable. We will exemplify the standard approach for this design using a recent and straightforward experiment.</p>
<div id="ex:urry" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Example Data: Note Taking Experiment</h2>
<p>Heather Urry and 87 of her undergraduate and graduate students <span class="citation">(<a href="#ref-urry2021" role="doc-biblioref">Urry et al. 2021</a>)</span> (yes, all 87 students are co-authors!) compared the effectiveness of taking notes on a laptop versus longhand (i.e., pen and paper) for learning from lectures. 142 participants (which differed from the 88 authors) first viewed one of several 15 minutes lectures (TED talks) during which they were asked to take notes either on a laptop or with pen and paper. As this was a proper experiment, participants were randomly assigned to either the laptop (<span class="math inline">\(N = 68\)</span>) or longhand condition (<span class="math inline">\(N = 74\)</span>). After a 30 minutes delay, participants were quizzed on the content of the lecture. The answers from each participant were then independently rated from several raters (which agreed very strongly with each other) using a standardised scoring key resulting in one memory score per participant representing the percentage of information remembered ranging from 0 (= no memory) to 100 (= perfect memory).<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a> Figure <a href="standard1.html#fig:laptop-dist">3.1</a> below shows the memory scores across both note taking conditions.</p>
<div class="figure" style="text-align: center"><span id="fig:laptop-dist"></span>
<img src="stats_for_experiments_files/figure-html/laptop-dist-1.png" alt="Distribution of memory scores from Urry et al. (2021) across the two note taking conditions." width="70%" />
<p class="caption">
Figure 3.1: Distribution of memory scores from Urry et al. (2021) across the two note taking conditions.
</p>
</div>
<p>In Figure <a href="standard1.html#fig:laptop-dist">3.1</a>, each black point shows the memory score of one participant so the full distribution of the data is visible. The shape of the distribution is also shown via a violin plot (i.e., the black outline around the points) to which we have added three lines representing three summary statistics of the data. From top to bottom these lines are the 75% quantile, the 50% quantile (i.e., the median), and the 25% quantile. The red points show the mean and the associated error bars show the standard error of the mean<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a>. We see that the two means are quite similar, although the mean in the laptop condition is slightly larger, by 2.0 points (mean laptop = 68.2, mean longhand = 66.2).</p>
</div>
<div id="the-logic-of-inferential-statistics" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> The Logic of Inferential Statistics</h2>
<p>The previous paragraph provide us with <em>descriptive statistics</em> describing the results in the experiment by <span class="citation"><a href="#ref-urry2021" role="doc-biblioref">Urry et al.</a> (<a href="#ref-urry2021" role="doc-biblioref">2021</a>)</span>: There is a memory difference of 2.0 on the scale from 0 to 100 between the laptop and the longhand condition for the sample of 142 participants. However, as researchers we are usually not primarily interested what happens in our sample. What we would like to know if our results generalises to the <em>population</em> from which this sample is drawn. In this case, we would like to know whether there is a memory difference between note taking with a laptop or in longhand format for students (as this is roughly the population the sample is drawn from).</p>
<p>Going beyond the the present sample is the goal of <em>inferential statistics</em>. There are different inferential statistical approaches, and we are focussing on the most popular one, <em>null hypothesis significance testing</em> (NHST). We will describe NHST more thoroughly in the next chapter, but will already introduce its main ideas here. For NHST, the question of generalising from sample to population is about two different possible true states of the world: There either is no difference in conditions means in the population (i.e., there only is a mean difference in our sample due to chance) or there is a difference in the condition means in the population. To decide between these possibilities, we set up a <em>statistical model</em> for the data. This statistical model allows us to assess if there is no difference in the population – we call this possible state of the world the <em>null hypothesis</em>. More specifically, the statistical model allows us to test how compatible the data is with the null hypothesis of no difference. The test of the null hypothesis proceeds as follows: (1) We assume that the state of the world in which there is no difference in the population means is true. (2) Based on this assumption we calculate how likely it is to observe a difference as large as the one we have observed in our sample. (3) If the probability of observing a difference as large as the one we have is very small, we take this as evidence that the null hypothesis of no difference is not true – we reject the null hypothesis. (4) We act as if there were a difference in the population. In this case (i.e., we reject the null hypothesis and act as if there is a difference), we say our experimental manipulation <em>has an effect</em>.</p>
<p>As is clear from this description, the logic underlying inferential statistics using NHST is not trivial. To make it clearer, let us apply the logic to the example data. We want to know whether the observed mean memory difference between note taking with a laptop and note taking in long hand format in our sample generalises to the population of students that take note in either of these formats. To do so, we set up a statistical model for our data. This model allows us to test how compatible our data is with the null hypothesis that there is no mean memory difference in the population. Specifically, the model allows us to calculate the probability of obtaining a memory difference as large as the one we have observed assuming the there is no mean memory difference in the population. If our data is incompatible with the null hypothesis (i.e., it is unlikely to obtain a memory difference as large as the one we have observed if the null hypothesis were true), we reject the null hypothesis of no mean memory difference in the population. Because we reject the null hypothesis we then act as if there was a mean memory difference in the population. In other words, we then act as if the type of note taking had an effect on memory after lectures in the population.</p>
<p>Even though the logic of NHST is not necessarily intuitive, it is clearly helpful for researchers. After running an experiment we really would like to know if the difference observed in our experiment (i.e., in our sample) is meaningful in the sense that it generalises to the population from which we have sampled our participants. And in almost every actual experiment there is some mean difference between the condition (i.e., it is extreme unlikely that both conditions have exactly the same mean). Thus, we pretty much always face this question. NHST allows us to test whether the observed difference is compatible with a world in which there is no difference. If this is unlikely, we decide (i.e., act as if) there were a difference.</p>
<p>What we can see from spelling out the logic in detail is that there are quite a few inferential steps we have to make to get to what we want. We design experiments with the goal in mind to find a difference between the different experimental conditions. However, we then do not test this directly. Instead, we test the compatibility of the data with the converse of what we are actually interested in – the null hypothesis of no effect. If this test “fails” (i.e., shows that the data is likely incompatible with the null hypothesis) we then make two inferential steps. First we reject the null hypothesis and then we act as if there were a difference. Both of these inferential steps are not necessitated logically. What this means is that inferences based on NHST alone are never extremely strong.</p>
<p>NHST is the de facto standard procedure for inferential statistics across empirical sciences (i.e., not only in psychology and related disciplines). Understanding the logic of NHST will enable you to understand the majority of empirical papers and will also allow you to apply inferential statistics in your own research. Nevertheless, there exist a long list of popular criticisms of NHST <span class="citation">(e.g., <a href="#ref-rozeboom1960" role="doc-biblioref">Rozeboom 1960</a>; <a href="#ref-meehl1978" role="doc-biblioref">Meehl 1978</a>; <a href="#ref-cohen1994" role="doc-biblioref">Cohen 1994</a>; <a href="#ref-nickerson2000" role="doc-biblioref">Nickerson 2000</a>; <a href="#ref-wagenmakers2007" role="doc-biblioref">Wagenmakers 2007</a>)</span>. We will discuss these criticisms in more detail in later chapters, but for now it is important to realise that NHST does not allow to test, or prove, whether there is a mean difference in the population. The only thing NHST calculates is a probability of how compatible the data is with the null hypothesis. If this probability is low that does not necessarily mean that there is a difference. Likewise, if this probability is high that does not necessarily mean there is no difference. All inferences we draw based on NHST results are probabilistic in itself (i.e., can be false). So the most important rule when interpreting the results from NHST is to be humble. NHST never “proves” or “confirms” anything. Instead NHST results “suggest” or “indicate” certain interpretations. If we do not over-interpret results, but stay instead stay humble in our interpretations, we are unlikely to fall prey to the common (and often justifiable) criticisms of the NHST framework.</p>
</div>
<div id="the-basic-statistical-model" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> The Basic Statistical Model</h2>
<p>To apply inferential statistics in the NHST framework to our data, we begin by setting up a <em>statistical model</em> to the data. A statistical model attempts to explain (or predict) the observed values of the dependent variable (DV) from the independent variable (IV).<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a> In the experimental context this means predicting our observed outcome, the DV, from the experimental manipulation, the IV.</p>
<p>The basic statistical model partitions the observed DV into three parts that: the overall mean, which for reasons that will become clear later is called the <em>intercept</em>, the effect of the IV, and the part of the data that cannot be explained by the model, the <em>residuals</em>. When summing these three parts together, they result in the observed value. In mathematical form we can express this as</p>
<span class="math display" id="eq:statmodel">\[\begin{equation}
\text{DV} = \underbrace{\text{intercept}}_{\text{overall mean}} + \text{IV-effect} + \text{residual}.
\tag{3.1}
\end{equation}\]</span>
<p>(For those not used to reading mathematical expressions, the point at the end of the equation is simply a full stop that ends the sentence and has no mathematical meaning.) As someone without a mathematics background myself, I know that equations in a text are often more intimidating than immediately useful. Consequently, before moving on it makes sense to go through this equation in more detail. Furthermore, all statistical analyses discussed in this book are applications of Equation <a href="standard1.html#eq:statmodel">(3.1)</a>. This equation forms the foundation for the statistical analysis of experimental data and thus understanding it will unlock all analyses discussed in this book. Consequently, it makes sense to spend more time on it.</p>
<p>Let us consider the the variables in Equation <a href="standard1.html#eq:statmodel">(3.1)</a> in more detail. When doing so, we also consider how many different possible values each variable can take on.<a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a> The following Figure, a variant of Figure <a href="standard1.html#fig:laptop-dist">3.1</a>, shows the elements graphically and we explain them in the text just below.</p>
<div class="figure" style="text-align: center"><span id="fig:laptop-model"></span>
<img src="stats_for_experiments_files/figure-html/laptop-model-1.png" alt="Data from Urry et al. (2021) showing the overall mean (intercept, blue dotted line), the condition specific effects (difference between dashed red lines for the condition means and the blue line), and the residuals (grey lines from condition means to data points)." width="70%" />
<p class="caption">
Figure 3.2: Data from Urry et al. (2021) showing the overall mean (intercept, blue dotted line), the condition specific effects (difference between dashed red lines for the condition means and the blue line), and the residuals (grey lines from condition means to data points).
</p>
</div>
<ul>
<li><p><span class="math inline">\(\text{DV}\)</span>: The dependent variable, DV, are the observed values, one for each observation/participant. For the example data this are all the 142 black data points shown in Figure <a href="standard1.html#fig:laptop-model">3.2</a>. Thus, our statistical model tries to explain the individually observed values.</p></li>
<li><p><span class="math inline">\(\text{intercept}\)</span>: The intercept represents the overall mean. Consequently, we only have one intercept (i.e., the intercept is the same for each observation). In experimental designs we define this as the mean of all condition means. For the example data the intercept is (68.2 + 66.2) / 2 = 67.2 and is shown as a blue dotted line in Figure <a href="standard1.html#fig:laptop-model">3.2</a>.</p></li>
<li><p><span class="math inline">\(\text{IV-effect}\)</span>: The IV-effect represents the effect of our independent variable which we define as the difference between the condition means and the intercept (i.e., the deviation of the condition means from the intercept). Thus, we always have as many different IV-effects as we have conditions. For the example data with only two conditions, we only have two different IV-effects, both of which with the same magnitude and only differ in sign, 1.0 for the laptop condition and -1.0 for the longhand condition. If we add these values to the intercept, we get the condition means. As we will discuss further below, this is the most relevant part for answering the statistical question of interest. In Figure <a href="standard1.html#fig:laptop-model">3.2</a>, the red dashed line (and the red points) show the condition means, thus the condition effects are the differences between the blue line and the red lines.</p></li>
<li><p><span class="math inline">\(\text{residual}\)</span>: The residuals are the idiosyncratic aspects of the data that are left unexplained by the statistical model. As the model only predicts the condition means (i.e., intercepts plus independent variable), these are the deviations of the individual observations from the condition means. Thus, as for the DV, we have as many residuals as we have values of the DV. In Figure <a href="standard1.html#fig:laptop-model">3.2</a>, the residuals are shown as grey lines from the condition means to each data point. This is all the information (or variability in the data) our model cannot explain.</p></li>
</ul>
<div id="model-predictions" class="section level3" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Model Predictions</h3>
<p>A simplification of Equation <a href="standard1.html#eq:statmodel">(3.1)</a> that makes it clearer what the statistical model predicts is obtained if we ignore the residuals for a moment. As a reminder, the residuals are the part of the data that remains unexplained. In other words, these represent all the idiosyncratic parts of the data independent of our manipulation (e.g., some participants have better memory than others independent of how they took notes). What remains from our statistical model if we ignore all idiosyncratic aspects are only the predictions based on our IV. In the case of experimental data, the IV is the experimental condition. Thus, what a statistical model actually predicts is the means of the experimental conditions. We can again formalise this as</p>
<span class="math display" id="eq:predmodel">\[\begin{equation}
\hat{\text{DV}} = \text{intercept} + \text{IV-effect}.
\tag{3.2}
\end{equation}\]</span>
<p>Here, the hat symbol (<span class="math inline">\(\hat{}\)</span>) means predicted value. Thus in contrast to the actual DV above, we only have the predicted DV in this equation.</p>
<p>When performing statistical analyses it sometimes help to remind oneself that all a standard statistical model predicts are the condition means. We generally do not make predictions about individual participants or consider other factors that are not part of the model. We only predict, and are interested in, the condition means.</p>
</div>
<div id="statistical-model-for-the-example-data" class="section level3" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Statistical Model for the Example Data</h3>
<p>Let us take a look at the first six participants and their values for all the variables in the basic statistical model to get a better understanding of Equation <a href="standard1.html#eq:statmodel">(3.1)</a>.</p>
<table>
<thead>
<tr class="header">
<th align="left">pid</th>
<th align="left">condition</th>
<th align="right">overall</th>
<th align="left">—–</th>
<th align="right">intercept</th>
<th align="right">iv_effect</th>
<th align="right">prediction</th>
<th align="right">residual</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">laptop</td>
<td align="right">65.8</td>
<td align="left"></td>
<td align="right">67.2</td>
<td align="right">1</td>
<td align="right">68.2</td>
<td align="right">-2.4</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">longhand</td>
<td align="right">75.8</td>
<td align="left"></td>
<td align="right">67.2</td>
<td align="right">-1</td>
<td align="right">66.2</td>
<td align="right">9.6</td>
</tr>
<tr class="odd">
<td align="left">4</td>
<td align="left">longhand</td>
<td align="right">50.0</td>
<td align="left"></td>
<td align="right">67.2</td>
<td align="right">-1</td>
<td align="right">66.2</td>
<td align="right">-16.2</td>
</tr>
<tr class="even">
<td align="left">5</td>
<td align="left">laptop</td>
<td align="right">89.0</td>
<td align="left"></td>
<td align="right">67.2</td>
<td align="right">1</td>
<td align="right">68.2</td>
<td align="right">20.8</td>
</tr>
<tr class="odd">
<td align="left">6</td>
<td align="left">longhand</td>
<td align="right">75.6</td>
<td align="left"></td>
<td align="right">67.2</td>
<td align="right">-1</td>
<td align="right">66.2</td>
<td align="right">9.4</td>
</tr>
<tr class="even">
<td align="left">8</td>
<td align="left">longhand</td>
<td align="right">83.3</td>
<td align="left"></td>
<td align="right">67.2</td>
<td align="right">-1</td>
<td align="right">66.2</td>
<td align="right">17.1</td>
</tr>
</tbody>
</table>
<p>The first three columns show the data. <code>pid</code> is the participant identifier (id) column. As it is often the case for real data, some ids are missing (here 3 and 7) for various reasons (e.g., potential participants were interested in the study and received an id, but then did not finish or start the experiment) so the first 6 rows already go up to <code>pid</code> = 8. <code>condition</code> tells us in which note taking condition a participant was and <code>overall</code> is their memory score on the scale from 0 to 100 which serves as the DV in the statistical model (i.e., the left-hand side in Equation <a href="standard1.html#eq:statmodel">(3.1)</a>).</p>
<p>The four right most columns contain the values of the variables on the right-hand side of Equation <a href="standard1.html#eq:statmodel">(3.1)</a>, the <code>intercept</code>, the <code>iv_effect</code>, and the <code>residual</code>. In addition, the <code>prediction</code> column shows the left-hand side of Equation <a href="standard1.html#eq:predmodel">(3.2)</a>. As described above, every observation (i.e., row) has a idiosyncratic DV and residual. We also see that all values share one intercept, and the IV-effect is condition specific. As a consequence, the <code>prediction</code> column (which is the sum of intercept and iv-effect) also has two values, one for each condition. Finally, we can see that the sum of the three values on the right-hand side of Equation <a href="standard1.html#eq:statmodel">(3.1)</a> equals the observed value of the DV. For example, consider <code>pid</code> = 4. If we enter the values into Equation <a href="standard1.html#eq:statmodel">(3.1)</a> we have</p>
<p><span class="math display">\[
50.0 = 67.2 + (-1) + (-16.2).
\]</span></p>
<p>From this example data we can also understand better what the residuals mean, they are the difference between the observed value and the predicted value, <span class="math inline">\(\text{residual} = \text{DV} - \hat{\text{DV}}\)</span>. Consider again <code>pid</code> = 4. Here we have</p>
<p><span class="math display">\[
-16.2 = 50- 66.2.
\]</span></p>
<p>We can also see how the residual captures the idiosyncratic aspects of our data that cannot be explained by the condition means. For example, some participants – such as <code>pid</code> = 5 and <code>pid</code> = 8 – have large positive residuals indicating that they have good memory independent of their note taking condition. Likewise, <code>pid</code> = 4 has a large negative residual indicating comparatively worse memory (again independent of the note taking condition).</p>
</div>
<div id="understanding-the-statistical-model" class="section level3" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Understanding the Statistical Model</h3>
<p>Now that we have described the parts of the statistical model we are almost ready to fit the model and interpret the output. Before doing so it makes sense to look at all the parts again individually and try to understand why we set up the statistical model in the way we do. Remember, our goal is to evaluate whether there is an effect of the experimental manipulation (i.e., a difference between the two note taking conditions) in the population from which the data is sampled. To do so, we set up a model that partitions the observed data into three parts, the intercept representing the overall mean, the condition specific effect (IV-effect) representing the difference of the condition means from the intercept, and the residuals representing the idiosyncratic part not explained by the model. The reason for doing so is that it allows us to zoom in on what matters for our statistical question, the condition specific effect. To answer the question if there is a difference between the conditions in the population, we can now focus on this part of the model. The overall level of performance captured in the intercept and the residuals can (for now) be ignored for this question. Consequently, the statistical test reported below is a statistical test of the condition effect. Thus, the reason for setting up the statistical model in this way is to make it easy to get an answer to the question that interests us: Is there an effect of the note taking manipulation/conditions on memory? To answer this question we only need to consider the condition effect.</p>
</div>
</div>
<div id="estimating-the-statistical-model-in-r" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Estimating the Statistical Model in R</h2>
<div id="package-and-data-setup" class="section level3" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Package and Data Setup</h3>
<p>For the statistical analyses reported in this book we generally use the <a href="https://cran.r-project.org/package=afex"><code>afex</code></a> package <span class="citation">(<a href="#ref-R-afex" role="doc-biblioref">Singmann et al. 2021</a>)</span> in combination with the <a href="https://cran.r-project.org/package=emmeans"><code>emmeans</code></a> package <span class="citation">(<a href="#ref-lenth2021" role="doc-biblioref">Lenth 2021</a>)</span>. <code>afex</code> stands for “analysis of factorial experiments” and simplifies many of the things we want to do (full disclaimer: I am the main developer of <code>afex</code>). Most analyses can also be performed with different functions, but it is often easiest to use <code>afex</code> functions as they are developed particularly for cognitive and behavioural researchers working with experimental data. More specifically, <code>afex</code> functions provide the expected results for experimental data sets out-of-the-box without the need to change any settings (which is not true for the corresponding non-<code>afex</code> functions). <code>emmeans</code> stands for “estimated marginal means” and is the package we use once a statistical model is estimated to further investigate the results. <code>afex</code> and <code>emmeans</code> are fully integrated with each other which allows to test practically any hypotheses of interest with a combination of these two packages in a straight forward manner. We already introduce the interplay of these two packages here, and the next chapters will showcase the full power of this combination.</p>
<p>We also regular use functions from the <a href="https://cran.r-project.org/package=tidyverse"><code>tidyverse</code></a> package (e.g., for plotting). <code>tidyverse</code> is a collection of packages developed mainly by <code>RStudio</code> and their head data scientist Hadley Wickham. A full introduction of the <code>tidyverse</code> is beyond the scope of the present book, interested readers are encouraged to read the introductory book, <span class="citation"><a href="#ref-wickham2017" role="doc-biblioref">Wickham and Grolemund</a> (<a href="#ref-wickham2017" role="doc-biblioref">2017</a>)</span>, which is also <a href="https://r4ds.had.co.nz/">available for free online</a>.</p>
<p>We begin the analysis by loading the three packages first (use <code>install.packages(c("afex", "emmeans", "tidyverse"))</code> in case they are not yet installed). We also change the default <code>ggplot2</code> theme using <code>theme_set()</code> to a nicer one.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="standard1.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;afex&quot;</span>)</span>
<span id="cb1-2"><a href="standard1.html#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;emmeans&quot;</span>)</span>
<span id="cb1-3"><a href="standard1.html#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;tidyverse&quot;</span>)</span>
<span id="cb1-4"><a href="standard1.html#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">theme_set</span>(<span class="fu">theme_bw</span>(<span class="at">base_size =</span> <span class="dv">15</span>) <span class="sc">+</span> </span>
<span id="cb1-5"><a href="standard1.html#cb1-5" aria-hidden="true" tabindex="-1"></a>            <span class="fu">theme</span>(<span class="at">legend.position=</span><span class="st">&quot;bottom&quot;</span>, </span>
<span id="cb1-6"><a href="standard1.html#cb1-6" aria-hidden="true" tabindex="-1"></a>                  <span class="at">panel.grid.major.x =</span> <span class="fu">element_blank</span>()))</span></code></pre></div>
<p>The next step would be loading in the data. This is made easy here as the data from <span class="citation"><a href="#ref-urry2021" role="doc-biblioref">Urry et al.</a> (<a href="#ref-urry2021" role="doc-biblioref">2021</a>)</span> is part of <code>afex</code>, under the name <code>laptop_urry</code>. So we can load it with the <code>data()</code> function. We then also get an overview of the variables in this data set using <code>str()</code>, which returns the structure of a <code>data.frame</code>.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="standard1.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;laptop_urry&quot;</span>)</span>
<span id="cb2-2"><a href="standard1.html#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(laptop_urry)</span>
<span id="cb2-3"><a href="standard1.html#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; &#39;data.frame&#39;:    142 obs. of  6 variables:</span></span>
<span id="cb2-4"><a href="standard1.html#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ pid       : Factor w/ 142 levels &quot;1&quot;,&quot;2&quot;,&quot;4&quot;,&quot;5&quot;,..: 1 2 3 4 5 6 7 8 9 10 ...</span></span>
<span id="cb2-5"><a href="standard1.html#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ condition : Factor w/ 2 levels &quot;laptop&quot;,&quot;longhand&quot;: 1 2 2 1 2 2 1 2 2 1 ...</span></span>
<span id="cb2-6"><a href="standard1.html#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ talk      : Factor w/ 5 levels &quot;algorithms&quot;,&quot;ideas&quot;,..: 4 4 2 5 1 3 5 2 5 4 ...</span></span>
<span id="cb2-7"><a href="standard1.html#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ overall   : num  65.8 75.8 50 89 75.6 ...</span></span>
<span id="cb2-8"><a href="standard1.html#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ factual   : num  61.7 68.3 33.3 85.7 69.2 ...</span></span>
<span id="cb2-9"><a href="standard1.html#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  $ conceptual: num  70 83.3 66.7 92.3 82.1 ...</span></span></code></pre></div>
<p>The <code>str</code> function shows six variables, three of which we have already mentioned above:</p>
<ul>
<li><p><code>pid</code>: participant identifier, a <code>factor</code> with 142 levels, one for each participant.</p></li>
<li><p><code>condition</code>: <code>factor</code> identifying which note taking condition a participant belongs to, with two levels, <code>laptop</code> and <code>longhand</code>.</p></li>
<li><p><code>talk</code>: A <code>factor</code> identifying which TED talk a participant saw, with 5 level.</p></li>
<li><p><code>overall</code>: Numeric variable with participants’ overall memory performance on a scale from 0 (= no memory) to 100 (= perfect memory). This variable is called <code>overall</code> because it is the average of two separate memory performance scores given below.</p></li>
<li><p><code>factual</code>: Numeric variable with participants’ memory score for factual questions (ignored in this chapter).</p></li>
<li><p><code>conceptual</code>: Numeric variable with participants’ memory score for conceptual questions (analysed in the next chapter).</p></li>
</ul>
</div>
<div id="estimating-the-statistical-model" class="section level3" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Estimating the Statistical Model</h3>
<p>For estimating a basic statistical model using <code>afex</code> we can use the <code>aov_car()</code> function. The next code snippet show how to do so for the example data, when saving the output in object <code>res1</code> .</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="standard1.html#cb3-1" aria-hidden="true" tabindex="-1"></a>res1 <span class="ot">&lt;-</span> <span class="fu">aov_car</span>(overall <span class="sc">~</span> condition <span class="sc">+</span> <span class="fu">Error</span>(pid), laptop_urry)</span>
<span id="cb3-2"><a href="standard1.html#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Contrasts set to contr.sum for the following variables: condition</span></span></code></pre></div>
<p>The first argument to <code>aov_car()</code> is a <code>formula</code> specifying the statistical model, <code>overall ~ condition + Error(pid)</code>. The second argument identifies the <code>data.frame</code> containing the data (i.e., all the variables appearing in the <code>formula</code>), <code>laptop_urry</code>. We can also see that calling <code>aov_car()</code> produces a status message informing us that contrasts are set to <code>contr.sum</code> for the IVs in the model. This message is only shown for information purposes and can be safely ignored (we want <code>contr.sum</code> as contrasts for our variables, but as this is not the default <code>R</code> behaviour a message is shown).</p>
<p>A <code>formula</code> in <code>R</code> is defined by the presence of the tilde-operator <code>~</code> and the main way for specifying statistical models. It allows specifying statistical models in a similar way to the mathematical formulation, specifically the prediction equation of the statistical model, Equation <a href="standard1.html#eq:predmodel">(3.2)</a>. Therefore, a <code>formula</code> provides a comparatively intuitive approach for specifying a statistical model. On the left hand side of the <code>~</code> we have the dependent variable, <code>overall</code>. On the right hand side we have the variables we want to use to predict the dependent variable.</p>
<p>In the present case, the right-hand side consists of two parts concatenated by a <code>+</code>, the independent variable <code>condition</code> and an <code>Error()</code> term with the participant identifier variable <code>pid</code>. Thus, there are two difference between the <code>formula</code> used here and the prediction Equation <a href="standard1.html#eq:predmodel">(3.2)</a>, the <code>formula</code> misses an explicit intercept and we have specified an <code>Error()</code> term that is missing in Equation <a href="standard1.html#eq:predmodel">(3.2)</a>. Let us address these two difference in turn. The intercept is not actually missing from this equation, but implicitly included. More specifically, an intercept is specified using a <code>1</code> in a <code>formula</code>. However, unless an intercept is explicitly suppressed – which can be done by including <code>0</code> in the formula (and which should only be done if there are very good statistical reason to do so; i.e., it makes very rarely sense) – it is always assumed to be part of the models. Consequently, including it explicitly produces equivalent results. The following code shows this by comparing the previous result without explicit intercept, <code>res1</code> with an <code>aov_car</code> call with explicit intercept using the <code>all.equal()</code> function. This function can be used to compare arbitrary <code>R</code> objects and only returns <code>TRUE</code> if they are equal.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="standard1.html#cb4-1" aria-hidden="true" tabindex="-1"></a>res1b <span class="ot">&lt;-</span> <span class="fu">aov_car</span>(overall <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> condition <span class="sc">+</span> <span class="fu">Error</span>(pid), laptop_urry)</span>
<span id="cb4-2"><a href="standard1.html#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Contrasts set to contr.sum for the following variables: condition</span></span>
<span id="cb4-3"><a href="standard1.html#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="fu">all.equal</span>(res1, res1b)</span>
<span id="cb4-4"><a href="standard1.html#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; [1] TRUE</span></span></code></pre></div>
<p>The <code>Error()</code> term is a mandatory part of the model <code>formula</code> when using <code>aov_car()</code> and is used to specify the participant identifier variable (i.e., <code>pid</code> in this case). For a simple example as the present one that seems unnecessary, but later in the book we will see why the requirement of the <code>Error()</code> term is useful.</p>
<p>Before looking at the results, let us quickly explain why the function for specifying models is called <code>aov_car()</code>. A regular statistical model such as the ones considered here that solely includes factors (i.e., categorical variables) as independent variables is also known as <em>analysis of variance</em>, which is usually shortened to ANOVA.<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a> The basic <code>R</code> function for ANOVA models is simply called <code>aov()</code>. However, <code>aov()</code> does not in all cases return the expected results for all types of ANOVA models considered in this book (i.e., in some situations <code>aov()</code> can return results that would be considered inappropriate, even when used carfeully). An alternative to <code>aov()</code> is the <code>Anova()</code> function from package <a href="https://cran.r-project.org/package=car"><code>car</code></a> <span class="citation">(<a href="#ref-foxCompanionAppliedRegression2019" role="doc-biblioref">Fox and Weisberg 2019</a>)</span> (where <code>car</code> stands for the book title, “Companion to Applied Regression”). <code>Anova()</code> always returns the expected and appropriate ANOVA results when used correctly. However, calling <code>Anova()</code> requires at least two function calls and can become tricky with more complicated models discussed in later chapters. <code>aov_car()</code> combines the simplicity of model specification of the <code>aov()</code> function with the appropriate statistical results from the <code>Anova()</code> function from the <code>car</code> package (i.e., <code>aov_car()</code> calls <code>Anova()</code> internally).</p>
</div>
<div id="interpreting-the-results" class="section level3" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> Interpreting the Results</h3>
<p>We can now look at the results of our statistical model. For this, we simply call the object that contains the results <code>res1</code> (we would get the same output when calling <code>print(res1)</code> or <code>nice(res1)</code>).</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="standard1.html#cb5-1" aria-hidden="true" tabindex="-1"></a>res1</span>
<span id="cb5-2"><a href="standard1.html#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Anova Table (Type 3 tests)</span></span>
<span id="cb5-3"><a href="standard1.html#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb5-4"><a href="standard1.html#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Response: overall</span></span>
<span id="cb5-5"><a href="standard1.html#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;      Effect     df    MSE    F  ges p.value</span></span>
<span id="cb5-6"><a href="standard1.html#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 condition 1, 140 269.66 0.52 .004    .471</span></span>
<span id="cb5-7"><a href="standard1.html#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb5-8"><a href="standard1.html#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1</span></span></code></pre></div>
<p>The default <code>aov_car()</code> output is an “Anova Table” we will see throughout the book. We can also see that the results table contains “Type 3 tests,” but we will ignore this for now. The only other option, Type 2 tests, produces the same results for the example data. We will get back to the meaning of “type of test” in later chapters when it makes a difference and ignore this part until then.</p>
<p>The next line of the results table is only reference information. We see that the response variable, which we also know as DV, is <code>overall</code>, just as we intended.</p>
<p>We then get a table of effects, which in this case only has one row, the effect of <code>condition</code>. This row contains all the information for our null hypothesis significance test (NHST) for the condition effect. The most important column in this output is the last column, <code>p.value</code>, or <span class="math inline">\(p\)</span>-value. The <span class="math inline">\(p\)</span>-value in this column is the main results of NHST and allows us to judge the compatibility of the data with the null hypothesis. It is the probability of obtaining a difference as extreme as observed when assuming that the null hypothesis of no difference is true. We see that in this case the <span class="math inline">\(p\)</span>-value is not small, it is .47. Thus, the data are not incompatible with the null hypothesis and does not suggest that there is a memory difference between note taking with a laptop or in longhand format during lectures.</p>
<p>In general, researchers have adopted a significance level of .05. This means that if a <span class="math inline">\(p\)</span>-value is smaller than .05 we treat this as evidence that the data is incompatible with the null hypothesis. In this case we would say the result is “significant.” However, as in our case the result is not smaller than .05 the result is “not significant” (I would avoid saying “insignificant” if the <span class="math inline">\(p\)</span>-value is larger than .05, as “significant” is a technical term here). Thus, in the present case we do not reject the null hypothesis. The present data therefore do not provide evidence that the observed difference between the two modes of note taking generalises from the sample to the population according to NHST.</p>
<p>There are two further important columns whose results generally need to be reported, <code>df</code>, which stands for “degrees of freedom” (or <em>df</em>), and <code>F</code>. Understanding these columns in detail is beyond the scope of the present chapter, so we will only introduce them briefly. There are two degrees of freedom reported here, the first value, 1, is the numerator degree of freedom. It is always given by number of conditions minus 1. In the present case, we have two conditions, <code>laptop</code> and <code>longhand</code>, so the numerator <em>df</em> are 2 - 1 = 1. The second value is the denominator <em>df</em>, which are generally given by number of participants minus numerator df minus 1. Here we have 142 participants and therefore 142 - 1 - 1 = 140. In general, the larger the denominator <em>df</em> (i.e., the more participants we have) the better we can detect incompatibility with the null hypothesis (i.e., the easier it is to get small <span class="math inline">\(p\)</span>-values). The <span class="math inline">\(F\)</span>-value is a value expressing the observed incompatibility of the data with the null hypothesis. If <span class="math inline">\(F \leq 1\)</span>, the data are compatible with the null hypothesis. If <span class="math inline">\(F &gt; 1\)</span> the data are to some degree incompatible with the null hypothesis, with larger values indicating more incompatibility. The <span class="math inline">\(p\)</span>-value is calculated from <em>df</em> and <span class="math inline">\(F\)</span>-value. Consequently, the results are usually reported in the following way: <span class="math inline">\(F(1, 140) = 0.52\)</span>, <span class="math inline">\(p = .471\)</span>.</p>
<p>The next column that is important is <code>ges</code> which stands for generalised eta-squared, using the mathematical notation with Greek letters, <span class="math inline">\(\eta^2_G\)</span>. <span class="math inline">\(\eta^2_G\)</span> is a <em>standardised effect size</em> that tells us something about the absolute magnitude of the observed effect <span class="citation">(<a href="#ref-olejnikGeneralizedEtaOmega2003" role="doc-biblioref">Olejnik and Algina 2003</a>; <a href="#ref-bakemanRecommendedEffectSize2005" role="doc-biblioref">Bakeman 2005</a>)</span>. More specifically, <span class="math inline">\(\eta^2_G\)</span> is supposed to be a measure of the proportion of variance in the DV that can be accounted for by a specific factor or IV in the model. For example, in the present case the condition effect is supposed to explain 0.4% of the variance in performance. In general, we should avoid standardised effect sizes such as <span class="math inline">\(\eta^2_G\)</span> and instead report <em>simple effect sizes</em>. A simple effect size is expressed in units of our measured DV. For example, throughout this chapter we have mentioned that the observed difference in memory performance between both note taking conditions is 2.0 on the scale from 0 to 100. Here, the difference of 2.0 is a simple effect size. We will have to say more about effect sizes later, but as some journal editors or publishing guidelines require standardised effect sizes (which is statistically not a reasonable recommendation in my eyes) the default output contains it.</p>
<p>Finally, the default output contains the <code>MSE</code> column, which stands for “mean squared errors.” This column is mainly included for historical reasons. Traditionally, ANOVA models could relatively easily be calculated by hand or by calculator based on different variance terms (hence the name, analysis of variance). One of this term is the mean squared error from which, in combination with the residual squared error, the <span class="math inline">\(F\)</span>-value can be calculated. In my undergrad studies I still learned to calculate ANOVA by hand, but this seems rather unnecessary nowadays. Hence, we will simply ignore this column. Interested reader can find a detailed explanation about the meaning of MSE for example in <span class="citation"><a href="#ref-howellStatisticalMethodsPsychology2013" role="doc-biblioref">Howell</a> (<a href="#ref-howellStatisticalMethodsPsychology2013" role="doc-biblioref">2013</a>)</span> or <span class="citation"><a href="#ref-baguleySeriousStatsGuide2012" role="doc-biblioref">Baguley</a> (<a href="#ref-baguleySeriousStatsGuide2012" role="doc-biblioref">2012</a>)</span>.</p>
<p>One thing we note in the results table is that it does not contain any information about the intercept. However, as discussed above, the intercept is included in the model. The reason for omitting the intercept from the default output is that it is generally not of primary interest. In experimental research usually the main interest is in the effect of our independent variables, the effect of the experimental manipulation. The statistical model that separates the intercept (i.e., overall mean) from the condition effect allows to zoom in on the relevant part. In line with this, the default output of <code>aov_car</code> does the same. Later chapters will show how we can also get information about the intercept.</p>
<p>Estimating a statistical model with <code>aov_car()</code> provides us with the inferential statistical results, the null hypothesis tests for the IV-effects shown above. To get these, we just need to call the object containing the results at the <code>R</code> prompt (e.g., calling <code>res1</code> in the present case). However, we can use the results object also for others parts of the statistical analyses, for data visualisation and follow-up analyses.</p>
</div>
<div id="data-visualisation" class="section level3" number="3.4.4">
<h3><span class="header-section-number">3.4.4</span> Data Visualisation</h3>
<p>For data visualisation we can use the <code>afex</code> function <code>afex_plot()</code> which is built on top of the <code>ggplot2</code> package. <code>afex_plot()</code> requires an estimated model object (e.g., as returned from <code>aov_car()</code>) and specifying which factors of the model we want to plot. In the present case, we only have one factor, <code>condition</code>, so we can only choose this one. Importantly, all factors passed to <code>afex_plot()</code> need to be passed as character strings (i.e., enclosed with <code>"..."</code>).</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="standard1.html#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">afex_plot</span>(res1, <span class="st">&quot;condition&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:fig1"></span>
<img src="stats_for_experiments_files/figure-html/fig1-1.png" alt="afex\_plot() figure for data from Urry et al. (2021)" width="70%" />
<p class="caption">
Figure 3.3: afex_plot() figure for data from Urry et al. (2021)
</p>
</div>
<p>This simple call to <code>afex_plot()</code> produces already a rather good looking results figure combining the individual-level data points (in the background in grey) with the condition means (in black). Individual data points in the background that have the same or very similar values are displaced on the x-axis so they do not lie on top of each other. This is achieved through package <a href="https://cran.r-project.org/package=ggbeeswarm"><code>ggbeeswarm</code></a> (which needs to be installed once: <code>install.packages("ggbeeswarm")</code>). The plot also per default shows 95% confidence intervals of the means, which we will explain in detail in a later chapter.</p>
<p>As <code>afex_plot()</code> returns a <code>ggplot2</code> plot object, we can manipulate the plot to make it nicer.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="standard1.html#cb7-1" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">afex_plot</span>(res1, <span class="st">&quot;condition&quot;</span>)</span>
<span id="cb7-2"><a href="standard1.html#cb7-2" aria-hidden="true" tabindex="-1"></a>p1 <span class="sc">+</span> </span>
<span id="cb7-3"><a href="standard1.html#cb7-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;note taking condition&quot;</span>, <span class="at">y =</span> <span class="st">&quot;memory performance (0 - 100)&quot;</span>) <span class="sc">+</span></span>
<span id="cb7-4"><a href="standard1.html#cb7-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">100</span>)) <span class="sc">+</span></span>
<span id="cb7-5"><a href="standard1.html#cb7-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">group =</span> <span class="dv">1</span>))</span></code></pre></div>
<p><img src="stats_for_experiments_files/figure-html/fig2-1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>For example, in the code snippet above we first save the plot object as <code>p1</code> and then call a number of <code>ggplot2</code> function on this plot object to alter the plot appearance (in <code>ggplot2</code> graphical elements are added to a plot using <code>+</code>). Function <code>labs()</code> is used to change the axis labels, <code>coord_cartesian()</code> changes the extent of the y-axis (i.e., the plot now show the full possible range of memory performance score), and <code>geom_line(aes(group = 1)</code> adds a line connecting the two means. This figure could now be used in a results report or manuscript as is.</p>
</div>
<div id="follow-up-analysis" class="section level3" number="3.4.5">
<h3><span class="header-section-number">3.4.5</span> Follow-Up Analysis</h3>
<p>Follow-up analysis refers to an inspection of the predicted condition means and their relationships. In the case of a single independent variable with two levels (e.g., laptop versus longhand) their is not much to investigate in this regard. We can nevertheless show the general procedure. For follow-up analyses we generally begin with function <code>emmeans()</code> from package <code>emmeans</code> <span class="citation">(<a href="#ref-lenth2021" role="doc-biblioref">Lenth 2021</a>)</span>. Function <code>emmeans()</code> then returns the estimated marginal means, which is a slightly complicated way of saying condition means, plus additional statistical information.</p>
<p>Similarly to <code>afex_plot()</code>, <code>emmeans()</code> requires an estimated model object as well as the specification of a factor in the model for which we want to get the condition means:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="standard1.html#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">emmeans</span>(res1, <span class="st">&quot;condition&quot;</span>)</span>
<span id="cb8-2"><a href="standard1.html#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  condition emmean   SE  df lower.CL upper.CL</span></span>
<span id="cb8-3"><a href="standard1.html#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  laptop      68.2 1.99 140     64.3     72.1</span></span>
<span id="cb8-4"><a href="standard1.html#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  longhand    66.2 1.91 140     62.4     70.0</span></span>
<span id="cb8-5"><a href="standard1.html#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb8-6"><a href="standard1.html#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; Confidence level used: 0.95</span></span></code></pre></div>
<p>For now we only focus on the estimates means in column <code>emmean</code> and ignore the additional inferential statistical information in columns <code>SE</code> to <code>upper.CL</code>. We can see that the reported means match the means given in the text at the very beginning of the chapter, <a href="standard1.html#ex:urry">3.1</a>.</p>
<p>The power of <code>emmeans</code> is not only to provide the condition means, but it also allows us to perform calculation on the condition means. For example, in the case of a factor with two levels we can easily calculate the difference between the condition means as our simple effect size. For this, we can save the object returned by <code>emmeans()</code> and then call the <code>pairs()</code> function on this object which gives us all pairwise comparisons of conditions means of which there is only one in the present case (we would get the same results by combining both calls into one: <code>pairs(emmeans(res1, "condition"))</code>):</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="standard1.html#cb9-1" aria-hidden="true" tabindex="-1"></a>em1 <span class="ot">&lt;-</span> <span class="fu">emmeans</span>(res1, <span class="st">&quot;condition&quot;</span>)</span>
<span id="cb9-2"><a href="standard1.html#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(em1)</span>
<span id="cb9-3"><a href="standard1.html#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  contrast          estimate   SE  df t.ratio p.value</span></span>
<span id="cb9-4"><a href="standard1.html#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;  laptop - longhand     1.99 2.76 140   0.722  0.4715</span></span></code></pre></div>
<p>The output shows a mean difference of 1.99 which slightly differs from the 2.0 reported above, which is slightly concerning. However, the results reported above are rounded to one decimal only. If we do so for the present results, we also get an estimated difference of 2.0 (we will not explain this code in detail here):</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="standard1.html#cb10-1" aria-hidden="true" tabindex="-1"></a>em1 <span class="sc">%&gt;%</span> </span>
<span id="cb10-2"><a href="standard1.html#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pairs</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb10-3"><a href="standard1.html#cb10-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb10-4"><a href="standard1.html#cb10-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">format</span>(<span class="at">digits =</span> <span class="dv">1</span>, <span class="at">nsmall =</span> <span class="dv">1</span>)</span>
<span id="cb10-5"><a href="standard1.html#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt;            contrast estimate  SE    df t.ratio p.value</span></span>
<span id="cb10-6"><a href="standard1.html#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">#&gt; 1 laptop - longhand      2.0 2.8 140.0     0.7     0.5</span></span></code></pre></div>
</div>
</div>
<div id="summary-1" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Summary</h2>
<p>The goal of this chapter was to introduce the standard statistical approach for analysing experimental data with one independent variable with two levels – an experiment with two conditions. Practically every time when we run such an experiment, we observe that there is some mean difference in the dependent variable between the two conditions. For our example data by <span class="citation"><a href="#ref-urry2021" role="doc-biblioref">Urry et al.</a> (<a href="#ref-urry2021" role="doc-biblioref">2021</a>)</span> there was a memory difference of 2.0 points between the two note taking conditions (laptop versus longhand) on the response scale from 0 to 100.</p>
<p>The important statistical question we then have is whether there is any evidence suggesting that the observed difference in our sample generalises to the population. The sample are the participants in our experiment and the population refers to all possible participants that could have been sampled. For <span class="citation"><a href="#ref-urry2021" role="doc-biblioref">Urry et al.</a> (<a href="#ref-urry2021" role="doc-biblioref">2021</a>)</span> this population could be loosely described as students taking notes or maybe more precisely undergraduate students at research intensive (R1) US universities. The question we would like to get a statistical answer to is: Should we believe that there generally is a memory difference between note taking with a laptop versus longhand? To answer this question we need <em>inferential statistics</em>.</p>
<p>The inferential statistical approach we are using is called <em>null hypothesis significance testing</em> or NHST. However, NHST does not directly address the question whether there is evidence for a difference in the population. Instead, NHST tests the compatibility of the data with the <em>null hypothesis</em> – the assumption that there is no difference between the condition in the population. The most important result from NHST is the <span class="math inline">\(p\)</span>-value. The <span class="math inline">\(p\)</span>-value is a measure of the compatibility of the data with the null hypothesis; it is the probability of obtaining a results as extreme as observed assuming the null hypothesis is true. If the <span class="math inline">\(p\)</span>-value is smaller than .05 we reject the null hypothesis that there is no difference. In this case we decide that there is evidence for a difference (although this does not follow with logical necessity).</p>
<p>To apply NHST to the data we set up a <em>statistical model</em> that observed partitions the data into three parts (Equation <a href="standard1.html#eq:statmodel">(3.1)</a>): the intercept representing the overall mean, the effect of the independent variable (i.e., the difference of the condition means from the intercept), and the residuals representing the idiosyncratic aspects not explained by the other parts of the model. This partitioning allows us to zoom in on the part of the data that we are interested in, the effect of our independent variable, the experimental manipulation.</p>
<p>To estimate a statistical model to the data we used function <code>aov_car()</code> from the <code>afex</code> package. <code>aov_car()</code> allows us to specify the statistical model using a formula of the form <code>dv ~ iv + Error(pid)</code> (where <code>pid</code> refers to the variable in the data with the participant identifier) mimicking the mathematical specification of the statistical model. The default output returns an ANOVA table which provides a null hypothesis significance test for our <code>iv</code>, the independent variable. The returned table is called an ANOVA table because statistical models that only contain factors are called analysis of variance or ANOVA. In the present case, the statistical model only has a single factor, note taking condition, with two levels, laptop versus longhand. In the returned ANOVA table, we do not only have the <span class="math inline">\(p\)</span>-value for our experimental factor, but additional inferential statistical information such as the degrees of freedom, <em>df</em>, and the <span class="math inline">\(F\)</span>-value.</p>
<p>We can also use the object returned from <code>aov_car()</code> for plotting using function <code>afex_plot()</code>. This function produces a plot combining the individual-level data points with the condition means. This provides a comprehensive display of the data of the experiment. As the function returns a <code>ggplot2</code> object, this plot can be be easily modified to create a figure that can be used in a results report.</p>
<p>We can also use the object returned from <code>aov_car</code> for follow-up analyses using <code>emmeans</code>. With <code>emmeans</code> we can easily obtain the condition means (or estimated marginal means) on the dependent variable. Based on these condition means we can calculate the observed effect size (i.e., the mean difference).</p>
<p>Applying the statistical model to the data from <span class="citation"><a href="#ref-urry2021" role="doc-biblioref">Urry et al.</a> (<a href="#ref-urry2021" role="doc-biblioref">2021</a>)</span> showed a non significant difference, <span class="math inline">\(F(1, 140) = 0.52\)</span>, <span class="math inline">\(p = .471\)</span>. This suggests that there is no difference in memory performance after watching a talk and taking notes with either a laptop or in longhand format.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-baguleySeriousStatsGuide2012" class="csl-entry">
Baguley, Thomas. 2012. <em>Serious Stats : A Guide to Advanced Statistics for the Behavioral Sciences</em>. Houndmills, Basingstoke, Hampshire; New York: Palgrave Macmillan.
</div>
<div id="ref-bakemanRecommendedEffectSize2005" class="csl-entry">
Bakeman, Roger. 2005. <span>“Recommended Effect Size Statistics for Repeated Measures Designs.”</span> <em>Behavior Research Methods</em> 37 (3): 379–84. <a href="https://doi.org/10.3758/BF03192707">https://doi.org/10.3758/BF03192707</a>.
</div>
<div id="ref-cohen1994" class="csl-entry">
Cohen, Jacob. 1994. <span>“The Earth Is Round (p &lt; .05).”</span> <em>American Psychologist</em> 49 (12): 997–1003. <a href="https://doi.org/10.1037/0003-066X.49.12.997">https://doi.org/10.1037/0003-066X.49.12.997</a>.
</div>
<div id="ref-foxCompanionAppliedRegression2019" class="csl-entry">
Fox, John, and Sanford Weisberg. 2019. <em>An <span>R</span> Companion to Applied Regression</em>. Third. <span>Thousand Oaks CA</span>: <span>Sage</span>.
</div>
<div id="ref-howellStatisticalMethodsPsychology2013" class="csl-entry">
Howell, David C. 2013. <em>Statistical Methods for Psychology</em>. <span>Belmont, CA</span>: <span>Wadsworth Cengage Learning</span>.
</div>
<div id="ref-kline2015" class="csl-entry">
Kline, Rex B. 2015. <em>Principles and Practice of Structural Equation Modeling</em>. Guilford Press.
</div>
<div id="ref-lee2013" class="csl-entry">
Lee, Michael D., and Eric-Jan Wagenmakers. 2013. <em>Bayesian Cognitive Modeling: A Practical Course</em>. Cambridge: Cambridge University Press.
</div>
<div id="ref-lenth2021" class="csl-entry">
Lenth, Russell. 2021. <em>Emmeans: Estimated Marginal Means, Aka Least-Squares Means</em>. <a href="https://CRAN.R-project.org/package=emmeans">https://CRAN.R-project.org/package=emmeans</a>.
</div>
<div id="ref-meehl1978" class="csl-entry">
Meehl, Paul E. 1978. <span>“Theoretical Risks and Tabular Asterisks: Sir Karl, Sir Ronald, and the Slow Progress of Soft Psychology.”</span> <em>Journal of Consulting and Clinical Psychology</em> 46: 806–34. <a href="https://doi.org/10.1037//0022-006X.46.4.806">https://doi.org/10.1037//0022-006X.46.4.806</a>.
</div>
<div id="ref-nickerson2000" class="csl-entry">
Nickerson, Raymond S. 2000. <span>“Null Hypothesis Significance Testing: A Review of an Old and Continuing Controversy.”</span> <em>Psychological Methods</em> 5 (2): 241.
</div>
<div id="ref-olejnikGeneralizedEtaOmega2003" class="csl-entry">
Olejnik, Stephen, and James Algina. 2003. <span>“Generalized <span>Eta</span> and <span>Omega Squared Statistics</span>: <span>Measures</span> of <span>Effect Size</span> for <span>Some Common Research Designs</span>.”</span> <em>Psychological Methods</em> 8 (4): 434–47. <a href="https://doi.org/10.1037/1082-989X.8.4.434">https://doi.org/10.1037/1082-989X.8.4.434</a>.
</div>
<div id="ref-rozeboom1960" class="csl-entry">
Rozeboom, William W. 1960. <span>“The Fallacy of the Null-Hypothesis Significance Test.”</span> <em>Psychological Bulletin</em> 57 (5): 416–28. <a href="https://doi.org/10.1037/h0042040">https://doi.org/10.1037/h0042040</a>.
</div>
<div id="ref-R-afex" class="csl-entry">
Singmann, Henrik, Ben Bolker, Jake Westfall, Frederik Aust, and Mattan S. Ben-Shachar. 2021. <em>Afex: Analysis of Factorial Experiments</em>.
</div>
<div id="ref-urry2021" class="csl-entry">
Urry, Heather L., Chelsea S. Crittle, Victoria A. Floerke, Michael Z. Leonard, Clinton S. Perry, Naz Akdilek, Erica R. Albert, et al. 2021. <span>“Don<span>’</span>t Ditch the Laptop Just Yet: A Direct Replication of Mueller and Oppenheimer<span>’</span>s (2014) Study 1 Plus Mini Meta-Analyses Across Similar Studies.”</span> <em>Psychological Science</em>, February, 0956797620965541. <a href="https://doi.org/10.1177/0956797620965541">https://doi.org/10.1177/0956797620965541</a>.
</div>
<div id="ref-wagenmakers2007" class="csl-entry">
Wagenmakers, Eric-Jan. 2007. <span>“A Practical Solution to the Pervasive Problems of p Values.”</span> <em>Psychonomic Bulletin &amp; Review</em> 14 (5): 779–804. <a href="https://doi.org/10.3758/BF03194105">https://doi.org/10.3758/BF03194105</a>.
</div>
<div id="ref-wickham2017" class="csl-entry">
Wickham, Hadley, and Garrett Grolemund. 2017. <em>R for Data Science: Import, Tidy, Transform, Visualize, and Model Data</em>. Sebastopol CA: O’Reilly.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="10">
<li id="fn10"><p>Urry et al. transformed the memory scores before their analysis based on an earlier paper doing the same. However, the memory scores from 0 to 100 are easier to understand than the transformed scores. As the results are qualitatively the same for both approaches, we use the untransformed memory scores here.<a href="standard1.html#fnref10" class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p>We will discuss standard errors in detail in the coming chapters. Until then, it is enough to understand the standard error as a representation of the statistical precision of the mean. In other words, the larger the standard error, the less sure we are about the “true” value of the mean (again, what we mean with true will be discussed later).<a href="standard1.html#fnref11" class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p>All statistical models considered in this book are solely based on observable quantities. We predict the DV (i.e., what we measure) from the IVs we either manipulate (in experiments) or observe/measure (for non-experimental DVs). However, statistical models can also use non-observable (latent) quantities to explain the DV. Popular examples in psychology are structural equation models <span class="citation">(e.g., <a href="#ref-kline2015" role="doc-biblioref">Kline 2015</a>)</span> or cognitive models <span class="citation">(e.g., <a href="#ref-lee2013" role="doc-biblioref">Lee and Wagenmakers 2013</a>)</span>.<a href="standard1.html#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>Equation <a href="standard1.html#eq:statmodel">(3.1)</a> is displayed in simplified and not fully correct form. Mathematically correct would be that either each variable (i.e., <span class="math inline">\(\text{DV}\)</span>, <span class="math inline">\(\text{intercept}\)</span>, <span class="math inline">\(\text{IV-effect}\)</span>, and <span class="math inline">\(\text{residual}\)</span>) has an index, such as <span class="math inline">\(i\)</span>, that goes from 1 to <span class="math inline">\(N\)</span> (where <span class="math inline">\(N\)</span> is the total number of observations) or that each variables is a vector (i.e., holds multiple values) of length <span class="math inline">\(N\)</span>.<a href="standard1.html#fnref13" class="footnote-back">↩︎</a></p></li>
<li id="fn14"><p>Describing in detail why statistical models with solely factors as IVs are called analysis of variance even though we are comparing condition means and not variances is beyond the scope of the present work. The short answer is that this has historical reasons. One can calculate the statistical tests in these models by hand by comparing different variance terms. For a full explanations, interested readers are encouraged to read the excellent explanation in <span class="citation"><a href="#ref-howellStatisticalMethodsPsychology2013" role="doc-biblioref">Howell</a> (<a href="#ref-howellStatisticalMethodsPsychology2013" role="doc-biblioref">2013</a>)</span> (any edition of the book should have it).<a href="standard1.html#fnref14" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="role-of-statistics-in-the-research-process.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="case-study-1-more-results-from-note-taking-studies.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["stats_for_experiments.pdf", "stats_for_experiments.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
