[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"online textbook currently development. written Henrik Singmann, lecturer UCL.book provides introduction research methods statistics mostly experimental research psychology related discipline. contrast introductory texts, tries stay light mathematics instead focus logic practical application statistical methods actual research data. reason generally computer perform calculations us. Therefore, able calculations hand seem important. idea matters becoming competent statistics user understanding logic statistics. Furthermore, focussing important bits mathematics, allows us spend time bits explain everything detail without assuming much prior knowledge. performing statistical analyses, using statistical programming language R.focus logic real research applications also provides benefit compared textbooks. Recent years shown one largest issues field fact statistics used wrongly – many cases researchers use appropriate statistical methods. problem often researchers tend draw conclusions fully justified. words, issue people interpret results statistical analysis, necessarily analysis performed. Using real data sets allows us discuss detail interpret results. show matters understanding statistical result context generated, also demonstrate conclusions permitted actual published data sets.sum , present approach several benefits: focuses skills researchers need analyse data; accessible readers without particular background mathematics; allows discuss role statistics research process; allows discussing interpretations justifiable given certain results; provides opportunity highlight problems associated mindless application statistics.","code":""},{"path":"index.html","id":"intended-audience","chapter":"Preface","heading":"Intended Audience","text":"book intended postgraduate (.e., MSc level ) audience. Besides basic understanding summary statistics average (.e., mean) probabilities used express uncertainty, background knowledge required. important skill understanding book, interest using data answer interesting research questions. see later, situation statistics important tool can potentially assist providing answer.","code":""},{"path":"index.html","id":"chapter-overview","chapter":"Preface","heading":"Chapter Overview","text":"point time, book written limited set chapters already available.Chapter 1 provides introduction role statistics research process. also answers important question: need statistics?Chapter 2 provides overview important concepts correct terminology need describe research designs (e.g., variable? difference dependent independent variables?). also introduces distinction experimental observational studies.Chapter 3 provides introduction R `tidyverse.Chapter 4 provides introduction data visualisation R using ggplot2 package.Chapter 5 introduces basic statistical approach using simple experiment two conditions.","code":""},{"path":"index.html","id":"acknowledgments","chapter":"Preface","heading":"Acknowledgments","text":"project exist without help feedback provided Stuart Rosen, Anna Krason, David Kellen, Lukasz Walasek.","code":""},{"path":"index.html","id":"license-and-attribution","chapter":"Preface","heading":"License and Attribution","text":"book licensed Attribution-NonCommercial 4.0 International (CC -NC 4.0) license. license allows share adapt work long give appropriate attribution use materials commercial purposes.Parts book uses materials released compatible CC license. , source clearly indicated. Please ensure attribute original source case re-use materials.book adopted ideas code back-end (specifically quizzes) PsyTeachR project University Glasgow.","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"role-of-statistics-in-the-research-process","chapter":"1 Role of Statistics in the Research Process","heading":"1 Role of Statistics in the Research Process","text":"book concerned experimental psychology, particular statistical analysis experiments psychology related disciplines language science, behavioural science, cognitive science, neuroscience. order expressed previous sentence – science first statistical analysis second – one overarching principles think statistics. Whereas goal introduce concepts techniques required perform statistical analysis (mostly experimental) data, perspective taken statistical analysis can performed understood within scientific context takes place .One consequence perspective start point statistical analysis needs specific clear research question. case data collection analysis guided research question, statistical analysis generally indispensable part research process. describe detail coming chapters, statistics tool allows us draw inferences go beyond data observed. ability generalise allows us connect experimental results research questions underlying theories. sum, statistical techniques introduced book can provide meaningful scientifically helpful answers data collected analysed clear research question mind.","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"the-research-process","chapter":"1 Role of Statistics in the Research Process","heading":"1.1 The Research Process","text":"One way describe research process psychology related sciences terms four interrelated steps: research question, operationalisation research question data collection, statistical analysis data, finally communication results. Let us explain steps detail.research begin research question. disciplines considered often theory hypothesis human behaviour human mind. example, widely accepted idea decision making behavioural science people exhibit loss aversion – displeasure resulting losing £10 stronger pleasure derived winning £10. look example research question, whether evidence idea loss aversion, detail chapter. example already shows something common research questions – contains general statement involves directly observable quantities (pleasure displeasure).research questions general question whether loss aversion. fact, many research questions lot specific applied. example, many applied domains important question whether specific intervention – new therapeutical procedure workplace training – better intervention already existing one.Sometimes, researcher just wants find answer specific issue come previous study. concrete example, consider Hinze Wiley (2011) addresses specific question concerning testing effect. testing effect describes now well established phenomenon testing newly learned materials (e.g., self-tests quizzes) leads better memory additional studying (e.g., re-reading) (Roediger Karpicke 2006). Hinze Wiley (2011) noticed time research testing effect used limited ways implementing testing, multiple choice tests open-ended questions. see whether phenomenon general, research question whether testing effect also occurred another type testing, fill--blank tests (.e., sentences learned materials shown words missing needed filled ). results showed fill--blank tests effective mere re-reading show genuine testing effect (least one goes beyond specific words filled ). Thus, fill---blank testing () strategy effective testing types require involved processing materials (e.g., open ended questions).next step research process transformation research question empirical statistical hypothesis. call step operationalisation. , instead talking abstract ideas research questions, need concretely decide study run. need find tasks measures (e.g., questionnaires) allow us collect data addresses research question. important part operationalisation specification relevant variables. can understand variables dimensions, features, characteristics individuals situations can differ. example, relevant variables loss aversion magnitude potential loss gain intensity resulting pleasure displeasure (e.g., measured questionnaire). testing effect, relevant variables type additional learning (e.g. re-reading versus multiple choice testing) final memory performance.can see example testing effect, sometimes appears difficult separate research question operationalisation. case, research question tied specific aspect operationalisation (.e., testing implemented). However, even concrete research question always aspects study require operationalisation (e.g., measure learning?). example loss aversion shows, research questions general. Consequently, multitude possible studies can performed investigate one research question. However, specific study researcher needs decide one specific study design. exactly tasks measures using investigate question interested ?traditional view research process – known hypothetico-deductive method – operationalisation step, researchers derive empirical prediction tests theory. words, theory coupled operationalisation predicts specific outcome (empirical prediction), needs occur theory true. example, discuss detail , loss aversion predicts people unwilling gamble money chance losing specific amount money equal chance winning amount money (losing amount hurts winning amount). According traditional view, predicted outcome occur, learn theory false. Furthermore, predicted outcome occur, entail theory necessarily true, provide support theory. However, see (Section 1.3.1), reality generally learn less prescribed idealised form hypothetico-deductive method.Research practice often diverges idealised view research process (e.g., Haig 2014). cases one clear prediction specific theory tested. Researchers might compare multiple theories diverging predictions, vague hypothesis instead fully fledged theory single prediction follows, might simply curious happens specific situation. example, Hinze Wiley (2011), research question fill--blank test show testing effect testing procedures. None possible results, even complete absence testing effect type testing, provided evidence testing effects general. goal research confirm disconfirm testing effect. Instead, goal test generality testing effect. Consequently, seem appropriate say operationalisation always involves making specific empirical predictions. However, even absence specific empirical prediction, operationalisation must result empirical hypothesis relating two variables part research design research question.1 final step operationalisation data collection.research question operationalised corresponding data collected, time statistical analysis. Generally, statistical analysis answers one specific question: data provide evidence empirical hypothesis derived research question? remainder book show detail perform statistical analyses common study designs interpret results light research question operationalisation.data sufficiently analysed, reached final step research process, need communicate results (step also known dissemination). different forms dissemination depending goal audience (e.g., scientific journal article, dissertation report, conference presentation, press release). Whereas different forms differ amount detail background provided, need provide truthful comprehensive account whole research process: research question? investigated (.e., describe operationalisation)? results? mean research question? Often, difficult problem solve step provide comprehensive truthful account succinct manner. One important tool graphical means – pictures results. Consequently, book discuss present statistical results text create appropriate graphs.Whereas abstract overview leaves important things also part research – research questions come – shows three important things.primacy research question. research question determines operationalisation thus data collected. research question also determines statistical analysis, indirectly; research question determines empirical hypothesis tested analysis.primacy research question. research question determines operationalisation thus data collected. research question also determines statistical analysis, indirectly; research question determines empirical hypothesis tested analysis.statistical analysis directly connected research question. statistical analysis performed operationalisation research question, research question . means statistical analysis directly inform us research question. words, statistically test research question. Instead, statistics can tell us something specific operationalisation. Whether allows strong inferences research question depends operationalisation. shown following example, important part scientific discourse argue whether certain operationalisations allow one address specific research questions. However, generally statistical question.statistical analysis directly connected research question. statistical analysis performed operationalisation research question, research question . means statistical analysis directly inform us research question. words, statistically test research question. Instead, statistics can tell us something specific operationalisation. Whether allows strong inferences research question depends operationalisation. shown following example, important part scientific discourse argue whether certain operationalisations allow one address specific research questions. However, generally statistical question.Statistics end goal research. Instead, end goal usually written communication research. cases, statistical analysis indispensable part communication can provide evidence specific empirical hypothesis. However, understand full meaning implications particular statistical result, important know context – research question operationalisation. task research communicate context communicating research. Without context, impact meaning statistical result severely limited.Statistics end goal research. Instead, end goal usually written communication research. cases, statistical analysis indispensable part communication can provide evidence specific empirical hypothesis. However, understand full meaning implications particular statistical result, important know context – research question operationalisation. task research communicate context communicating research. Without context, impact meaning statistical result severely limited.","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"example-the-psychology-of-loss-aversion","chapter":"1 Role of Statistics in the Research Process","heading":"1.2 Example: The Psychology of Loss Aversion","text":"get better understanding research process problems can arise , let us consider detail concrete example research question can investigated empirically. Specifically, let us return example loss aversion. Loss aversion one assumptions underlying prospect theory (Kahneman Tversky 1979), mathematically formalised theory combining cognitive psychology economic theory.2 concise description loss aversion “losses loom larger gains” (Kahneman Tversky 1979, 279). described , loss aversion means negative feeling associated loss certain amount money larger positive feeling associated gain amount. example, loss aversion predicts displeasure pain losing £10 larger pleasure joy winning £10.can see loss aversion theoretical statement involving latent - , unobservable - quantities negative positive feelings (.e., displeasure versus pleasure). can ask people feel, easily observe feelings without asking. psychology, generally call unobservable theoretical concepts constructs. can test whether people indeed show loss aversion directly observe constructs form core ?One possibility testing hypothesis individuals show loss aversion hinted . either give people certain amount, say £10, take away , ask feel. procedure runs least two problems. First, clearly ethically unacceptable perform experiment consists taking £10 away participants. Second, even overcome ethical problems (e.g., first giving participants endowment taking money away endowment) still problem measure feelings associated two events.One way avoid ethical problem taking away money participants ask imagine feel lost gained certain amount money. Even though certain whether imagined feeling corresponds actual feeling participants actually losing gaining amount, procedure commonly used. example, McGraw et al. (2010) asked participants imagine play single game 50% chance losing $200 50% chance winning $200 (e.g., flipping coin comes heads win $200; otherwise lose $200). asked participants imagine feel either two outcomes. identified two different ways ask question, shown Figure 1.1 (McGraw et al. 2010). first possibility, bipolar scale, shown upper part. response scale participants loss condition respond left side scale participants gain condition respond right side scale. compare ratings, measured participants’ responses absolute distance rating neutral point “Effect” (.e., “Small Positive Effect” treated intensity “Small Negative Effect”). second possibility shown lower part Figure 1.1 shows unipolar intensity scale. scale, participants conditions provide response scale rate intensity displeasure pleasure.\nFigure 1.1: Example two different scales measuring feelings potential gain loss. upper part shows bipolar scale losses receive rating left side (“Effect”) gains receive rating right side. lower part shows unipolar scale intensity losses gains given scale. Image adapted McGraw et al. (2010).\nreading , take moment ask believe two scales shown Figure 1.1 make difference whether participants show loss aversion. put differently, can think reason matters ask participants’ feelings loss gain? difference, scale expect likely loss aversion occurs?investigate question whether loss aversion scales, McGraw et al. (2010) asked half participants use bipolar scale half use unipolar scale rate feelings imagined loss gain $200. data hand, compared feeling ratings imagined losses wins two conditions. results showed indeed matters scale used. bipolar scale, evidence loss aversion. feeling ratings gains losses approximately equal around 3.4 (1 = Effect 5 = Large Effect). However, using unipolar scale, found evidence loss aversion. loss condition, participants reported stronger feeling average (around 3.6) compared gain condition (around 3.1).McGraw et al. (2010) explain results terms relative versus absolute feeling judgements. bipolar scale, people first judge valence feeling – , whether good bad – determine side provide response. done, judge intensity feeling. However, intensity judgement follows valence judgement, make intensity judgement comparing feelings valence. specifically, McGraw et al. (2010) assume loss condition, loss compared negative events. Similarly, gain condition, gain compared positive events. words, bipolar scale, participants make relative judgement intensity compared loss gains. Consequently, argue results bipolar scale helpful answering question whether loss aversion.3For unipolar scale, McGraw et al. (2010) argue judgement feeling intensity preceded valence judgement. Therefore, people use absolute judgement feeling, comparing negative positive feelings. Consequently, data, shows pattern line loss aversion, helpful question whether loss aversion. Overall conclude study provides evidence loss aversion, appears using unipolar scale.results McGraw et al. (2010) show seemingly minor differences operationalisation research question can tremendous effect results. line , perhaps surprising operationalisation investigating loss aversion – asking participants feelings – common. discuss alternative operationalisations.whereas McGraw et al. (2010) provide explanation results, explanation might difficult come seen data. inability able predict effect response scale results , potential consequences reaching. take results seriously generalise domains, might conclude whenever interested participants’ feelings matters whether use bipolar unipolar scales. One even go another step say whenever use subjective rating scales, type scale unintended effect results. , often good idea try find operationalisation research questions involve subjective rating scales, types responses choices, response times, complex behaviour.","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"evidence-for-loss-aversion-lotteries","chapter":"1 Role of Statistics in the Research Process","heading":"1.2.1 Evidence for Loss Aversion: Lotteries","text":"different operationalisation testing loss aversion compare choices across different risky choices, lotteries, gambles (terms can understood interchangeably ), common experimental paradigm decision making behavioural economics. lottery context consists different options, associated one multiple outcomes, participant choose one.simplest type lotteries ones consisting one option. case, participants can decide whether accept reject lottery. example, evidence loss aversion can found lotteries used Battalio, Kagel, Jiranyakul (1990). One lotteries (Question 15) :play following gamble:\n50% chance losing $20 50% chance winning $20.Participants decide whether accept reject lottery.4 43% participants accepted lottery.participants experiment Battalio, Kagel, Jiranyakul (1990) dislike symmetric 50-50 lotteries. following clip YouTube channel Veritasium, also observes random people street.result Battalio, Kagel, Jiranyakul (1990) provide evidence loss aversion? evidence comes fact participants likely reject lottery accept (.e., acceptance rate 50%). important lottery symmetric 50-50 lottery. , magnitude potential loss equal magnitude potential gain possible outcomes appear equal probability 50%.5 Remember loss aversion means disliking loss liking gain magnitude. symmetric lottery exactly situation. fact participants likely decline participate symmetric lottery therefore much line loss aversion.One problem results Battalio, Kagel, Jiranyakul (1990) collected data 35 participants provide statistically compelling evidence loss aversion. Whereas data shows descriptive pattern line loss aversion (.e., 50% acceptance symmetric lotteries) supported statistical analysis. specifically, statistical analysis provide support empirical prediction observed acceptance rate 50%.[] descriptively look like , evidence data enough surpass statistical criterion use judging evidence. fuller description set statistical criterion come later.compelling evidence loss aversion comes study Brooks Zank (2005). task, participants asked make decision complex lotteries participants decide two options. example, one lotteries following:option prefer?: 25% chance +£11, 50% chance £0, 25% chance -£11.B: 25% chance +£10, 50% chance £0, 25% chance -£10.see two options, two symmetric outcomes magnitude. options B, 25% chance loss 25% chance gain magnitude (50% probability participants neither lose gain anything). difference B , magnitude potential loss gain larger £1.notion loss aversion makes interesting prediction case. Participants increasingly dislike symmetric lottery larger potential outcomes . lottery means dislike losing £10 liking gaining £10, difference dislike liking larger potential outcomes £11. terms empirical prediction means participants willing choose option B option lottery .study Brooks Zank (2005), 49 participants worked around 50 lotteries similar structure lottery . , two options loss gain magnitude probability (third potential outcome always smaller magnitude loss/gain). example lottery , difference options always loss/gain magnitude one option £1 larger option. line empirical prediction loss aversion, participants chose option smaller magnitude loss/gain 63% cases. given larger sample size study, statistical analysis also supported prediction 63% larger 50%.show thing loss aversion? Results Brooks Zank (2005) certainly appear support theoretical idea (see also Camerer 2005). also makes intuitive sense. people (included) feel symmetric lotteries really attractive become increasingly unattractive increasing magnitude (really thinks flipping coin chance losing winning say £100,000 sounds like good idea?). However, previous example, evidence loss aversion discussed hinges particular operationalisation theoretical idea. results, really learned magnitude loss gain psychologically relevant factor minds people. thing learned people dislike certain lotteries options lotteries. Can find alternative data pattern involve loss aversion?","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"alternative-explanation-loss-aversion-or-loss-seeking","chapter":"1 Role of Statistics in the Research Process","heading":"1.2.2 Alternative Explanation: Loss Aversion or Loss Seeking?","text":"One clever alternative explanation looks like loss aversion provided Walasek Stewart (2015). study, participants also presented 50-50 lotteries. example one lotteries shown Figure 1.2 . shown figure, lottery involved mixed outcomes, gains losses (.e., -$18 +$20 example). Furthermore, lottery consisted one option, participants choose whether accept play lottery . Finally, possible outcomes always 50% probability occurring. make logic clearer participants, told accepting lottery shown Figure 1.2 equal flipping coin -$18 one side +$20 side. Depending outcome came top, money change accordingly. participants rejected lottery, neither lost won money.\nFigure 1.2: Screenshot lottery task used investigate loss aversion. screenshot Walasek Stewart (2015, Figure 1).\ndescribing study results detail, let first lay gist argument Walasek Stewart (2015). Among lotteries participants saw study symmetric lotteries potential loss equal potential gain. example, -$12/+$12 lottery. participants accepted lotteries average neither win lose money.clever manipulation Walasek Stewart (2015) across different conditions (.e., groups participants) manipulated attractive symmetric lotteries , relative lotteries participant saw. one condition, many lotteries potential loss smaller potential gain, -$12/+$20 lottery. , participant condition accepted lotteries, likely win money. condition symmetric lotteries therefore relatively unattractive. another condition, many lotteries potential loss larger potential gain, -$20/+$12 lottery. , participant condition accepted lotteries, likely lose money. condition symmetric lotteries therefore relatively attractive.According original idea loss aversion, thing matters lottery absolute magnitude potential gains losses manipulation relative attractiveness symmetric lotteries effect whatsoever. However, results Walasek Stewart (2015) showed manipulation mattered. condition symmetric lotteries relatively unattractive, participants accepted 21% cases, condition lotteries relatively attractive, participants accepted 71% cases. statistical analysis supported empirical prediction acceptance rates symmetric lotteries differed across conditions. results pattern contradiction original idea loss aversion.next section present full details study Walasek Stewart (2015), coming back means loss aversion fits within general theme chapter. decision making research behavioural economics main interest, details may super relevant , okay understand perfectly. However, use study example next chapters great least give cursory read.","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"full-details-of-walasek-and-stewart-2015","chapter":"1 Role of Statistics in the Research Process","heading":"1.2.2.1 Full Details of Walasek and Stewart (2015)","text":"many experiments fall within cognitive domain, study Walasek Stewart (2015) consisted series similar trials participants task (.e., accept reject shown lottery). differed across trials values two possible outcomes. example, one conditions experiment losses ranged -$6 -$20 increments -$2 (resulting 8 different possible losses) gains ranged $12 $40 increments $4 (resulting 8 different possible gains). Across trials one participant condition, possible losses combined possible gains total participants decide \\(8 \\times 8 = 64\\) lotteries whether accepted rejected .Within 64 trials subset trials allowed researchers directly address question whether evidence loss aversion.6 Whereas lotteries shown participants asymmetric – , potential loss differed numerically potential gain (example Figure 1.2) – small subset lotteries symmetric. lotteries, amounts potential loss equal amount potential gain. specifically, symmetric lotteries -$12/+$12, -$16/+$16, -$20/+$20.condition described losses ranged -$20 gains ranged +$40, 191 participants accepted symmetric lotteries 21% time. 21% descriptively 50% indicating participants indeed disliked lotteries liked (.e., overall likely reject accept symmetric lotteries). Furthermore, statistical analysis supported empirical hypothesis (.e., provided evidence pattern results generalises beyond current data). described , result makes sense light loss aversion. losing certain amount money worse winning amount money, one reject symmetric lottery one equally likely lose win certain amount money.clever manipulation Walasek Stewart (2015) included three conditions changed range possible outcomes. addition -$20/+$40 condition discussed , 202 participants $-20/+$20 condition saw lotteries losses ranging -$20 gains also ranging +$20 . Another group 190 participants, -$40/+$40 condition, saw lotteries losses ranging -$40 gains also ranging +$40. Finally, Walasek Stewart (2015) also included -$40/+$20 condition 198 participants losses ranged -$40, gains +$20 (.e., complement -$20/+$40 condition). Importantly, conditions number possible outcomes losses gains 8 (step size either \\(\\pm\\)$2 \\(\\pm\\)$4). Table 1.1 shows possible outcome conditionTable 1.1:  Possible outcomes lotteries Experiment 1 Walasek Stewart (2015). condition, participant saw 64 lotteries resulting combining possible gains possible losses condition.consequence design, changed across conditions whether symmetric lotteries relatively good relatively bad. understand , need look remaining asymmetric lotteries. -$20/+$40 condition discussed far, lotteries possible gain larger possible loss (e.g., -$18/+$20 lottery) lotteries possible loss larger gain (e.g., -$20/+$18 lottery). Consequently, symmetric lotteries relatively bad (.e., compared many lotteries possible gain larger possible loss). -$20/+$20 -$40/+$40 conditions, asymmetric lotteries balanced. half asymmetric lotteries possible gain larger possible loss, whereas half possible loss larger possible gain. Consequently, symmetric lotteries neither relatively good relatively bad. Finally, -$40/+$20 condition pattern flipped respect -$20/+$40 condition. lotteries possible gain larger possible loss compared many lotteries possible loss larger gain. Consequently, symmetric lotteries relatively good.matter whether symmetric lotteries relatively good ? Indeed . reminder, people unlikely accept symmetric lotteries -$20/+$40 condition symmetric lotteries relatively bad. Participants condition accepted 21% symmetric lotteries. -$20/+$20 condition symmetric lotteries neither relatively good relatively bad, participants accepted 50% symmetric lotteries. Similarly, -$40/+$40 condition participants accepted 45% symmetric lotteries. Finally, -$40/+$20 condition symmetric lotteries relatively good, participants accepted 71% symmetric lotteries. can see 71% descriptively 50%, indicating participants liked symmetric lotteries disliked (.e., overall likely accept reject lotteries). Furthermore, statistical analysis supported empirical hypothesis (.e., provided evidence results pattern generalises beyond current data).","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"what-do-the-results-mean-for-loss-aversion","chapter":"1 Role of Statistics in the Research Process","heading":"1.2.2.2 What do the Results Mean for Loss Aversion?","text":"results Walasek Stewart (2015) show, choice pattern lotteries always line idea loss aversion. context symmetric lottery relatively bad see evidence line idea loss aversion. context symmetric lottery relatively good, see opposite pattern one term loss seeking. mean loss aversion? original idea Kahneman Tversky (1979) matters magnitude loss gain surely line results Walasek Stewart (2015). Instead, Walasek Stewart (2015) argue relevant determine psychological impact gain loss relative magnitude rank possible outcome: Compared gains losses regularly experience, large gain large loss?moving linking example psychology loss aversion general goal book, let us answer one last question. matters rank (suggested Walasek Stewart (2015)) magnitude outcome (proposed Kahneman Tversky (1979)), see evidence effect magnitude studies investigating loss aversion manipulate context gains losses (e.g., Brooks Zank 2005)? answer question provided Stewart, Chater, Brown (2006). argue (also provide empirical evidence) daily lives experience small losses (e.g., buying something bakery) larger gains (e.g., monthly salary). consequence, gain loss magnitude, relative rank gain compared gains lower corresponding relative rank loss compared losses. difference, generally observe pattern consistent loss aversion different theoretical reason proposed Kahneman Tversky (1979).","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"epistemic-gaps","chapter":"1 Role of Statistics in the Research Process","heading":"1.3 Epistemic Gaps: The Difference Between What we Want to Know and What we can Know","text":"goal chapter provide introduction role statistics research process. need statistics can tell us research questions interested ? However, far talked much statistics. Instead introduced abstract concept research process exemplified example loss aversion. example tried show even though can find statistical evidence supports empirical hypothesis, mean found answer research question. question whether loss aversion statistical question. answer research question depends whether can rule possible alternative explanations, example, rank potential loss gain important, rather magnitude. nevertheless need statistics help us determine whether data provides support particular empirical hypothesis operationalises research question.Let’s put bluntly. yet know statistics detail, application statistical methods can appear like magical machine provides us answer research question . throw data , turn handle statistics machine, get answer research question . Sadly, image statistics (mention needing handle!) false. reason least two epistemic gaps research process prevent us getting straight answer research question. section introduce gaps ensure can get realistic image role statistics research process.explaining epistemic gap , need take step back think science general. least two different domains constitute scientific discipline. Firstly, substantive content, science . example, psychology concerned human mind behaviour. one domain psychology consists theories mind behaviour (e.g., people exhibit loss aversion). , just theories enough. problem many intuitively plausible ultimately untrue theories (e.g., idea distinct “learning styles”; see Pashler et al. 2008). second domain constituting science research; systematic investigations research questions provide us evidence. evidence allows us decide theories believe theories discard.perspective science allows us draw conclusions means us scientists. Firstly, important adopt theories easily early. scientists need natural sceptics. Instead believing theory sounds compelling, need ask evidence first. need evaluate evidence use basis degree belief. example, independent studies support certain theoretical position (case loss aversion), seems appropriate consider certain theoretical position possibility also consider alternative accounts (example, rank based account Walasek Stewart 2015). evidence weaker, say studies proponents theory, even cautious degree belief assign theory. scientific evidence overwhelming willing treat theory approximately true. psychology, many theories majority researchers agree latter criterion reached (besides maybe operant classical conditioning). consequence, many cases, best thing honest admit evidence sufficient hold strong theoretical position. us scientists, perspective also means need able evaluate evidence research area interest. requires substantive knowledge research domain, also statistical knowledge introduce book. Finally, means need open revising beliefs light new evidence (case loss aversion results Walasek Stewart 2015).last two paragraphs show, science ultimately evidence. evidence support theories field? scientists hope statistics tool helps us answer question evidence. degree , much hope. need aware epistemic gaps. Let us explain now means.Epistemology branch philosophy concerned knowledge (e.g., knowledge, know know) justification (e.g., reason believe something) (philosophical overview see: Steup Neta 2020). “Epistemic” corresponding adjective. Epistemology therefore, field deals philosophical questions core science, example, theories believe based available evidence. epistemic gap describes difference want know can actually know.7 Ideally, want know whether theory hypothesis true. already seen example , many cases even carefully designed experiments unambiguously answer question (e.g., Walasek Stewart (2015) available evidence appeared support ‘loss aversion,’ now sure ). following, take abstract look question discuss two general problems complicate issue. can know often quite different like know. competent application statistics requires one aware problem avoids -interpreting results one’s research.","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"epistemic-gap-1","chapter":"1 Role of Statistics in the Research Process","heading":"1.3.1 Epistemic Gap 1: Underdetermination of Theory by Data","text":"provided example theoretical (basic) research question (evidence loss aversion?). seen answering research question requires careful thinking operationalisation; exactly set study test question? decided one operationalisation, seen can find alternative explanations can explain results without making assumptions original research question. words, even though operationalisation carefully chosen, unambiguously answer research question.fact compellingly answer research questions despite employing carefully chosen operationalisation problem unique example. contrast, important insight philosophy science problem empirical study. issue also known underdetermination theory data Duhem-Quine thesis always occurs difference research question corresponding operationalisation. seen examples, essentially always .different aspects underdetermination. first specification research question operationalisation. Research questions usually involve unobservable constructs – emotions (e.g., fear), memory, attention, comprehension, learning – vague phrases, “works better” “improves.” empirical investigation questions however requires precise specification operationalisation. going research question concrete operationalisation, guarantee operationalisation captures intended meaning research question.example, consider test new therapeutic intervention compared standard one. Imagine found new intervention decreased self-reported discomfort symptoms patient questionnaire strongly old treatment, reduce number sick days due disorder. interpret mean new treatment “works better” old one? sense , another . problem nuances result operationalising research question concretely always align broad way like think research questions.point might think yet sound like big problem. just need define research questions precisely enough able learn something research question. Sadly, easier said done. first problem often impossible precisely define research question, yet found way precisely define constructs involved (known problem coordination, (Kellen et al. 2021)). cases, precisely defining abstract constructs possible anyway.example, hypothesis specific emotion, say fear, related behavioural pattern, say aggression, run problem generally agreed upon definition either constructs. probably exist questionnaires measuring fearfulness aggressive tendencies, questionnaires represent corresponding constructs definition . ask sample participants fill questionnaires found scores participants two questionnaires related, allow conclude fearfulness aggression related. conclusion allowed fearfulness measured one questionnaire related aggression measured another questionnaire. course, scientists like make general conclusion constructs related, inference logically follow.general problem run Duhem-Quine thesis: empirical hypothesis tested study two parts: theoretical prediction well set auxiliary assumptions links theoretical prediction data. stay example, theoretical prediction fear aggression related. auxiliary assumptions additional assumptions needed test question empirically decided part operationalisation: questionnaire valid measure constructs (big assumption), data collection took place without unforeseen problems, tested enough participants find effect, use appropriate statistical procedures, etc. can seen, list auxiliary assumptions somewhat limitless difficult enumerate fully. also contains quite mundane assumptions assume research actually took place just made researcher (exception, see case Diederik Stapel).core Duhem-Quine thesis empirical result pertain solely theoretical prediction interest, union (conjunction) theoretical prediction interest auxiliary assumptions. results line empirical hypothesis, supports theoretical prediction auxiliary assumptions true. Likewise, results line empirical hypothesis, provides evidence theoretical prediction auxiliary assumptions true. However, testing whether auxiliary assumptions true done study tests empirical hypothesis set test (can always come auxiliary assumptions specifically tested). Consequently, individual result provide conclusive evidence particular theoretical prediction. can always alternative explanation differs theory hypothesis one .8 meant underdetermination theory data.Whereas issue might seem like purely philosophical discussion, far . actual scientific discussions literature auxiliary assumptions part operationalisation research question. example, argument loss aversion proposed Kahneman Tversky (1979) hinges auxiliary assumption participants interpret possible outcomes lotteries terms magnitude absolute value. shown Walasek Stewart (2015), auxiliary assumption appear hold least cases participants instead interpret relative value possible outcomes lotteries. easy find similar examples research area interested .sum , problem underdetermination first epistemic gap particular result never uniquely supports challenges one theoretical position hypothesis. result appears support theory another theory makes prediction auxiliary hypothesis false thus require different theory. Likewise, result seems disagree theory, theory can always protected claiming one auxiliary assumptions incorrect. also exactly happens real scientific discourse. example, John Bargh, prominent social psychologist Yale, confronted results disagreed one prominent findings (Doyen et al. 2012) attacked (now deleted blog post still can found ) “incompetent ill-informed researchers” claimed study “many important differences procedure, worked eliminate effect.” section shown, questioning methods (.e., auxiliary assumptions) legitimate defence protects one’s theory. course, one can question auxiliary assumptions original results appeared support theory way. case Bargh, appears exactly happened. psychologists stopped believing original finding (e.g., Harris, Rohrer, Pashler 2021).","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"epistemic-gap-2","chapter":"1 Role of Statistics in the Research Process","heading":"1.3.2 Epistemic Gap 2: Signal versus Noise","text":"first epistemic gap strong logical link theories underlying research questions operationalisation research question. Thus, terms steps research process concerns relationship step 1, research question, step 2, operationalisation data collection. next epistemic gap concerns relationship steps 2 step 3, statistical analysis.described (1.1), important task operationalisation transform research questions empirical hypothesis. Ideally, hypothesis comes form empirical prediction, describing possible outcome support theoretical hypothesis (.e., outcome predicted theory). part also clearly designate possible outcome , occur, speak theoretical hypothesis. decided , collect data run statistical analysis. goal statistical analysis provides us evidence respect empirical hypothesis. data support empirical hypothesis ?providing overview done, let us go back example , study Walasek Stewart (2015). contrast original formulation loss aversion (Kahneman Tversky 1979), based magnitude absolute value gain loss, theoretical prediction Walasek Stewart (2015) drives people’s behaviour relative value gain loss. test , presented participants lotteries different conditions range gains losses differed. one condition small losses large gains another condition large losses small gains (ignore two conditions ). hypothesis follows design relative attractiveness symmetric lotteries (magnitude possible loss = magnitude possible gain) differs conditions. small losses/large gains condition symmetric lottery relatively unattractive large losses/small gains condition relatively attractive. resulting empirical prediction participants less willing accept symmetric lotteries small losses/large gains condition large losses/small gains condition. line prediction, participants small losses/large gains condition accepted 21% symmetric lotteries whereas participants large losses/small gains condition accepted 72% symmetric lotteries.just looking bare numbers, results appear support empirical prediction. Participants roughly 50 percentage points less likely accept symmetric lotteries small loss/large gains condition large loss/small gains condition. However, can sure particular difference chance occurrence? Maybe just got unlucky participants small loss/large gains condition reason generally less likely accept lotteries participants large loss/small gains condition. Maybe former participants horrible night sleep really bad mood time testing therefore reject gambles whereas case participants latter condition. case, observed difference actually tell us anything research questions.problem described previous paragraph heart statistical approach described book. core problem responses get participants experiments inherently noisy. Human participants can things number reasons. reasons related research question operationalisation others . example, participants read lotteries carefully think provide answer, likely values possible outcome play role answer. case, responses relevant research question. participants distracted message phone read problem fully? intend accept lottery accidentally reject (.e., press wrong button)? can also imagine matters participants relatively rich relatively poor. someone million bank, might really matter lose win $16 might inherently likely gamble lottery someone hourly wage. cases, values lotteries minor effect thus responses less irrelevant research question.question whether results support empirical hypothesis, therefore like distinguish responses relevant research question – can call signal – responses generated less randomly irrelevant research question – can call noise. procedure distinguish signal noise simply see whether signal supports empirical prediction. , data provide support hypothesis , data support empirical hypothesis. Sadly, procedure exist (know people , reason research first place).absence procedure can definitely separate contribution signal noise, statistical approach introduced compares estimate signal estimate noise. Let us assume moment estimated signal supports empirical prediction example (.e., predict participants less likely accept lottery small loss/large gains condition data shows). compare estimated signal estimated noise. estimated signal large given estimated level noise, assume data supports empirical prediction. estimated signal large given estimated noise, assume data support empirical prediction.can estimate signal noise? Estimating signal straight forward. just use observed difference conditions estimate signal. example Walasek Stewart (2015), observed difference accepting symmetric lotteries two conditions (roughly 50 percentage points). Estimating noise bit complicated described detail later chapters. now enough understand affected two components: (1) variability responses within condition (2) overall sample size (.e., number participants). variability within condition becomes smaller (.e., measurement becomes precise) sample size stays , levels noise decrease. Likewise, sample size increases constant level variability, level noise decreases.Another important question counts “large” comparing estimated signal estimated noise. following chapters introduce decision threshold signal noise ratio make judgement.9 signal noise ratio threshold, act signal change beliefs. , make decision. decision threshold chosen across many decisions control rate making false positive decisions (false positive occurs say signal none). particular, decision threshold chosen across many situations signal, incorrectly assume signal 5% decisions.Taken together, statistical procedures use attempt answer question whether signal supports empirical hypothesis given human data inherently random noisy. problem estimated signal – observed difference conditions – also affected noise. never know observed difference due signal interested just based noise. overcome problem compare observed signal observed level noise. observed level large relative observed noise, decide data supports idea genuine signal present . words, never really know current data genuinely supports prediction ; just act . always chance effect due noise. one data set analysing, 100% certain estimate signal noise fully accurate. However, describe detail later, across decisions use statistical decision procedure, controls rate making false positive decisions.case first epistemic gap, second epistemic gap also shows unambiguously learn wanted know – whether data supports empirical hypothesis . signal large relative noise, evidence , evidence never fully conclusive. evidence might strong, later see can identify , always remaining doubt back head. Maybe just got unlucky participants study responded way made look like signal, wasn’t. just one data set hand, ruled .","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"summary","chapter":"1 Role of Statistics in the Research Process","heading":"1.4 Summary","text":"chapter provided conceptual overview research process psychology related disciplines. concept, research always begins research question. want know? Often research question stems particular theory want test, can also purely applied hypothesis.next important step operationalisation question followed data collection. means need find appropriate tasks measures develop study design can test empirical hypothesis following research question. seen discussion first epistemic gap, consequence separation research question operationalisation , strictly speaking, study lets us learn tasks measures using. problem underdetermination theory data, even apparently positive result allow us infer supports theoretical hypothesis. core problem empirical hypothesis combination theoretical hypothesis auxiliary assumptions rule one auxiliary assumptions false.collected data hand, next step perform statistical analysis. , hope find evidence informs us empirical hypothesis. procedure use book attempts distinguish signal data, part data relevant empirical hypothesis, noise, randomness inherent using human participants.10 However, second epistemic gap entails even statistical procedure, find fully conclusive evidence. problem estimate true amount noise data. Research participants myriad potential reasons show certain behaviour reasons need related research question. precise measures participants, can control level noise degree, ultimately sure whether just get unlucky see due noise hypothesis.final step research process communication results. step essentially combines previous steps. need communicate research question, operationalisation, data collection process, results statistical analysis. Whereas communication results ultimately goal research project, also step mindful limits research. biggest danger forget epistemic gaps inherent empirical research oversell results. course want research allows us answer (potentially big broad) research questions, honest audience stick reality primarily learning something operationalisation. get statistical result appears support empirical prediction, want treat true, clear always chance result might fluke.","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"some-further-examples-and-passing-thoughts","chapter":"1 Role of Statistics in the Research Process","heading":"1.4.1 Some Further Examples and Passing Thoughts","text":"Let us end chapter another concrete example literature highlights issues discussed ties together thoughts communicate research. motivate one’s research question, good idea start big picture. real world issues theoretical problems want address? Whereas good idea, mistake operationalisation research question big picture. Surely, use particular task – symmetric lotteries investigating loss aversion – learn something underlying research question theory? surely , question difficult answer exactly learn.final example illustrate problem, let us consider research risk preferences decision making. idea risk preferences people might willing take risks (e.g., gambling choosing investment) others. exist number different tasks investigate risk preferences experimentally, balloon analogue risk task [BART; Lejuez et al. (2002)] Columbia card task (Figner et al. 2009), well number different questionnaires. large study around 1500 participants performed eight different tasks filled twelve different questionnaires designed measure risk preferences (Pedroni et al. 2017; Frey et al. 2017) show participants’ behaviour across tasks questionnaires surprisingly unrelated. Whereas participants scored high one questionnaire also scored high questionnaires (.e., different questionnaires shared common risk trait), scores questionnaires largely unrelated behaviours different tasks. Furthermore, behaviours across different risk tasks unrelated (.e., participant specifically risky one task particularly risky another task). words, even though tasks questionnaires appear measure risk preferences, failure find consistent pattern across participants suggests fail coherent manner. One might wonder fact questionnaires related represents sort silver lining. share interpretation instead attribute common-method variance. important result questionnaires also unrelated behaviour tasks. tells point time, really understand risk preferences , measure , exist way conceptualised.sum , important keep mind thing learn something research primarily operationalisation. want make case also learn something underlying research question, make good case spell alternative explanations rule auxiliary assumptions can take granted. usually requires considering results study. short, confuse task measures theory research question.communicating statistical results, also need avoid overselling results. general principle, report results humble manner. end, avoid language suggests level confidence provide. means, statistical results never “prove” “confirm” empirical hypothesis. Instead, may “support” “suggest” certain interpretations.","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"conclusion","chapter":"1 Role of Statistics in the Research Process","heading":"1.5 Conclusion","text":"scientists, aim answer interesting important questions domain studying. problem research address research questions directly. Instead, research addresses operationalisation research question. Drawing inferences research question requires accepting number usually untested auxiliary assumptions. even data appears support hypothesis willing accept auxiliary assumptions, always risk just got unlucky interpret noise signal. consequence, individual study can provide little evidence, especially large general research questions. Even worse, sometimes learn research chosen operationalisation unable answer research question. sum, definitive answers research questions need one study.Whereas paints less optimistic picture can learn research one hope, important stay realistic humble. Many ideas, especially , appear intuitive compelling feel must right. scientists need stay sceptical avoid urge believe theories overwhelming evidence conclusively rules possible alternative explanations (even haven’t yet thought ). one thing distinguishes science non-scientific belief systems science principle based solid evidence. overall strength evidence provided research depends whole research process statistics one part.","code":""},{"path":"chapter-1-quiz.html","id":"chapter-1-quiz","chapter":"Chapter 1: Quiz","heading":"Chapter 1: Quiz","text":"Note: pull-menu selecting answer turns green selecting correct answer.Exercise 1.1  start point statistical analysis ?data set many variables allows testing many different hypothesesA clearly specified research questionThe believe hypothesis probably trueAnswer: 123Exercise 1.2  following one steps research process?Research QuestionOperationalisation data collectionStatistical analysisResponding emailsCommunication resultsAnswer: 12345Exercise 1.3  part operationalisation?Hypothetico-deductive methodSpecifying tasks questionnaires usedTransformation research question empirical statistical hypothesisDeciding variables data collectedAnswer: 1234Exercise 1.4  main reason need statistics?prove/confirm theoryTo test whether data collected provide evidence empirical hypothesisTo publish papers high-impact journalsAnswer: 123Exercise 1.5  epistemic gap?difference want know can knowA specific outcome predicted theoryA mathematically formalized cognitive theoryAnswer: 123Exercise 1.6  Underdetermination theory data suggests :Every specific hypothesis theory false eventually shown falseWe need hypotheses answer research questionsNo single data sets unambiguously supports specific hypothesis theoryAnswer: 123Exercise 1.7  can researcher assume data support empirical prediction?data descriptively supports empirical predictionWhen epistemic gapsWhen estimated signal large given estimated level noiseWhen estimated level noise smaller pre-specified thresholdAnswer: 1234Exercise 1.8  can conclude statistical analysis provides strong support empirical hypothesis?research question true probably trueOur empirical hypothesis true probably trueIt unlikely alternative explanation explain resultsNone aboveAnswer: 1234","code":""},{"path":"research-designs.html","id":"research-designs","chapter":"2 Data and Research Designs","heading":"2 Data and Research Designs","text":"previous chapter (Chapter 1) provided overview research process psychology related disciplines. goal overview show statistics just one part full research endeavour usually end goal. also highlighted answer research questions requires results statistical analysis, context result generated. specifically, argued important part research process usually operationalisation research question: measures? task participants ? study design? goal chapter provide us necessary conceptual knowledge terminology answer questions research.","code":""},{"path":"research-designs.html","id":"empirical-evidence-and-data","chapter":"2 Data and Research Designs","heading":"2.1 Empirical Evidence and Data","text":"book concerned empirical research. words, generally interested research questions answer proof can found purely thinking hard, mathematics philosophy. Instead, interested research question evidence comes form observations experiences: empirical evidence short. discussed previous chapter, mean theories include unobservable quantities mental states (e.g., fear, enjoyment, attention). However, theories include unobservable quantities, must causally responsible something observable (e.g., behaviour, electrical wave brain). way, can still test theories (e.g., theory predicts fear lead aggression, can induce fear without leading aggression, learn theory must wrong).fact interested empirical research means ultimate arbiter whether believe theory empirical evidence. matter elegant intuitive theory . observed behaviour people disagrees theory, wrong. also means theories vague possible empirical evidence disprove considered part empirical sciences (.e., empirical theories). criterion also known falsifiability introduced philosopher Karl Popper 1930s. example, never ending discussion whether Freudian psychoanalysis principle falsifiable . Whereas Karl Popper strong belief (render psychoanalysis non-scientific), proponents Freudian psychoanalysis naturally see rather differently.11Empirical evidence comes least two different forms, either anecdotes data. Whereas anecdote typically refers single person, data usually contains information multiple persons. However, anecdotes data differ dimensions just number observations, summarised following aphorism: plural anecdote data.Anecdotes unsystematic observations, typically form stories (e.g., “friend friend”), somehow address research questions. problem anecdotes generally difficult verify investigate . makes impossible rule possible alternative explanations relationship anecdote research question. seen previous chapter, one main criteria deciding whether observation provides evidence theoretical claim whether can rule plausible alternative explanations. sum, anecdotes surely matter coming good hypotheses ideas study, mature sciences anecdotes play minor evidentiary role deciding claims believe.Data systematic observations collected specific purpose, answering research question bookkeeping. Data generally consists observations multiple variables. previous chapter defined variables dimensions, features, characteristics individuals situations can differ. technical definition variable corresponds specific set possible outcomes (states affair/events), possible outcome corresponds one value variable. Furthermore, can define observation smallest unit data. technically, one observation results collecting least one value one variable values different variables one unit observation (psychology, unit observation usually participant).example data, consider study Walasek Stewart (2015) discussed previous chapter (1). task participants accept reject 50-50 lotteries (example, see Figure 1.2) participant 64 trials. Table 2.1 shows six observations two different participants study. way data shown exactly format Lukasz Walasek used analyse data (.e., variables added removed). Observations shown rows variables shown columns. tabular representation data observations rows variables columns common used throughout book.Table 2.1: Selected observations data Walasek & Stewart (2015, Exp. 1a). “[…]” indicates part whole data set observations shown.total can see six different variables data set. Let us discuss turn. first variable, subno (generally use monospace font refer variable names appear data set), participant identifier “subject number” (actual individuals take part research passive subjects, term “participant” now preferred “subject”). variable part data set uniquely identify participant (generally, unit observation) specific observation belongs. , see takes numbers. uncommon use numbers participant identifier variable, can also combination numbers letters, (ideally anonymous) name.second third variables, loss gain, specify possible outcomes lotteries trial. example, second observation/row shows lottery potential loss $6 potential gain $8. Based two columns, can see observations ordered combination loss gain. means order observations data reflect actual order trials participants saw (order random). values two variables numbers lowest possible loss/gain 6 largest possible loss/gain 40, depending condition participant .Column four shows response participant lottery, either accept reject. sixth variable, resp, numeric version response. , accept decision represented 1 reject decision 0. two variables carry information, different formats different benefits. response variable makes easy understand ordinary language participant’s response . resp variable makes easy perform calculation results uses numerical code represent information. example, reject mapped onto 0 accept mapped onto 1, take mean observations get overall acceptance rate across gambles (mean whole data set 0.38 corresponds acceptance rate 38%).12 However, resp variable, additionally need information responses 0 1 correspond .Finally, variable five informs us condition participant . Remember previous chapter experiment varied range gains losses across participants, resulting four conditions total: condition loss gains ranging -$20/+$40, -$20/+$20 condition, -$40/+$40 condition, -$40/+$20. information provided form decimal number values decimal point referring range gains values decimal range losses without trailing 0 (.e., opposite order referred conditions far). Thus, participant subno 8 -$20/+$20 condition participant 369 -$20/+$40 condition.example Walasek Stewart (2015) hopefully clarifies abstract definition variable provided . variable, set possible outcomes defined research design. example, participant identifier subno, set encompasses possible participants took part study. loss gain, set contains potential losses potential gains occur lotteries. response two possible outcomes: accept reject lottery. define values possible outcome. case subno assign different number every participant collect data. loss gain, values correspond magnitude potential loss gain US dollars (currency used experiment). response, use numeric code, word represent two outcomes.things note example data shown Table 2.1.Every observation data set complete; observation values variable missing data. Whereas common experimental research, always case types research. Whereas missing data something discuss detail book, important aware can happen think case (sadly, general solution). example, type experimental research used example , missing data can happen computer used data collection crashes. usually happens rarely unsystematic manner, generally simply discard incomplete data collect data another participant. different issue exists data missing systematically. can happen research sensitive issues. example, easy imagine study sexual health, participants sexually transmittable disease (STD) unwilling report therefore just answer questions STDs. Discarding cases missing data problematic bias results sense resulting lower rates STD actually present.Every observation data set complete; observation values variable missing data. Whereas common experimental research, always case types research. Whereas missing data something discuss detail book, important aware can happen think case (sadly, general solution). example, type experimental research used example , missing data can happen computer used data collection crashes. usually happens rarely unsystematic manner, generally simply discard incomplete data collect data another participant. different issue exists data missing systematically. can happen research sensitive issues. example, easy imagine study sexual health, participants sexually transmittable disease (STD) unwilling report therefore just answer questions STDs. Discarding cases missing data problematic bias results sense resulting lower rates STD actually present.multiple observations per participant, 64 precise. 64 observations given different rows. call data format – data participant potentially spans multiple rows, one row per observation – long format. long format contrasts wide format also commonly found social sciences. wide format, data one participant spans single row. participant multiple observations, given different columns. procedures introduced book, generally want data long format. Note participant provides one observation, difference long wide format.multiple observations per participant, 64 precise. 64 observations given different rows. call data format – data participant potentially spans multiple rows, one row per observation – long format. long format contrasts wide format also commonly found social sciences. wide format, data one participant spans single row. participant multiple observations, given different columns. procedures introduced book, generally want data long format. Note participant provides one observation, difference long wide format.variables differ whether contain numbers (variables response) (response).variables differ whether contain numbers (variables response) (response).","code":""},{"path":"research-designs.html","id":"data-types","chapter":"2 Data and Research Designs","heading":"2.2 Data Types","text":"Let us discuss last point detail. common intuition think numbers thinking data. seen example data, necessary. Data numbers. response variable shows can use values, words phrases, represent values variables. However, conception data primarily numbers also completely false. example, statistical analyses introduced book need represent variables terms numbers. Fortunately us, tools using generally convert data using numbers numeric data necessary. means use type data representation makes easiest understand data stands .","code":""},{"path":"research-designs.html","id":"numerical-versus-categorical-variables","chapter":"2 Data and Research Designs","heading":"2.2.1 Numerical Versus Categorical Variables","text":"important issue arises thinking data numbers numbers can mean different things. One possibility numbers represent numerical information; , represent measurement, magnitude, count something. However, can also use numbers broader sense serve label (e.g., numbers football jerseys telephone numbers). case, numbers represent categorical information; observation falls one set mutually exclusive categories. meaning numbers important consequences use . numbers represent numerical information, mathematical operations make sense. example, make much sense calculate average two telephone numbers. ’ll demonstrate variables example data.Let’s begin loss/gain variable pair (can consider together, type information ; difference whether number refers potential loss potential gain). variables, meaning numbers corresponds common understanding numbers magnitude something. , magnitude potential loss potential gain. understand variables measuring magnitude potential loss potential gain lottery. larger number, larger potential loss gain. can treat variables numeric variables statistical analysis, () numbers variables represent numeric information, (b) performing mathematical operations, addition calculating average numbers, meaningful variable. example, useful calculate average potential loss/gain participant (.e., interpret average meaningful way, example comparing average loss/gain different condition).second example, let us consider subno variable. , numbers measure magnitude anything. Participant number 16 twice participant 8. just looking variable, also know values mean. described , one just needs assign unique numbers way participant. example, one assign number 1 first participant completed experiment, number 2 second, forth. Alternatively, one also assign number 1 first participant invited, number 2 second, forth. Another possibility specify maximum number participants, say 500 , just assign unique random number 1 500 one. Importantly, need know procedures used. reason subno variable know participant particular observation belongs . numbers subno serve purpose label identifying participant. Instead numbers, also use non-numeric labels, random strings letters, participant variable. Consequently, make much sense perform mathematical operation subno variable. example, average participant number provide useful information.purposes book, distinction two data types central: treat variable numerical variable categorical variable? statistical methods introduced following chapters can deal two types variables (categorical variables can generally also serve role explanatory variable outcome variable). can identify whether variable numerical categorical?Usually, easy identify categorical variables among variables numbers. Whenever numbers represent label, variable usually categorical variable. example, addition subno variable, numbers condition variable serve label identify condition. interpret numbers condition variable shown Table 2.1 actually representing numerical value either 20.2 40.2. Instead, four possible values variable, 20.2, 20.4, 40.2 40.4, refers one four conditions experiment, loss gains ranging either -$20/+$20, -$40/+$20, -$20/+$40, -$40/+$40 (note value decimal point range potential loss value decimal point range potential gains). non-numeric variables values labels, response variable, also clearly categorical variables.difficult decision resp variable. Clearly, two possible values response variable, accept reject lottery, response categories labels. However, transforming variable numbers 1 0, can perform meaningful mathematical operations . discussed , mean variable can interpreted average proportion lotteries accepted. generally, binary categorical variable (.e., categorical variable two categories) can seen special case treating numerical variable can certain situations meaningful. However, whether meaningful depends situation. example, want calculate proportion different observations one category, makes sense represent binary categorical variable numeric variable. contrast, one variables experimental design know often appeared, might need . general best explicitly treat variable categorical unless one sure treating numerical meaningful.sum , statistical purposes book distinguish numerical variables categorical variables. Numerical variables hold numerical information magnitudes something degree something holds. categorical variables values variable serve labels designating membership one number mutually exclusive categories. categorical variables part experimental design, later also call factors.","code":""},{"path":"research-designs.html","id":"assumptions-of-numerical-variables","chapter":"2 Data and Research Designs","heading":"2.2.2 Assumptions of Numerical Variables","text":"case resp variable (.e., numerical representation binary categorical variable) shows decision whether something numerical categorical variable can depend situation. help decision, helpful know exactly entailed treating variable numerical. statistical methods used , treat variable numerical assume represents continuous numerical information. means assume :certain difference interval meaning anywhere scale. example, difference 1 unit variable means whether add 10 20. can see holds loss/gain variable pair later discuss examples questionable assumption. corollary assumption calculating mean variable must meaningful . interpret mean, variable treated numeric.certain difference interval meaning anywhere scale. example, difference 1 unit variable means whether add 10 20. can see holds loss/gain variable pair later discuss examples questionable assumption. corollary assumption calculating mean variable must meaningful . interpret mean, variable treated numeric.variable can principle take real-valued (.e., decimal) number. , even though might used discrete values variables,13 loss/gain variable pair subset whole numbers 6 40 (see Table 1.1), statistical method assumes -values possible principle meaningful.variable can principle take real-valued (.e., decimal) number. , even though might used discrete values variables,13 loss/gain variable pair subset whole numbers 6 40 (see Table 1.1), statistical method assumes -values possible principle meaningful.can see, loss/gain variable pair, two assumptions fully satisfied. However, loss/gain variable pair actually outcome measured experiment. Therefore, statistical analysis play role variable important assumptions fulfilled. Instead, variable pair part design experiment. Consequently, let us consider example variables see well fulfil two assumptions numerical variable.example, numerical outcome variable data set resp. Clearly, resp fulfil assumptions two discrete outcomes, values 0 1. However, can calculate interpret mean (average proportion accepted). also assume specific difference, say 0.1 (10%) difference, means whether happens acceptance rate 50% acceptance rate 85%.14 whereas assumptions violated, others fulfilled. entails whether can interpret results analysis depends exact context circumstances. example, statistical analysis lead results predictions beyond probability range 0 1, clearly problematic results meaningful. words, learned little meaningful data statistical analysis.popular kind variable psychology related sciences involves subjective rating scales (also known “Likert scales”). example, discussed study McGraw et al. (2010) participants one condition asked rate intensity emotional reaction potential loss potential gain response scale ranging 1 = “Effect” 5 = “Large Effect” (see Figure 1.1, unipolar intensity scale). variable represent numerical variable? Clearly, value 5 represents emotional reaction larger value 1.variable represent magnitude, fulfil assumptions spelled ? can also take average scale interpret meaningful way. Specifically, average emotional intensity participants loss condition, 3.6, larger average emotional intensity participants gain condition, 3.1. However, questionable whether difference 1 means everywhere across scale. specifically, difference “Effect” “Small Effect” (difference 1 2) difference “Moderate Effect” “Substantial Effect” (difference 3 4)? Numerically , whether also holds psychologically question difficult answer. Like researchers McGraw et al. (2010) treated variable numerical variable made assumption (also implicit process calculating average). validity conclusions rests degree whether believe reasonable.Let’s try generalise conclusion previous paragraph order answer following question: Can treat variable numeric variable statistical model perfectly meets two assumptions stated ? ideal world (statistical otherwise!) answer yes. everyday realities data analysis always differ ideal. Many variables regularly encounter research violate one assumptions degree. Yet, still need treat numerical variables, simply treating categorical help us answering research questions. Whenever assumptions violated degree, can interpreted another instance epistemic gap (specifically instance first epistemic gap, Section 1.3.1). fact assumptions violated opens possibility alternative explanation results differs hypothesis. words, assumptions perfectly met, evidence provided statistical analysis stronger assumptions partially met.problem numbers treat numerical variable, computer treats numbers way, say, satisfy two assumptions stated . everyday language, “numbers don’t remember came ” (Lord 1953)15. – researchers – know numbers came need take account interpreting statistics. can also interpret insight terms concepts introduced previous chapter. numbers part operationalisation; establish procedure maps real world entities (called possible outcomes states affairs) onto values variables (many cases numbers). numbers emerge procedure related research question, identical research question. inference statistical results based numbers requires many auxiliary assumptions, one assume numerical variables continuous. can never sure auxiliary assumptions true, careful humble conclusions draw research.","code":""},{"path":"research-designs.html","id":"measurement","chapter":"2 Data and Research Designs","heading":"2.3 Measurement","text":"far, categorized different variables appear data set can integrate statistical analysis. , take step back consider principled manner variables created. discussing way measure different variables experiment influences can infer .","code":""},{"path":"research-designs.html","id":"measurement-scales","chapter":"2 Data and Research Designs","heading":"2.3.1 Measurement Scales","text":"One way interpret discussion , meaning numbers collect, relates first epistemic gap introduced previous chapter, difference research question operationalisation (Section 1.3.1). , epistemic gap can understood distinction actual magnitude latent construct (e.g., strength emotional intensity personal risk preference) measurement magnitude latent construct (.e., application procedure assigns number attribute observation). speaking measurement, latent construct also called attribute. Thus, concerned distinction attribute measurement. important theoretical contribution distinction within context psychology comes Stevens (1946). claimed can distinguish four different types measurement scales differ respect type relationship attributes reveal: nominal, ordinal, interval, ratio scales.describing four scales detail, let’s already make clear Stevens’ (1946) goal . hoped using particular measurement scale, bridge epistemic gap measurement attribute attribute . example, claimed measuring attribute using interval scale revealed attribute existed interval scale (see means just bit). claim (better: hope) probably reason idea still extremely popular. However, discussed , bridging epistemic gaps never easy also case. get back issue introducing four scales detail.nominal scale equivalent termed categorical variable, values exhibit quantitative relationship among . addition examples discussed , many demographic variables can understood nominal scale gender (e.g., male, female, non-binary, ) handedness (right-handed, left-handed, ambidextrous).scale constructed rank orderings attributes, called ordinal scale. consequence, can order attributes along dimension make quantitative distinctions. common example ordinal scale final result sports competition first place, second place, . important aspect ordinal scale differences values ordinal scale need correspond equivalent differences attribute measured ordinal scale. stay within sports competition example, difference first second place terms performance need difference second third place. example, 2020 Olympics 100-m women sprints final, difference first place (Elaine Thompson-Herah) second place (Shelly-Ann Fraser-Pryce) 0.13 seconds, whereas difference second third place (Shericka Jackson) 0.02 seconds. case difference ordinal rank scale (.e., one rank difference), correspond difference underlying attribute (.e., performance, time needed sprinting 100 m). demographic characteristics can also understood ordinal scale, education levels (e.g., primary secondary school education, compulsory education age 16, college, higher education professional & vocational equivalents).interval scale results differences, intervals, values attributes scale meaning across scale. Thus, interval scale can also understood fulfilling requirements numerical variable.16 typical example interval scale temperature measured either degrees Celsius (°C) Fahrenheit (°F). Within temperature scale, 1 degree difference meaning independent current temperature. Furthermore, scales can transformed . interval scales also makes sense calculate mean (e.g., mean temperature), calculating ratios make sense. example, saying 40 °C double temperature 20 °C really meaningful statement (e.g., 20 °C = 68 °F 40 °C = 104 °F \\(2 \\times 68 \\neq 104\\)).final scale type, ratio scale, results , addition maintaining meaning differences across scale, scale also contains true zero point attribute. stay within example temperature, whereas zero point degrees Celsius Fahrenheit arbitrary represent interval scales, zero point Kelvin scale, 0 K, lowest possible temperature making Kelvin scale ratio scale. Many physical scales ratio scale, length time. example, makes sense say sprinter took 20 seconds 100 m sprint took twice time sprinter took 10 seconds 0 seconds true zero point time.Whereas Stevens’ four measurement scales widely popular psychology related disciplines thanks prominence introductions textbooks, actual scientific contribution needs considered critically (following Michell 1997, 2002, 1999). Stevens proposes attempts bridge epistemic gap attribute measurement. According position, established interval ratio scale, learned underlying attribute exhibits interval ratio structure. Unfortunately, problem underdetermination discussed , learning actual structure theoretical construct attribute easy. numbers look like numerical variable, mean underlying attribute behaves like numerical variable. Therefore, recommend using four different measurement scales discuss psychological measures. example, book use theoretically neutral terms categorical numerical variables. discussed previous chapter, way description research remains level performed, level operationalisation statistical analysis.17Even though Stevens’ idea measurement scales ultimately untenable psychology, distinction nevertheless helpful allows us understand limits can learn data. can take away present discussion common measurement approach psychology related fields generally able establish ordinal relationships. example, subjective ratings scales, also choices among lotteries, represent ordinal relationships terms underlying latent attributes. Nevertheless, generally treat data variables numerical statistical analyses. reinforces point made interpret results statistical analyses directly answering research questions. statistical analysis generally makes assumptions nature underlying attribute construct verify.However, mean hope lost. Stevens (1946) already mentioned introducing ordinal scales, treating numeric always pointless (p. 679): “numerous instances leads fruitful results.” just mindful measurement psychology generally measurement physics interpreting results. primarily learn something operationalisations directly theoretical constructs use formulating research questions.","code":""},{"path":"research-designs.html","id":"reliability-and-validity","chapter":"2 Data and Research Designs","heading":"2.3.2 Reliability and Validity","text":"main message previous section measurement mind behaviour straightforward measuring physical attributes length. Nevertheless, aim use measures high quality. Two concepts important judging quality measurement reliability validity.Reliability refers consistency measure. One intuitive way understand reliability extent obtain value measure something conditions, different times. Reliability also inversely related noise measurement process. measure high reliability repeated measurements conditions lead similar outcomes (.e., level noise measurement process low). measure low reliability repeated measurements conditions lead widely different outcomes (.e., level noise measurement process high). example, consider ordinary bathroom scale. expect scale high reliability. get pretty much exact results step several times row, long change weight (e.g., drinking glass water).Validity refers strongly measure measures supposed measure. Within concepts introduced within book, validity thus refers ability measure bridge epistemic gap operationalisation construct actual (.e., “true”) value construct. Given difficulties defining even establishing constructs researchers interested , establishing whether measure high low validity generally difficult.One way visualise reliability validity given Figure 2.1 . , panel represents one operationalisation measure shot target represents one measurement measure. can see reliable measures low levels noise around one mean value (.e., low level dispersion shots). figure, validity visualised bias respect centre target. Valid measures centred target whereas invalid measures centred around -target value. figure highlights reliability validity principle distinct qualities. measure reliable mean valid. likewise, valid measure reliable.\nFigure 2.1: Visualisation reliability validity shots target. Figure taken “Statistical Thinking 21st Century” Russell . Poldrack (Figure 2.1).\nWhereas visualisation Figure 2.1 provide good first intuition reliability validity, conceptualisation validity bias way think . stay within metaphor provided figure, invalid measurement also one aims floor instead target. even something completely wrong, shooting darts goal shoot bullets. problem validity often don’t know better define measure hoping measure, assessing validity typically easy.previous paragraph already points important distinction reliability validity. Usually way quantify reliability, quantifying validity possible specific interpretations validity.common ways quantify reliability measure :Split-half reliability internal consistency refers reliability estimate results splitting measure sub-measures comparing scores across sub-measures (e.g., calculating score odd items comparing score even items). Generally, kind reliability can improved making measure longer (.e., adding additional items). Therefore, longer measures often reliable shorter measures (also intuitively makes sense – shorter measure likely noise plays important role).Split-half reliability internal consistency refers reliability estimate results splitting measure sub-measures comparing scores across sub-measures (e.g., calculating score odd items comparing score even items). Generally, kind reliability can improved making measure longer (.e., adding additional items). Therefore, longer measures often reliable shorter measures (also intuitively makes sense – shorter measure likely noise plays important role).Test-retest reliability refers extent get outcome applying measure different times. using measure questionnaire, difficulty calculating test-retest reliability possibility -called memory consistency effects temporal instability. Memory consistency effects refer observation participants often prefer self-consistent previous answers remember , thus potentially artificially increasing reliability. Temporal instability, hand, can result artificially lower reliabilities measured construct varies naturally time (mood).Test-retest reliability refers extent get outcome applying measure different times. using measure questionnaire, difficulty calculating test-retest reliability possibility -called memory consistency effects temporal instability. Memory consistency effects refer observation participants often prefer self-consistent previous answers remember , thus potentially artificially increasing reliability. Temporal instability, hand, can result artificially lower reliabilities measured construct varies naturally time (mood).Inter-rater reliability applies measures outside judge used make measurement. Prominent examples situations one can calculate inter-rater reliabilities medical diagnoses (.e., comparing diagnosis across multiple doctors) essay marks (marked multiple independent markers).Inter-rater reliability applies measures outside judge used make measurement. Prominent examples situations one can calculate inter-rater reliabilities medical diagnoses (.e., comparing diagnosis across multiple doctors) essay marks (marked multiple independent markers).quantify validity need external criterion also measures construct interest. criterion, can compare value criterion value measure gives us estimate criterion validity measure. example, previous chapter discussed exist different tasks questionnaires measuring individuals’ risk preferences. Suppose also access persons’ financial history showing degree invest money relatively high risk financial instruments (e.g., stock options), medium risk ones (e.g., individual stocks), lower risk ones (e.g., index funds). risk preference measure high criterion validity one participants high score measure invest actual money high-risk investments. discussed previous chapter, unclear measure risk preferences exists.types validity non quantifiable. Construct validity usually considered important type validity refers empirical theoretical support provides evidence measure measures supposed measure. construct validity essentially asks well measure bridges first epistemic gap, abstract concept one talks measures, rather actual criterion used describe measures fact, aware psychological measure construct validity.Another interesting type validity face validity. refers degree test appears measure supposed measure. Whereas face validity objectively important validity test (.e., matters test looks like measures supposed measure, ), can important commitment participants. example, participants intrinsically interested contributing time effort research question, measure high face validity (.e., one matches interest) reduce participant drop compared measure lower face validity.Considerations reliability validity incorporated overall assessment results judging empirical evidence provided given study. example, dependent variable study consists single item response, generally implies low internal consistency compared dependent variable based items responses. case, level noise relatively high, influence strongly weigh evidence study.","code":""},{"path":"research-designs.html","id":"independent-and-dependent-variables","chapter":"2 Data and Research Designs","heading":"2.4 Independent and Dependent Variables","text":"addition distinguishing type information variables can contain, can also distinguish different roles variables can play research process. Remember, discussing operationalisation step research process, identified need identify relevant variables hope can address research question, well empirical hypothesis involving least two variables. see , can assign two different roles (minimum) two different variables.Usually, single variable represents main result interested. psychology, called dependent variable. Synonyms “dependent variable” common statistical literature response variable, outcome variable, criterion. dependent variable main outcome study, variable primarily interested measuring.variable(s) called independent variable(s) psychology, believe values dependent variable depend values independent variable(s). popular synonym “independent variable” statistical literature predictor covariate, dependent variable assumed covary independent variables.18 , also used term explanatory variable describe independent variables. Loosely speaking, can describe distinction study effect independent variable dependent variable.19Let’s look distinction loss aversion study Walasek Stewart (2015), data shown Table 2.1 . hypothesis matters people’s preference symmetric 50-50 lotteries absolute value potential loss gain, relative rank lotteries compared lotteries. operationalisation research question involved manipulation range lotteries one variable, condition, measuring participants’ responses symmetric lotteries response variable. distinction variable types relatively straight forward. interested effect condition response: impact different ranges lotteries manipulated across conditions, participants’ responses. makes condition independent variable response dependent variable.general, distinction independent dependent variables easy understand experiment, example discussed . experiment, independent variables manipulated, condition. “Manipulated” means assign participants different conditions, rather measuring condition participant .studies can experiments independent variable manipulated. example, common research question effect demographic variable outcome. However, demographic variables readily manipulated assigned participants. example, might interested studying effect parental wealth children’s educational attainment. Whereas manipulating parental wealth principle possible, common approach measure variable well children’s educational attainment. Nevertheless, can still make distinction independent variable (parental wealth) dependent variable (educational attainment).variables experiment neatly fall within distinction independent dependent variables. example, let us go back six variables shown Table 2.1 make full data set collected study Walasek Stewart (2015). discussed , response/resp variable pair dependent variable condition independent variable. leaves us three variables need classified. go try classify three variables continuing reading.Let’s begin loss/gain variable pair determines possible outcomes lottery shown participants. Clearly, also manipulated. specifically, condition determines exactly lotteries therefore values loss/gain variable pair participant works . Thus, loss/gain variable pair also independent variables jointly determine independent variable condition. words, condition variable data set determine loss/gain variable pair. means determining independent variables study depends perspective one takes. focus main research question symmetric lotteries condition independent variable. However, also look lotteries individually, condition loss/gain variable pair independent variables.Table 2.1 contains one variable, subno, participant identifier. might seem bit surprising think variable terms independent dependent variables, able classify variable somehow. Clearly, subno dependent variable. interested values results subno variable. However, also seems clearly independent variable. specific expectations ideas different subjects affect response variable. help us classification, recall subno variable data first place – collect data multiple participants need identify participant observation belongs. follow-question : collect data multiple participants? discussed previous chapter (Section 1.3.2), participants source noise experiment different participants can multitude reasons. data one participant, distinguish idiosyncratic noise participant signal interested . collecting data multiple participants, try control noise averaging hope remains signal. overarching idea noise unsystematic effect results; participants may likely show particular behaviour whereas participants may less likely show behaviour. average noise cancels . sum, collect data multiple participants control noise inevitable dealing real people. Thus, say subno control variable. even specific say control variable independent variable, use control noise level design. situations might measure variable control purposes case call control variable dependent variable.sum section : Jointly, dependent independent variables key components operationalisation research question. central concepts make study design link practical reality research (.e., research actually takes place measured) research question. wrong say study defined primarily dependent independent variables. designing one’s study, making clear dependent independent variables maybe important decision decided research question. Likewise, reading scientific article describing study, understanding clearly independent dependent variables central understanding study. Therefore, whenever thinking talking research, make sure clear dependent independent variables . experimental research often boils asking: task participants? variables part study usually serve control purpose can denoted control variables.","code":""},{"path":"research-designs.html","id":"experimental-versus-observational-variables","chapter":"2 Data and Research Designs","heading":"2.5 Experimental versus Observational Variables","text":"seen discussing distinction independent dependent variables, can distinguish different types independent variables research designs, namely experimental non-experimental independent variables. adopt common terminology use observational variable describe non-experimental independent variables. study solely consists experimental variables (drop “independent” part now experimental observational variables always independent variables), can call experimental study experiment short. study solely consists observational variables, can call observational study. study contains experimental observational variables, agreed upon name depending variable relevant research question, researchers tend use either experimental observational study. However, experimental variables provide number evidential benefits discussed , tendency call study experiment even also contains observational variables. Depending actual situation inferences drawn, can seen stretch.experimental variable one control , can manipulated researcher. means values variables can assigned participants researcher.example, study Walasek Stewart (2015), researchers assigned participant one four conditions corresponding different range potential losses gains. Likewise, previous chapter, briefly introduced study Hinze Wiley (2011) generality testing effect. initial reading piece text, participants assigned one three experimental conditions: control condition re-read materials, testing condition using open-ended questions, testing condition using fill---blank text. studies, researchers decided condition participant part .important part experiment variable participants can assigned different conditions, assigned. specifically, experimental variable, assignment needs performed randomly; say participants randomised available conditions. One way understand random assignment experiment takes place, probability experimental conditions needs every participant.20For example, random assignment means every participant study Walasek Stewart (2015), probability four conditions 0.25 (.e., 1/4). every participant study Hinze Wiley (2011), probability three conditions approximately 0.33 (.e., 1/3).can imagine randomisation actual physical process produces random outcome, toss coin throw die. example, study Hinze Wiley (2011) imagine every participant takes part experiment, researcher (research assistant) throws regular six-sided die. die lands 1 2 participant assigned re-reading condition, die lands 3 4 participant assigned open-ended question condition, die lands 5 6 participant assigned fill---blank condition. Alternatively, pre-specify sample size want ensure every group approximately size, use different approach. prepare many sheets paper number participants want collect. sheet write one condition among sheets, condition appears equally often. , shuffle sheets bowl randomise order. performing experiment, take one sheet bowl every participant (without putting back) assign participant condition written sheet. Back day, common practice, nowadays, randomisation mostly done computer using -called random number generators.Different experimental variables observational variables, variables control researcher randomly assigned participants. words, whenever randomisation impossible, independent variable observational variable.talked demographic characteristics, particular parental wealth, independent variable. already described, demographic variables generally randomly assigned mostly immutable part person. Consequently, demographic characteristics (e.g., age, gender) generally observational variables. true many psychological characteristics person personality traits (e.g., extraversion) abilities (e.g., intelligence quotient). vast majority cases, variables observational variables.21At point might wonder experiment necessarily entails randomisation independent variable. benefit experimental observational variable? reason randomisation allows drawing causal inferences study. experimental – randomised – independent variable can say independent variable cause dependent variable. Without randomisation (.e., dealing observational independent variable) inference permitted.Remember , said one way think distinction dependent independent variables want learn effect independent variable dependent variable. specify meant “effect,” say way think dependent independent variables holds loosely. fact, say “effect,” mean causal relationship; changes independent variable cause changes dependent variable. absence cause-effect relationship, seems wrong speak “effect independent variable.” can now see reason said holds loosely, holds independent variable experimental variable, observational variable.","code":""},{"path":"research-designs.html","id":"epistemic-gap-3-causal-inference-and-confounding-variables","chapter":"2 Data and Research Designs","heading":"2.5.1 Epistemic Gap 3: Causal Inference and Confounding Variables","text":"reason experimental variable allows causal inference due another epistemic gap, possible influence confounding variables dealing observational data. causal inference means learn independent variable nothing else responsible effect observed dependent variable. causal inference possible plausible alternative explanations observed effect dependent variable involve independent variable can ruled . context causal inference, alternative explanation known confounding variable confounder short. randomly assign participants conditions, can theoretically rule confounders alternative explanation. However, case observational variable ; may reasons, confounders, related observational variable responsible observed effect. Similar first epistemic gap, inference independent variable cause effect dependent variable underdetermined observational variable, sofor experimental variable.22Let us exemplify problem new example. Imagine want investigate effectiveness novel drug control treatment (.e., old drug) treatment viral infection hospital setting. independent variable treatment (control treatment versus novel drug) dependent variable viral load (.e., whether virus can still detected patient). Let us imagine results show new drug effective control treatment. , patients show lower viral load (.e., less sick) novel treatment control treatment. question trying answer now whether matters inference can draw study assignment treatment condition random .Let us begin considering non-random assignment independent variable. example, one way implement non-random assignment use novel drug one hospital control treatment another. run study, allow us conclude difference dependent variable due differences treatment? inference allowed two hospitals identical. systematic differences hospitals, say patients hospital control treatment average older patients hospital new treatment (patients drawn different neighbourhoods), difference responsible difference dependent variable.systematic difference age hospitals plays role confounder. average age patients differs two hospitals, age may also responsible difference dependent variable, older patients less likely recover thus higher viral load end study young patients. situation confounder present, infer treatment cause observed effect. logical structure indicative underdetermination: two conditions differ terms two characteristics, treatment status age, either () can responsible differences dependent variable.Let’s now consider situation participants randomly assigned two treatment conditions. , data two hospitals, participants hospital randomly assigned treatment conditions. example, new patient shows relevant symptoms, doctor administering treatment takes pre-randomised envelope contains either old drug novel drug.23 assignment conditions random, expect confounders age balanced across two conditions (course checked afterwards). Every participant comes two hospitals chance either get control treatment novel drug, independent age characteristics. Consequently, long randomness introduce accidental confounding (see Section 1.3.2) can attribute effect dependent variable independent variable.summary, difference experimental observational variable degree can rule possible alternative explanations. experimental variable participants randomly assigned conditions, theory, confounding variables balanced. long randomisation proceed planned, one can certain alternative explanation effect independent variable dependent variable random chance. always might get unlucky confounding variable just happens unbalanced data set. However, larger sample sizes larger effects, explanation due chance becomes increasingly unlikely. experimental variables, epistemic gap wanting judge whether independent variable genuinely responsible effect independent variable dependent variable, effect random chance noise (see Section 1.3.2).example, main study led approval Biontech Covid-19 vaccine (Polack et al. 2020), 40,000 participants randomly assigned either receive real vaccine placebo (saline injection without active ingredients). Randomisation done equal proportions conditions 20,000 participants. Among participants received vaccine, 8 participants developed Covid-19. Among participants received placebo, 162 participants developed Covid-19. Whereas results definitely rule confounding variable explains difference contracting Covid-19, seems extremely unlikely. Participants trial recruited six different countries (e.g., USA, Turkey, Brasil) diverse demographic characteristics (e.g., sex, ethnicity, age, weight), characteristics extremely similar participants conditions (Polack et al. 2020, Table 1).observational variable participants randomly assigned condition, know whether potential confounding variable. One way address problem measure known confounding variables show responsible difference dependent variable. even able control measure large number possible confounding variables, can never certain another unobserved confounding variable responsible effect. observational variables always deal two epistemic gaps wanting judge whether independent variable responsible effect independent variable dependent variable: problem possible confounders plus random chance noise.end section, let’s come back example trial testing new drug hospital setting. lengthy discussion observational versus experimental variables, can hopefully see idea administering new drug one hospital control treatment another hospital bad idea. Without proper randomisation participants treatments, inference drug responsible effect viral load seems weak, thanks possible influence confounders. might even go far wonder ever run study without proper randomisation, believe corresponding results.Sadly, study pretty much exactly sketched – administering novel drug one hospital control treatment another hospital, patients systematically differing hospitals – played unfortunate role Covid-19 pandemic. particular, first study suggest Hydroxychloroquine effective Covid-19, study Gautret et al. (2020), exactly problem.24 Whereas critics quick point problems study (Bik 2020; Rosendaal 2020; Sayare 2020), damage done. current US president Donald Trump praised Hydroxychloroquine wonder cure Covid-19. required much scientific effort follow-studies, using resources potentially used productively elsewhere, show (full timeline events see Sattui et al. 2020). problem case whereas medical statistical experts immediately see problems study, general public . false claim appears scientific established public discourse (.e., within media reporting along lines “researchers shown …”), often difficult combat . general seems discussing empirical evidence provided particular scientific study either beyond expertise available mass media unwilling invest time commitment .","code":""},{"path":"research-designs.html","id":"is-causal-inference-from-observational-data-possible-at-all","chapter":"2 Data and Research Designs","heading":"2.5.2 Is Causal Inference from Observational Data Possible at All?","text":"previous section argues causal inference generally possible experimental independent variables. observational variables, can always confounder responsible effect instead independent variable. However, many interesting research questions investigated experiments, observational variables. discussed , demographic variables immutable features individuals, personality traits, observational variables definition. Likewise, many variables relating lifestyle choices, dietary exercise habits, might principle amenable experimental manipulations, reality seems difficult impossible completely unethical run corresponding experiments. mean draw causal inferences research questions? believe honest realistic answer vast majority cases . eyes fair assessment situation , causal inference observational data literally difficult problem empirical sciences.25Importantly, causal inference observational data primarily statistical problem. introduced problem confounders pose epistemic gap. epistemic gaps, overcoming epistemic gap requires diverse conceptually strong evidence. statistical methods can assist providing evidence, provide type compelling evidence needed. problem even observational data strongly suggests something, always possibility confounder missed adequately taken account.example problem, let us consider case vitamin supplements, specifically vitamin C E supplements (Lawlor et al. 2004; Woodside et al. 2005; Mozaffarian, Rosenberg, Uauy 2018). Early evidence large observational studies 1990s tens thousands participants suggested taking vitamin C E supplements reduces chance getting cancer cardiovascular diseases considerable degree. Based positive results, large scale experiments (.e., also tens thousands participants) followed participants randomly assigned either take vitamin supplements placebo (sugar pill without vitamins) monitored several years. large, experiments replicate positive effects found observational studies. Unless individual susceptible vitamin deficiency, vitamin supplements appear measurable health benefit. probable reason difference observational studies experiments likely due insufficient adjustment socioeconomic status confounder. often found, participants higher socioeconomic status healthy (.e., less likely develop cancer cardiovascular diseases) also likely take vitamin pills (believed helpful). Whereas observational studies measured tried account differences socioeconomic status participants already take vitamin supplements , partially [e.g., account differences socioeconomic status parents participants led developmental differences also affected probability developing cancer cardiovascular diseases well probability taking vitamin pills; Lawlor et al. (2004)].example shows even situation confounder principle known (.e., socioeconomic status) observational data sets large (> 10K participants), causal inference observational data possible. Even attempting account confounding, observational data suggested relationship turned spurious. apparent problem accurately measuring influence confounder possible, knowing observed relationship fact spurious. experiment able reveal effect vitamin supplements. example suggests many research questions data sets common psychology related disciplines, causal inference observational data equally difficult even impossible. Extra complications data sets often considerably smaller less known causal relationships existing domain (.e., variables act confounders).consequence problem observational data, current book primarily focuses experimental data sets , non-experimental variables considered, limitations discussed. Whereas focussing experimental data restricts type research questions can investigated, least eliminates one three epistemic gaps introduced . also means applying methods introduced observational data sets require additional care trying draw justifiable conclusions recommended. repeat said , question whether effect found observational data reflects causal relationship statistical question. statistical tools introduced can determine effect explained, provide answer question whether relationship observational data causal.researchers interested analysing observational data, good introductory literature attempts approach problem confounders principled manner Rohrer (2018), Gelman, Hill, Vehtari (2021), McElreath (2020), Hernán Robins (2021), Shadish, Cook, Campbell (2002). Note , given additional epistemic gap needs bridged, methods advanced methods introduced , require advanced technical mathematical knowledge use). , drawing causal inferences observational data literally difficult problem empirical sciences.","code":""},{"path":"research-designs.html","id":"internal-external-validity","chapter":"2 Data and Research Designs","heading":"2.5.3 Internal versus External Validity","text":"talked validity context measurement. context, question validity whether measure measures supposed measure (e.g., risk attitudes questionnaire really measures risk attitudes high validity). However, term “validity” also used context experimental versus observational studies. context, two relevant types validity internal validity external validity. terms refer specific measure, used describe complete studies research designs. provide brief introduction two terms , see Shadish, Cook, Campbell (2002).26Internal validity refers internal structure study reflects degree study provides evidence causal relationship independent dependent variables. means , generally speaking, internal validity high study experiment (.e., independent variable randomised) internal validity low study observational study.27 Within terminology epistemic gaps introduced book, internal validity related third epistemic gap. can sure possible confounders internal validity high, can sure case experimentally manipulated independent variable.External validity refers degree results study generalise different settings, different situations, people, stimuli, times. Within terminology epistemic gaps introduced book, external validity related first epistemic gap, underdetermination theory data. degree can sure results really address research question confined specifics operationalisation, can sure results generalise situations. words, learn causal link holds specific circumstances tested within study, actually hold general terms research question formalised, external validity low. example, study Hinze Wiley (2011) introduced Chapter 1 directly addressed external validity seeing whether testing effects also hold different operationalisation “testing.”","code":""},{"path":"research-designs.html","id":"summary-1","chapter":"2 Data and Research Designs","heading":"2.6 Summary","text":"chapter introduced number important concepts allow us describe studies research designs. began highlighting , empirical scientists, ultimate arbiter whether believe theoretical position hypothesis empirical evidence. evidence come systematically collected data sets anecdotes.Data sets can used address research questions consist independent variable(s) usually one dependent variable. distinction assume dependent variable depends independent variable. independent variable experimental variable participants randomised conditions, can even infer independent variable causally responsible effect dependent variable. independent variable solely observational variable, generally make causal judgement.reason observational variables allow causal inferences lies third epistemic gap introduced . observational variable, can always different confounding variable responsible , effect independent variable dependent variable.Together three epistemic gaps put clear limits can learn empirical data psychology related disciplines. first epistemic gap, underdetermination theory data, difference research question operationalisation research question. Whereas operationalisation attempts address research question, usually . second epistemic gap, signal versus noise, concerns relationship operationalisation statistical analysis. Even statistical analysis appears provide support empirical hypothesis, 100% sure . always chance observed outcome just occurred chance – noise – represent genuine signal data. Finally, third epistemic gap, confounding variables, always present dealing observational independent variables. just summarised, absence randomisation can never really sure independent variable confounding variable reason observed effect. Thus, can reiterate message ended previous chapter. interpret statistical results, need careful humble conclusions draw.also introduced different data types can deal statistical analysis. Independent variables can numerical categorical variables. independent variable categorical, generally call experimental factor, just factor. Dependent variables can generally numerical variables, unless binary categorical variable treating numerical.also argued genuine psychological variables collect, responses rating scales, ordinal scale satisfy assumptions numerical variable. However, analyses nevertheless treat numerical. violation statistical assumption places limits inferences permitted studies. line , argued measurement psychology generally difficult problem simply assuming measures provide information actually provide another inferential problem deal .","code":""},{"path":"chapter-2-quiz.html","id":"chapter-2-quiz","chapter":"Chapter 2: Quiz","heading":"Chapter 2: Quiz","text":"Note: pull-menu selecting answer turns green selecting correct answer.Exercise 2.1  mean statistical assumptions perfectly met?results statistical analysis true.results statistical analysis likely true.evidence provided statistical analysis stronger assumptions met.Answer: 123Exercise 2.2  one following good way checking quality measurement?ReliabilityValidityOperationalisationAnswer: 123Exercise 2.3  mean measure low reliability?means reliability analysis conductedIt means repeated applications conditions lead widely different outcomesIt means level noise measurement lowAnswer: 123Exercise 2.4  one following measure reliability?Test-retest reliabilityInter-rater reliabilitySplit-half reliabilityCriterion reliabilityAnswer: 1234Exercise 2.5  one synonym dependent variable?Result variableOutcome variableResponse variableCriterionAnswer: 1234Exercise 2.6  randomisation important experimental studies?allows researchers theoretically rule confounders cause effect observed dependent variableIt allows researchers run statistical analysisIt allows researchers correctly choose dependent independent variablesAnswer: 123Exercise 2.7  true numbers?\n1. Numbers can represent categorical information\n2. Numbers can represent numerical information\n3. apply mathematical operation numbers, can interpret results independent meaning numbersAnswer: 123Exercise 2.8  one type variable research design?\n1. Dependent variable\n2. Epistemic variable\n3. Control variableAnswer: 123Exercise 2.9  mean data presented wide format?\n1. data one participant spans single row\n2. data one participant spans multiple rows\n3. one row per observationAnswer: 123Exercise 2.10  mean variable represents ‘continuous numerical information?’\n1. variable can principle take real-valued (.e., decimal) number\n2. certain difference interval meaning anywhere scale\n3. answers correctAnswer: 123","code":""},{"path":"tidyverse-intro.html","id":"tidyverse-intro","chapter":"3 Short Introduction to R and the tidyverse","heading":"3 Short Introduction to R and the tidyverse","text":"previous two chapters provided theoretical conceptual background need performing statistical analysis. chapter switch focus practical side introduce basics tool use perform statistical analysis practice, statistical programming language R.discussed previous chapters, value scientific theory hypotheses depends ultimately evidence supports . evidence comes form data. Thus, able judge strength evidence provided data, central task research practice analyse interpret data. R tool use task. one comprehensive popular tools aspects data analysis.","code":""},{"path":"tidyverse-intro.html","id":"what-is-r","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.1 What is R?","text":"R comprehensive tool enables skilled user perform steps tasks data analysis . example, R can:Prepare data analysis: read data comes pretty much format, manipulate wrangle data.Explore data using summary statistics graphical summaries: exploratory data analysis, descriptive statistics, data visualisationPerform statistical analysis data: inferential statisticsCommunicate results: publication-ready results graphics, research reports combine narrative text statistical resultsAnd much : data simulations advanced statistical methods, machine learning, interactive data visualisations, websites, books (present one)top incredible list things can done R, R free software (sometimes also known open source software). means, R completely free download, install, use, also free inspect source code make changes (long make version R un-free).Given flexibility things can R, completely surprising requires effort get started R. Especially R first real experience learning programming language.important thing know new R user beginning hard almost everyone (including present author), using R gets lot easier smoother time effort dedicated learning basics. important message hang keep trying. likelihood, least rather frustrating situations first weeks interacting R, get better. taught R many users great variety backgrounds experiences many struggled way beginning. anyone kept hopes continued put time effort, struggles vain. can learn R just believe give . assured believe . Learning R incredibly powerful skill surely positive effect whatever comes later life, career Academia (.e., university) “real world” (academics like call everything university job).","code":""},{"path":"tidyverse-intro.html","id":"getting-started-with-r","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.2 Getting Started with R","text":"somewhat scary introduction way, next important question probably get going R? Providing comprehensive answer question beyond scope present work. Instead, point freely available resources expect go continuing rest chapter get speed R first. begin list minimum requirements work continuing. probably take time, must done. , provide links additional resources can also take look , either now later time.","code":""},{"path":"tidyverse-intro.html","id":"step-1-installing-r-and-rstudio","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.2.1 Step 1: Installing R and RStudio","text":"basic R software performs calculations can installed CRAN, Comprehensive R Archive Network. However, basic R installation quite bare bones, especially people used modern operating systems like Windows macOS. Therefore, addition R, also recommend install RStudio. RStudio popular IDE – integrated development environment – R, makes using quite bit comfortable.Note R RStudio need updated independently . Especially R updated least per year CRAN. , already R installed computer remember last time updated, now probably good time . personally update R usually within weeks new version appearing.need additional help installing R RStudio, Danielle Navarro created set video tutorials installing R. link brings Youtube playlist specific videos major operating system.","code":""},{"path":"tidyverse-intro.html","id":"step-2-getting-comfortable-with-r","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.2.2 Step 2: Getting Comfortable with R","text":"next step get going first steps R. best way work Part 1, “core toolkit”, R Psychological Science also created Danielle Navarro. Part 1 awesome online resource covers R installation, variables data types, scripts, packages, basic programming concepts, loops, branches, functions. minimum requirement continuing chapters 1, 2, 3, 4, 5, 6, 11). However, highly encourage work whole Part 1.addition Part 1, also recommend go least first chapters Part 2, “working data”, R Psychological Science. covers complex data types, importantly R’s central data type, data.frame, already provides introduction tidyverse, also used . recommend least go prelude data types (pay special attention data frames). minimum requirement data.frame chapter (Chapter 13)., risk repeating , please go resources listed (.e., Part 1 Part 2, R Psychological Science) reading chapter. work least minimum requirements understand follows.","code":""},{"path":"tidyverse-intro.html","id":"optional-step-3-file-system","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.2.3 Optional Step 3: File System","text":"working R (really programming language) generally work files folders. yet super comfortable , recommend also look Danielle Navarro’s videos project structure. provide great overview introduction topic fundamental working computers beyond R: working file system. covers naming files, file paths, folders, related technical stuff important programming, often taught explicitly.","code":""},{"path":"tidyverse-intro.html","id":"additional-optional-resources","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.2.4 Additional Optional Resources","text":"sections list minimum requirements go continuing chapter. resources enough , section provides overview additional free resources can take look now later get deeper R (.e., feel free skip following bullet points continue ).MSc Conversion: R Research Methods book Emily Nordmann book accompanying video lectures. book aimed similar audience present book. However, focuses concepts use (e.g., RMarkdown, accessing R runs server). book part psychTeachR book plus video series developed University Glasgow contains interesting books Data Skills: psyTeachR Books introductory.MSc Conversion: R Research Methods book Emily Nordmann book accompanying video lectures. book aimed similar audience present book. However, focuses concepts use (e.g., RMarkdown, accessing R runs server). book part psychTeachR book plus video series developed University Glasgow contains interesting books Data Skills: psyTeachR Books introductory.Learning Statistics R, also Danielle Navarro (can see, big fan Danielle’s work). completely free introductory book statistics using R, can downloaded website (currently available version 0.6). Part II (Chapters 3 4) provides gentle comprehensive introduction R newcomers. installing R RStudio, navigating console RStudio windows, basic data types, reading data file system, important data types, data.frames, roughly 70 pages (.e., pp. 35 - 109) covered. look two chapters, Chapter 8 great next step introduces R scripts. completely new statistics, Chapter 5 also provides great introduction important concepts. One downside resource comes form PDF website, read comfortably devices (great printing). Also note R Psychological Science updated version book, probably start first. Finally, note present book somewhat different conceptual focus introducing statistical tests methods (.e., especially compared Part IV).Learning Statistics R, also Danielle Navarro (can see, big fan Danielle’s work). completely free introductory book statistics using R, can downloaded website (currently available version 0.6). Part II (Chapters 3 4) provides gentle comprehensive introduction R newcomers. installing R RStudio, navigating console RStudio windows, basic data types, reading data file system, important data types, data.frames, roughly 70 pages (.e., pp. 35 - 109) covered. look two chapters, Chapter 8 great next step introduces R scripts. completely new statistics, Chapter 5 also provides great introduction important concepts. One downside resource comes form PDF website, read comfortably devices (great printing). Also note R Psychological Science updated version book, probably start first. Finally, note present book somewhat different conceptual focus introducing statistical tests methods (.e., especially compared Part IV).prefer introduction stronger programming focus, recommend free book Hands-Programming R Garrett Grolemund. Chapters 1 5 (pp. 1 - 99) also provide introduction starts installing R gentle manner introduces necessary basic concepts including different data types data.frames.prefer introduction stronger programming focus, recommend free book Hands-Programming R Garrett Grolemund. Chapters 1 5 (pp. 1 - 99) also provide introduction starts installing R gentle manner introduces necessary basic concepts including different data types data.frames.","code":""},{"path":"tidyverse-intro.html","id":"rstudio","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.3 RStudio","text":"comfortable R, let us take quick look RStudio together. Figure 3.1 shows basic RStudio interface empty R script opened. yet look like , open empty R script “File” - “New File” - “R script.”\nFigure 3.1: Screenshot RStudio window empty R script default layout. script shown top left, R console bottom left, Environment pane top right, Files Plot viewer bottom right.\nscreenshot shows four panes default location. R script top left, R console bottom left, environment history pane top right, files plot pane bottom left.alternative default location possible switching position console environment pane. , click small button left Addins just menu bar. button looks bit like version windows logo. clicking button can choose default “console left” alternative “console right.” Especially work lot R scripts, , makes sense put console right, shown Figure 3.2.\nFigure 3.2: Screenshot RStudio window empty R script alternative layout. image, console environment pane switched position allows R script console occupy larger space.\nbenefit using R script can come back code change rerun . Even plan save R script, development often easier script directly console. Furthermore, one great thing R scripts RStudio can send commands R script directly R console using keyboard shortcut: Ctrl + Enter Windows/Linux CMD + Enter Mac. , indicate options like : Ctrl/CMD + Enter. Try typing something R script window (e.g., 7 + 3) send console using shortcut. Let us now R.","code":""},{"path":"tidyverse-intro.html","id":"a-base-r-example-session","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.4 A Base R Example Session","text":"discussed previous chapter, important format data representation tabular format column representing single variable typically one row per observation. data represented base R data.frame, important data format needs. use term “base R” refer using R without additional packages. Let’s quickly recap data.frame base R looks like, basic operations . also set stage using R scripts.","code":""},{"path":"tidyverse-intro.html","id":"files","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.4.1 Data Files, Scripts, and Working Directory","text":"chapter, mainly working data Walasek Stewart (2015), introduced detail Section 1.2.2. data consists two data files, representing separate experiment, discussed together far, exact replications one another. Walasek Stewart (2015) reported Experiments 1a 1b. original data files sent Lukasz Walasek Excel files (can found : Experiment 1a Experiment 1b).base R allow read Excel files, opened original data files Walasek Stewart (2015) Excel saved .csv files (comma separated value files) can downloaded : Experiment 1a Experiment 1b. recommend download files save folder can access within R. problem downloading files, try clicking links right mouse button (Windows/Linux) CMD + click (Mac OS) select “Save link …” (something similar ) menu appears. Note click .csv files computer, open Excel (equivalent program), case wanted look outside R.particular, recommend following steps can follow along upcoming R code:Save downloaded data files folder named data current R working directory.Create new empty R script save working directory.example, say already created folder book/class, let’s assume folder called stats_r_intro-stuff easy find location (e.g., depending operating system Documents folder home folder ~). Let’s assume want folder working directory. , create new folder folder, called data (.e., stats_r_intro-stuff/data), download two data files (Experiment 1a Experiment 1b) folder. Next create empty R script folder chapter, say tidyverse-intro.R (.e., now stats_r_intro-stuff/tidyverse-intro.R).can now easily set current R session folder working directory opening tidyverse-intro.R RStudio (yet opened) using menu: Session - Set Working Directory - Source File Location. sets working directory folder (.e., location) currently active R script located, stats_r_intro-stuff folder. Note RStudio open, double-click R script file, RStusio open use folder R script file working directory (works RStudio already running).Note, using current R session already time, now good time restart session (necessarily RStudio) using menu : Session - Restart R.workflow laid previous bullet points represents general recommendation working R stage. folder designated project folder. folder R scripts well sub folder data. can open script RStudio set folder working directory menu (.e., Session - Set Working Directory - Source File Location). can access data files simply accessing data/-data.file.csv. Importantly, don’t forget restart R session starting something new menu: Session - Restart R. follow steps find different workflow works better . important thing know working directory files .","code":""},{"path":"tidyverse-intro.html","id":"read-data-and-inspecting-it","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.4.2 Read Data and Inspecting it","text":"set-steps way, can now load data Experiment 1a using base R’s read.csv() function:read data file without warnings problems.28 reason fail download file fail set correct working directory, can also try read directly internet following code. please tried downloading dealing actual file. Handling data files working directories important R skill need acquire.first thing usually want data.frame inspect structure using str() function, lists variables, data types, number observations variables.present case 20 thousand observations six variables. variables int means integer; , numeric variable consisting solely whole numbers (.e., discrete values). also one num – , numeric – variable (.e., numeric variable decimal values), condition. general, int num variables treated way, numeric variables, hardly ever reason transform one . Finally, condition chr character variable.","code":"\ndf1a <- read.csv(\"data/ws2015_exp1a.csv\")\ndf1a <- read.csv(\"https://github.com/singmann/stats_for_experiments/raw/master/data/ws2015_exp1a.csv\")\nstr(df1a)\n#> 'data.frame':    22912 obs. of  6 variables:\n#>  $ subno    : int  8 8 8 8 8 8 8 8 8 8 ...\n#>  $ loss     : int  6 6 6 6 6 6 6 6 8 8 ...\n#>  $ gain     : int  6 8 10 12 14 16 18 20 6 8 ...\n#>  $ response : chr  \"accept\" \"accept\" \"accept\" \"accept\" ...\n#>  $ condition: num  20.2 20.2 20.2 20.2 20.2 20.2 20.2 20.2 20.2 20.2 ...\n#>  $ resp     : int  1 1 1 1 1 1 1 1 0 1 ..."},{"path":"tidyverse-intro.html","id":"transforming-categorical-variables-into-factors","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.4.3 Transforming Categorical Variables into Factors","text":"discussed Section 2.2, numeric variables well condition actually categorical variables, factors R parlance. can transform variables factors using aptly named factor() function. generally use factor() instead functions (e.g., function .factor()) allows us specify ordering factor levels potentially labels factor levels. Let us three variables now, subno, response, condition. use $ operator access variables data.frame. transforming variables take another look structure data.frame using str().see running commands, expected three factors data. Let us take look calls factor() understand call different manner case.call subno passes variable (.e., df1a$subno vector) arguments factor(). consequence, factor levels ordered alpha-numerical increasing manner.call subno passes variable (.e., df1a$subno vector) arguments factor(). consequence, factor levels ordered alpha-numerical increasing manner.call response specifies ordering factor levels using levels argument passed vector levels, c(\"reject\", \"accept\") (.e., remember, c() function creating vectors kind, character vectors ). reason otherwise factor levels alphabetically ordered accept first level reject second level (“” comes “r” alphabet). inconsistent resp, 0 = reject 1 = accept.call response specifies ordering factor levels using levels argument passed vector levels, c(\"reject\", \"accept\") (.e., remember, c() function creating vectors kind, character vectors ). reason otherwise factor levels alphabetically ordered accept first level reject second level (“” comes “r” alphabet). inconsistent resp, 0 = reject 1 = accept.call condition specifies levels levels well new names factor levels using labels. labels use ordering used throughout book (.e., potential loss first, potential gain second) differs ordering original condition (.e., switches ordering format used now). arguments vector passed, elements mapped position (.e., new label first level, 40.2, first label, -$20/+$40). specify ordering factor levels levels maintain ordering used throughout, condition typically shows loss aversion, -$20/+$40 first condition. done , first level -$20/+$20 condition (20.2 smallest number original vector).call condition specifies levels levels well new names factor levels using labels. labels use ordering used throughout book (.e., potential loss first, potential gain second) differs ordering original condition (.e., switches ordering format used now). arguments vector passed, elements mapped position (.e., new label first level, 40.2, first label, -$20/+$40). specify ordering factor levels levels maintain ordering used throughout, condition typically shows loss aversion, -$20/+$40 first condition. done , first level -$20/+$20 condition (20.2 smallest number original vector).interesting part three calls factor() function run , already run , “break” condition variable. try , see values condition variable change NA run call second time. reason values passed levels argument need present variable. However, replaced original values new labels, none original levels (.e., c(40.2, 20.2, 40.4, 20.4)) part variable . Consequently, values replaced NA (.e., “available” means missing data). get values back need reload data using read.csv() command can run factor() call data state ran first time.shows assume running piece code twice gives output cases. problem code changes data (e.g., values condition). However, code also assumes certain values condition. assumption holds first time run code, second time, second call breaks results somewhat unexpected ways. means randomly re-running code can lead unexpected results (called “bugs” programming language parlance). Therefore, instead re-running individually pieces code, can often help re-start top script re-run everything order ensure data state think (ideally restarting R session Session - Restart R).One tip transforming variables factors. often bit annoying type factor levels hand, especially , say, two. case, handy trick know can get R produce c() call factor levels. just need make sure using correct ordering. heart trick dput() function creates text representation R output can copied console script. use factors basic structure call dput(unique(df$variable)) returns c() call unique elements variable. want elements ordered can use dput(sort(unique(df$variable))). example, create levels argument condition variable initially executed dput(sort(unique(df1a$condition))) console returns c(20.2, 20.4, 40.2, 40.4) original df1a data (.e., turning everything factors). just copied pasted bit vector get ordering right (admittedly, typing might faster copying pasting, whatever).Another thing often want reading data getting overview actually looks like. One way recommend data.frames just typing name variable console (calling just name script). , object printed console window, , large data.frames, leads hundreds thousands rows printed printing limit reached. alternative just look first rows using head() function (prints first 6 rows per default):Alternatively, can click object RStudio “Environment” pane (see Figures 3.1 3.2) opens data viewer pane. equivalent R call using View(df1a) opens viewer. However, View() used interactively console R script requires user interaction beyond script console. words, View() useful development get overview data, final analysis script.","code":"\ndf1a$subno <- factor(df1a$subno)\ndf1a$response <- factor(df1a$response, levels = c(\"reject\", \"accept\"))\ndf1a$condition <- factor(\n  df1a$condition, \n  levels = c(40.2, 20.2, 40.4, 20.4), \n  labels = c(\"-$20/+$40\", \"-$20/+$20\", \"-$40/+$40\", \"-$40/+$20\")\n)\nstr(df1a)\n#> 'data.frame':    22912 obs. of  6 variables:\n#>  $ subno    : Factor w/ 358 levels \"5\",\"8\",\"13\",\"24\",..: 2 2 2 2 2 2 2 2 2 2 ...\n#>  $ loss     : int  6 6 6 6 6 6 6 6 8 8 ...\n#>  $ gain     : int  6 8 10 12 14 16 18 20 6 8 ...\n#>  $ response : Factor w/ 2 levels \"reject\",\"accept\": 2 2 2 2 2 2 2 2 1 2 ...\n#>  $ condition: Factor w/ 4 levels \"-$20/+$40\",\"-$20/+$20\",..: 2 2 2 2 2 2 2 2 2 2 ...\n#>  $ resp     : int  1 1 1 1 1 1 1 1 0 1 ...\nhead(df1a)\n#>   subno loss gain response condition resp\n#> 1     8    6    6   accept -$20/+$20    1\n#> 2     8    6    8   accept -$20/+$20    1\n#> 3     8    6   10   accept -$20/+$20    1\n#> 4     8    6   12   accept -$20/+$20    1\n#> 5     8    6   14   accept -$20/+$20    1\n#> 6     8    6   16   accept -$20/+$20    1"},{"path":"tidyverse-intro.html","id":"the-tidyverse","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.5 The tidyverse","text":"popular extension base R tidyverse (Wickham et al. 2019), selection packages curated large parts developed RStudio company. mastermind behind tidyverse Hadley Wickham, RStudio chief scientist maybe one person can considered R superstar.29 core tidyverse packages introduced , dplyr ggplot2, developments (even though many others contributed packages well).reminder, packages collection functions R objects (e.g., data) provide additional functionality top base R. able use package, needs installed CRAN can loaded R session. easiest way install package using install.packages(). Note good idea run install.packages() console put R script. reason want execute install.packages() per R installation updating R every time run script.Therefore, begin installing tidyverse package CRAN using install.packages(). automatically install individual packages discussed . reminder, needs done everytime run script., can load tidyverse packages using library() function. Loading tidyverse package way something top pretty much scripts creating.loading packages common produces status messages console (packages , don’t). example, tidyverse lists package versions loaded (“attached”) packages lists function conflicts; , cases tidyverse function masks previous loaded function name. include messages show normal nothing worry , generally later book, get repetitive. However, note exact version numbers packages shown may differ version numbers see, simply packages might updated book written.core tidyverse packages (descriptions taken adapted official websites):tibble: modern version data.frametibble: modern version data.framereadr: reading data RStudio way.readr: reading data RStudio way.Data wrangling magrittr, tidyr, dplyr: coherent set functions tidying, transforming, working rectangular (.e., tabular) data. Supersedes many base R functions makes common data manipulations easy.Data wrangling magrittr, tidyr, dplyr: coherent set functions tidying, transforming, working rectangular (.e., tabular) data. Supersedes many base R functions makes common data manipulations easy.ggplot2: system data visualization, words, making graphs. discussed next chapter.ggplot2: system data visualization, words, making graphs. discussed next chapter.purr broom: Advanced modelling tidyverse tidymodels discussed .purr broom: Advanced modelling tidyverse tidymodels discussed .following provide short introduction core components tidyverse needed book. comprehensive introduction provided Wickham Grolemund book “R Data Science” available freely http://r4ds..co.nz. get good grip tidyverse, highly recommend working chapters 1 21, better still, chapter 25 (chapters “R Data Science” lot shorter chapters present book).","code":"\ninstall.packages(\"tidyverse\")\nlibrary(\"tidyverse\")\n#> -- Attaching packages ------------------- tidyverse 1.3.1 --\n#> v ggplot2 3.3.5     v purrr   0.3.4\n#> v tibble  3.1.4     v dplyr   1.0.7\n#> v tidyr   1.1.3     v stringr 1.4.0\n#> v readr   2.0.1     v forcats 0.5.1\n#> -- Conflicts ---------------------- tidyverse_conflicts() --\n#> x dplyr::filter() masks stats::filter()\n#> x dplyr::lag()    masks stats::lag()"},{"path":"tidyverse-intro.html","id":"tibble-and-readr","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.5.1 tibble and readr","text":"back bone tidyverse tibble, tbl_df (’s class name R), “modern reimagining data.frame, keeping time proven effective, throwing ” (quote tibble documentation). main difference data.frame tibble daily use tibbles try print rows columns thus overwhelm console just entering name data console. feature alone enough prefer tibbles data.frames. working analysis, one often wants quick look data see happened easiest way often just enter name data console.convert data.frame tibble can use as_tibble() function. Let’s existing data.frame Walasek Stewart (2015), df1a.output tibble, can see two things. First, output shows first 6 rows. Compare happens typing df1a console print many rows (see column names anymore). Second, data type column shown column names. another feature makes tibbles convenient everyday use. directly see example subno factor (<fct>) .alternative way create tibble instead standard data.frame loading data using tidyverse specific read functions readr package. Remember used base R read function read.csv(), one handful base R data read functions read.table() (reading data separated spaces) read.delim() (reading tab separated data). readr offers similar set functions main differences instead using . function name use _ instead data.frame return tibble. case csv file, use read_csv() instead read.csv(). Note following code chunks, going overwrite existing tbl1a object clutter work space.can see output read_csv() returns tibble, also message showing column specification (.e., data type column). use change column type reading data, usually find necessary. cases sufficient change data type data read , shown .status message also shows, can stop message appearing specifying additional argument read_csv() call, show_col_types = FALSE. don’t think necessary. However, following shows works expected.Another feature readr compared corresponding base R functions bit restrictive cases. , case data expected particular read function, readr fails often base R. Whereas can appear annoying programming, benefit one learns problems data early compared late. words, readr functions likely base R functions ensure data looks like expect . data likely look like expect , code less likely produce incorrect results.Therefore, addition using read_csv() instead read.csv(), often better use read_table() instead read.table() space separated data (read_table2() data format bit sloppy), read_tsv() instead read.delim() tab separated data, read_delim() want specify data delimiter.","code":"\ntbl1a <- as_tibble(df1a)\ntbl1a\n#> # A tibble: 22,912 x 6\n#>   subno  loss  gain response condition  resp\n#>   <fct> <int> <int> <fct>    <fct>     <int>\n#> 1 8         6     6 accept   -$20/+$20     1\n#> 2 8         6     8 accept   -$20/+$20     1\n#> 3 8         6    10 accept   -$20/+$20     1\n#> 4 8         6    12 accept   -$20/+$20     1\n#> 5 8         6    14 accept   -$20/+$20     1\n#> 6 8         6    16 accept   -$20/+$20     1\n#> # ... with 22,906 more rows\ntbl1a <- read_csv(\"data/ws2015_exp1a.csv\")\n#> Rows: 22912 Columns: 6\n#> -- Column specification ------------------------------------\n#> Delimiter: \",\"\n#> chr (1): response\n#> dbl (5): subno, loss, gain, condition, resp\n#> \n#> i Use `spec()` to retrieve the full column specification for this data.\n#> i Specify the column types or set `show_col_types = FALSE` to quiet this message.\ntbl1a\n#> # A tibble: 22,912 x 6\n#>   subno  loss  gain response condition  resp\n#>   <dbl> <dbl> <dbl> <chr>        <dbl> <dbl>\n#> 1     8     6     6 accept        20.2     1\n#> 2     8     6     8 accept        20.2     1\n#> 3     8     6    10 accept        20.2     1\n#> 4     8     6    12 accept        20.2     1\n#> 5     8     6    14 accept        20.2     1\n#> 6     8     6    16 accept        20.2     1\n#> # ... with 22,906 more rows\ntbl1a <- read_csv(\"data/ws2015_exp1a.csv\", show_col_types = FALSE)\ntbl1a\n#> # A tibble: 22,912 x 6\n#>   subno  loss  gain response condition  resp\n#>   <dbl> <dbl> <dbl> <chr>        <dbl> <dbl>\n#> 1     8     6     6 accept        20.2     1\n#> 2     8     6     8 accept        20.2     1\n#> 3     8     6    10 accept        20.2     1\n#> 4     8     6    12 accept        20.2     1\n#> 5     8     6    14 accept        20.2     1\n#> 6     8     6    16 accept        20.2     1\n#> # ... with 22,906 more rows"},{"path":"tidyverse-intro.html","id":"the-pipe","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.5.2 The Pipe: %>%","text":"One coolest, time novel (R), features tidyverse pipe, name %>% operator.30 understand great, makes sense begin taking step back thinking bit functions evaluation order R.One important features R can take result (output) one operation use input another operation. example, remember loss column data Walasek Stewart (2015) shows potential loss lotteries, ranging $6 $40. Now imagine want know midpoint minimum maximum loss ($23 present case). One way calculate taking mean minimum maximum. get R need two steps: get minimum maximum potential loss vector losses take mean minimum maximum. can get minimum maximum vector using range() function mean obtained using mean() function. R vector potential losses, tbl1a$loss, :code two-step calculation explicitly saves results first step temporary variable called tmp. also combine steps one passing results first step mean() function:first get minimum maximum (.e., range) pass mean() function. mean function last operation want, outermost call line code. words, R code use pipe, code executed inside outside. can make difficult chain many operations without need create temporary variables first.contrast executing code innermost outermost, pipe allows us chain operations left right. can start innermost call pipe next function using %>%. example, using pipe following:shows typical pipe workflow. start data, present case tbl1a$loss vector. data piped first operation, range() function. output call piped next operation, mean() function. end, get exactly output get code easier read. One feature pipe makes code particularly readable can start new line step chain.pipe also works like regular R operation, can easily save result whole chain operation new object later use. example:sum , base R order operations innermost outermost. pipe, contrast, can chain (pipe) operations left right. following image exemplifies using screenshot slide Andrew Heiss. top part see operations needs execute order leave house using base R (.e., starting innermost wakeup() function). coding style without pipe makes difficult see happens , generally difficult read, least hard see function arguments (like pants=TRUE) go function (like get_dressed). lower part shows operations using pipe. can see much easier see logic waking , getting bed, forth, mention arguments go function.\nFigure 3.3: Comparison execution order without pipe (upper part) pipe (lower part). : https://evalsp21.classes.andrewheiss.com/projects/01_lab/slides/01_lab.html#116\nfollowing sections see powerful piping can . can quite lot neat things relatively easily, especially pipe combined dplyr functions introduced next.Let us end section two points important. First, typing pipe (.e., typing characters %>%) annoying. RStudio, alternative, form keyboard shortcut: Ctrl/Cmd + Shift + M. highly recommend get used using shortcut. one shortcuts RStudio use regularly.31Second, base R now pipe, |>. works similarly tidyverse pipe (can many situations used instead tidyverse pipe) can used without loading packages. using reason adapted tidyverse pipe base R pipe seem many reasons prefer one tiydverse already loaded. see |> instead %>% somewhere, assume pretty much exact thing.","code":"\n# step 1: get minimum and maximum and save in temporary variables\ntmp <- range(tbl1a$loss)\ntmp # print minimum and maximum, to check everything is okay\n#> [1]  6 40\n#step 2: calculate mean of minimum and maximum:\nmean(tmp)\n#> [1] 23\nmean(range(tbl1a$loss))\n#> [1] 23\ntbl1a$loss %>% \n  range() %>% \n  mean()\n#> [1] 23\nloss_midpoint <- tbl1a$loss %>%\n  range() %>% \n  mean()\nloss_midpoint\n#> [1] 23"},{"path":"tidyverse-intro.html","id":"dplyr","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.5.3 dplyr","text":"Whereas back bone tidyverse tibble pipe, package significantly improves one’s workflow dplyr (pronounced dee-plier, like tool). five core functions dplyr , together, provide common data manipulation operations. quote official documentation: “dplyr grammar data manipulation, providing consistent set verbs help solve common data manipulation challenges.” five verbs/functions :mutate() adds new variables changes variables function existing variablesselect() picks variables based namesfilter() picks cases based valuessummarise() reduces multiple values summaryarrange() changes ordering rowsA great introduction overview functionality functions provided official getting started page, well, somewhat briefly.One important point dplyr functions work called non-standard evaluation. means can refer variable names data.frame/tibble working dplyr functions without enclosing quotes. sounds bit mysterious , remember thing can usually refer without quotes R objects exist workspace (.e., objects listed environment pane RStudio). variables tibbles/data.frames objects , exist context data.frame/tibble, many functions get know later packages require names enclosed quotes. However, generally case tidyverse functions saving keystrokes.Finally, dplyr functions work pipe, can chained.","code":""},{"path":"tidyverse-intro.html","id":"mutate","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.5.3.1 mutate()","text":"example, can use mutate() pipe convert variables data set factors, base R. start reading data . pipe data mutate() function convert three variables factors done , save object, tbl1a. get overview object.can see number changes compared base R code:block code variables converted factors wrapped mutate() call piped data, tbl1a.operation converts one variable factor, use = sign assignment operator <-, operations performed within mutate() function. generally use assignment operator <- outside function calls (try replace = <- see horrible consequences ).first two calls changing variable finish ,, inside one mutate() call. , tells mutate() yet done specifying set operations.Instead passing full vector factor() (using df1a$subno), can refer variable, subno, name (without quotes) directly (meant non-standard evaluation). , arises fact working within context data set originally piped mutate() call.","code":"\ntbl1a <- read_csv(\"data/ws2015_exp1a.csv\")\ntbl1a <- tbl1a %>% \n  mutate(\n    subno = factor(subno),\n    response = factor(response, levels = c(\"reject\", \"accept\")),\n    condition = factor(\n      condition, \n      levels = c(40.2, 20.2, 40.4, 20.4), \n      labels = c(\"-$20/+$40\", \"-$20/+$20\", \"-$40/+$40\", \"-$40/+$20\")\n    )\n  )\ntbl1a\n#> # A tibble: 22,912 x 6\n#>   subno  loss  gain response condition  resp\n#>   <fct> <dbl> <dbl> <fct>    <fct>     <dbl>\n#> 1 8         6     6 accept   -$20/+$20     1\n#> 2 8         6     8 accept   -$20/+$20     1\n#> 3 8         6    10 accept   -$20/+$20     1\n#> 4 8         6    12 accept   -$20/+$20     1\n#> 5 8         6    14 accept   -$20/+$20     1\n#> 6 8         6    16 accept   -$20/+$20     1\n#> # ... with 22,906 more rows"},{"path":"tidyverse-intro.html","id":"summarise","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.5.3.2 summarise()","text":"second important function dplyr summarise() name suggests, creates summaries. usually means reducing number rows data set, often just one row. example, use summarise() get average probability lotteries accepted across conditions lotteries. , just need take overall mean resp variable.see summarise() also returns tibble, values returned differ quite bit one get base R (using mean(tbl1a$resp)). can also see summarise() works structurally quite similarly mutate(). summarise() call, can create new variables using name = operation syntax. can also create multiple summary statistics separating using ,. can also add comments code usual using #, long right side ,.","code":"\ntbl1a %>% \n  summarise(mean_acc = mean(resp))\n#> # A tibble: 1 x 1\n#>   mean_acc\n#>      <dbl>\n#> 1    0.381\ntbl1a %>% \n  summarise(\n    mean_acc = mean(resp),\n    sd_acc = sd(resp),  ## sd() returns the standard deviation\n    mean_pot_loss = mean(loss),\n    mean_pot_gain = mean(gain)\n  )\n#> # A tibble: 1 x 4\n#>   mean_acc sd_acc mean_pot_loss mean_pot_gain\n#>      <dbl>  <dbl>         <dbl>         <dbl>\n#> 1    0.381  0.486          19.5          19.2"},{"path":"tidyverse-intro.html","id":"filter-select-and-arrange","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.5.3.3 filter(), select(), and arrange()","text":"discussed , one key ways using tidyverse chaining different operations together. example, first create data set observations \"-$20/+$40\" condition , calculate summaries condition alone. , pipe first filter() function, pipe results summarise():Note filter() requires use == operator = operator. reason want set variable equal something, done using =, want check equality, done using ==. filter() also allows chain multiple comparisons together, either separating using , & (.e., logical ). one multiple conditions needs hold, can use | (logical ).filter() also function removing observations. done writing filter retains observations except ones want remove. can also use logical operator, ! inverts logical vector, unequal operator, !=.example, ’ll recall , relation study Walasek Stewart (2015), actually interested responses symmetric lotteries (potential loss equal potential gain). select filter():equivalently:, use .equal() function show two objects . function can compare arbitrary R objects returns TRUE two objects identical.One problem filter() command used selects symmetric lotteries symmetric lotteries appear conditions. words, also selects symmetric lotteries appear subset conditions. goal compare performance across conditions – fair comparison needs done lotteries shared across conditions – filter() call therefore correct one. following code, shows symmetric lotteries condition, demonstrates .particular, output shows asymmetric conditions, -$20/+$40 -$40/+$20, three symmetric lotteries, whereas two symmetric conditions quite lot . However, three symmetric lotteries appear asymmetric conditions, 12-12, 16-16, 20-20, appear conditions.focussing can build better filter, let’s first examine detail code works. first filter obtain symmetric lotteries. data set, retain select() loss, gain, condition columns. use unique() function retain unique rows. specific ordering think conditions (expressed order factor levels) sort tibble along condition variable using arrange(). Finally, tibble prints rows default, use print(n = Inf) prints rows (.e., row infinity).better understand code, encouraged run line line see output intermediate step . easiest way selecting parts R script window sending console using shortcut (.e., Ctrl/CMD + Enter). Note selecting subset lines code, select %>% end last line sending console. Otherwise R think yet finished. general, ’s good idea step line--line much code present , speed, order fully understand operation.Now know symmetric lotteries , can select symmetric lotteries appear conditions? clearly multiple different filters . one first comes mind combine filter selects symmetric lotteries filter selects lotteries potential loss either 12, 16, 20. latter, introduce new operator, %% operator. returns TRUE element vector matches element another vector. example case, want retain lotteries potential loss c(12, 16, 20). shown . also include code shows now set symmetric lotteries conditions.","code":"\ntbl1a %>%\n  filter(condition == \"-$20/+$40\") %>% \n  summarise(\n    mean_acc = mean(resp),\n    sd_acc = sd(resp),\n    mean_pot_loss = mean(loss),\n    mean_pot_gain = mean(gain)\n  )\n#> # A tibble: 1 x 4\n#>   mean_acc sd_acc mean_pot_loss mean_pot_gain\n#>      <dbl>  <dbl>         <dbl>         <dbl>\n#> 1    0.514  0.500            13            26\nsymm1 <- tbl1a %>%\n  filter(loss == gain)\nsymm1\n#> # A tibble: 1,989 x 6\n#>   subno  loss  gain response condition  resp\n#>   <fct> <dbl> <dbl> <fct>    <fct>     <dbl>\n#> 1 8         6     6 accept   -$20/+$20     1\n#> 2 8         8     8 accept   -$20/+$20     1\n#> 3 8        10    10 accept   -$20/+$20     1\n#> 4 8        12    12 accept   -$20/+$20     1\n#> 5 8        14    14 accept   -$20/+$20     1\n#> 6 8        16    16 accept   -$20/+$20     1\n#> # ... with 1,983 more rows\nsymm2 <- tbl1a %>%\n  filter(!(loss != gain))\nall.equal(symm1, symm2)\n#> [1] TRUE\ntbl1a %>%\n  filter(loss == gain) %>% \n  select(loss, gain, condition) %>% \n  unique() %>% \n  arrange(condition) %>% \n  print(n = Inf)\n#> # A tibble: 22 x 3\n#>     loss  gain condition\n#>    <dbl> <dbl> <fct>    \n#>  1    12    12 -$20/+$40\n#>  2    16    16 -$20/+$40\n#>  3    20    20 -$20/+$40\n#>  4     6     6 -$20/+$20\n#>  5     8     8 -$20/+$20\n#>  6    10    10 -$20/+$20\n#>  7    12    12 -$20/+$20\n#>  8    14    14 -$20/+$20\n#>  9    16    16 -$20/+$20\n#> 10    18    18 -$20/+$20\n#> 11    20    20 -$20/+$20\n#> 12    12    12 -$40/+$40\n#> 13    16    16 -$40/+$40\n#> 14    20    20 -$40/+$40\n#> 15    24    24 -$40/+$40\n#> 16    28    28 -$40/+$40\n#> 17    32    32 -$40/+$40\n#> 18    36    36 -$40/+$40\n#> 19    40    40 -$40/+$40\n#> 20    12    12 -$40/+$20\n#> 21    16    16 -$40/+$20\n#> 22    20    20 -$40/+$20\ntbl1a %>%\n  filter(loss == gain, loss %in% c(12, 16, 20)) %>% \n  select(loss, gain, condition) %>% \n  unique() %>% \n  arrange(condition) \n#> # A tibble: 12 x 3\n#>    loss  gain condition\n#>   <dbl> <dbl> <fct>    \n#> 1    12    12 -$20/+$40\n#> 2    16    16 -$20/+$40\n#> 3    20    20 -$20/+$40\n#> 4    12    12 -$20/+$20\n#> 5    16    16 -$20/+$20\n#> 6    20    20 -$20/+$20\n#> # ... with 6 more rows"},{"path":"tidyverse-intro.html","id":"group-by","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.5.3.4 group_by()","text":"might remember main hypothesis Walasek Stewart (2015) absolute value lottery, relative rank, matters whether participants think lottery attractive. main evidence hypothesis proportion symmetric lotteries accepted differs across four conditions. canwe calculate tiydverse?applying know far, split data four subsets using filter(), apply summarise() subsets. Let’s show work splitting data two subsets, one “-$20/+$40” condition shows loss aversion one “-$40/+$20” condition shows loss seeking. need combine filter filter selecting symmetric lotteries. make logic easier, separate two steps separate filter() calls (also combine one).results show previously discussed pattern. Participants -$20/+$40 condition around 50 percentage points less likely accept symmetric lotteries participants -$40/+$20 condition. Whereas analysis clearly want , code bit verbose clunky. example, first filter() call summarise() call identical parts. question : better way get results?, course, rhetorical question answer yes, better way. answer group_by() function, function gives dplyr full power. use group_by(), need pass one multiple (categorical) grouping variables. group_by() creates grouped tibble consequence dplyr verbs applied grouped tibble, applied group separately. Let’s show example see means practice:can see output now contains average proportion accepting symmetric lotteries four conditions separately. instead first splitting data calculating summary statistics (, mean resp column) split separately, group_by() exactly internally.words, whenever categorical variable data, can use group_by() perform operations separately across levels categorical variable. Given ubiquity categorical variables – example, every experiment least one experimental factor – huge time effort saver.whereas shown use group_by() summarise() (probably common use), group_by() also works dplyr verbs results can depend rows, mutate() arrange().example , passed one variable (condition) group_by(). can also pass multiple variables. perform operations conditional combination variables. example, selected three symmetric lotteries across conditions. group_by(), easy get average proportion acceptances separately lottery condition. returns tiible rows shown default, use print(n = Inf) last command pipe chain show rows.Note pretty much result obtained passing one gain/loss variable pairs group_by() (.e., group_by(condition, loss) group_by(condition, gain)). Feel free try .results straightforward interpret show consistent pattern across conditions. symmetric conditions, difference acceptance rate seems small. However, asymmetric conditions, see somewhat larger differences, suggesting 16-16 lottery may somewhat lower acceptance rates 12-12 20-20 lottery. descriptive results without additional statistical evidence, much possible small differences within conditions reflect pure noise taken seriously.moving next function, one thing discuss. shown output , summarise() returns tibble still grouped due call group_by() command one variable. can seen status message “summarise() grouped output …” output still shows # Groups: .... means operations tibble, still performed grouped. remove grouping created group_by(), use ungroup():","code":"\n## \"loss aversion\" condition:\ntbl1a %>%\n  filter(loss == gain, loss %in% c(12, 16, 20)) %>% \n  filter(condition == \"-$20/+$40\") %>% \n  summarise(mean_acc = mean(resp))\n#> # A tibble: 1 x 1\n#>   mean_acc\n#>      <dbl>\n#> 1    0.167\n\n## \"loss seeking\" condition:\ntbl1a %>%\n  filter(loss == gain, loss %in% c(12, 16, 20)) %>% \n  filter(condition == \"-$40/+$20\") %>% \n  summarise(mean_acc = mean(resp))\n#> # A tibble: 1 x 1\n#>   mean_acc\n#>      <dbl>\n#> 1    0.670\ntbl1a %>%\n  filter(loss == gain, loss %in% c(12, 16, 20)) %>% \n  group_by(condition) %>% \n  summarise(mean_acc = mean(resp))\n#> # A tibble: 4 x 2\n#>   condition mean_acc\n#>   <fct>        <dbl>\n#> 1 -$20/+$40    0.167\n#> 2 -$20/+$20    0.462\n#> 3 -$40/+$40    0.437\n#> 4 -$40/+$20    0.670\ntbl1a %>%\n  filter(loss == gain, loss %in% c(12, 16, 20)) %>% \n  group_by(condition, loss, gain) %>% \n  summarise(mean_acc = mean(resp)) %>% \n  print(n = Inf)\n#> `summarise()` has grouped output by 'condition', 'loss'. You can override using the `.groups` argument.\n#> # A tibble: 12 x 4\n#> # Groups:   condition, loss [12]\n#>    condition  loss  gain mean_acc\n#>    <fct>     <dbl> <dbl>    <dbl>\n#>  1 -$20/+$40    12    12    0.190\n#>  2 -$20/+$40    16    16    0.119\n#>  3 -$20/+$40    20    20    0.190\n#>  4 -$20/+$20    12    12    0.469\n#>  5 -$20/+$20    16    16    0.458\n#>  6 -$20/+$20    20    20    0.458\n#>  7 -$40/+$40    12    12    0.448\n#>  8 -$40/+$40    16    16    0.437\n#>  9 -$40/+$40    20    20    0.425\n#> 10 -$40/+$20    12    12    0.681\n#> 11 -$40/+$20    16    16    0.659\n#> 12 -$40/+$20    20    20    0.670\ntbl1a %>%\n  filter(loss == gain, loss %in% c(12, 16, 20)) %>% \n  group_by(condition, loss, gain) %>% \n  summarise(mean_acc = mean(resp)) %>% \n  ungroup()\n#> `summarise()` has grouped output by 'condition', 'loss'. You can override using the `.groups` argument.\n#> # A tibble: 12 x 4\n#>   condition  loss  gain mean_acc\n#>   <fct>     <dbl> <dbl>    <dbl>\n#> 1 -$20/+$40    12    12    0.190\n#> 2 -$20/+$40    16    16    0.119\n#> 3 -$20/+$40    20    20    0.190\n#> 4 -$20/+$20    12    12    0.469\n#> 5 -$20/+$20    16    16    0.458\n#> 6 -$20/+$20    20    20    0.458\n#> # ... with 6 more rows"},{"path":"tidyverse-intro.html","id":"counting","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.5.3.5 Counting","text":"Another important functionality dplyr makes easy count observations using count() n() functions. Counting one operations seems somewhat unspectacular beginning, extremely important understanding data. example, important question might data many participants per condition many trials every participant worked . Counting also good way check exact number observations expect original design experiment.Let us begin looking number trials per participant (condition). two almost equivalent ways , either using group_by() summarise(n = n()) count():can see results pretty much , difference group_by() retains grouped tibble whereas count() .can also see number trials participants shown output 64. exactly number trials expect description study Section 1.2.2. However, far sure really holds participants. expect case looks like handful participants shown, really hold everyone?analysing data, questions regularly come one’s mind. One assumptions data look like, one sure assumptions really hold. , example, data collection finish participants partial data ? reality analysing actual data often assumptions one data set hold. Many things can go wrong data collection, important step analysis verify one’s assumptions.can check everyone exactly 64 trials? code , can done per group extending another group_by() summarise() combination. also allows us count, step, number participants condition:exactly happening ? first three lines (including first summarise()), create summarised tibble already shown previous code. tibble one row per participant n column giving number trials participant. group tibble condition ensure following results shown separately condition (really necessary, still grouped condition, makes code clearer). perform another summarise() summarises summarised tibble condition. summarise(), perform two operations:create new variable n_participants, contains number observations (number participants) summarised tibble. done using n() function counting observations.create new logical variable all_64, TRUE observations/participants within one condition number trials 64. fully understand call creating variable, read inside outside. innermost call n == 64. creates logical variable observation summarised tibble TRUE number trials (variable n) 64 FALSE otherwise. pass vector () function reduces logical vector single logical variable. () returns TRUE elements vector TRUE FALSE otherwise.results thus show two pieces information per condition. First, can see number participants per condition, 84 96. Second, conditions, participants exactly 64 trials, indicated values all_64 vector TRUE conditions.","code":"\ntbl1a %>% \n  group_by(condition, subno) %>% \n  summarise(n = n())\n#> `summarise()` has grouped output by 'condition'. You can override using the `.groups` argument.\n#> # A tibble: 358 x 3\n#> # Groups:   condition [4]\n#>   condition subno     n\n#>   <fct>     <fct> <int>\n#> 1 -$20/+$40 5        64\n#> 2 -$20/+$40 13       64\n#> 3 -$20/+$40 53       64\n#> 4 -$20/+$40 61       64\n#> 5 -$20/+$40 73       64\n#> 6 -$20/+$40 85       64\n#> # ... with 352 more rows\n\ntbl1a %>% \n  count(condition, subno)\n#> # A tibble: 358 x 3\n#>   condition subno     n\n#>   <fct>     <fct> <int>\n#> 1 -$20/+$40 5        64\n#> 2 -$20/+$40 13       64\n#> 3 -$20/+$40 53       64\n#> 4 -$20/+$40 61       64\n#> 5 -$20/+$40 73       64\n#> 6 -$20/+$40 85       64\n#> # ... with 352 more rows\ntbl1a %>% \n  group_by(condition, subno) %>% \n  summarise(n = n()) %>% \n  group_by(condition) %>% \n  summarise(\n    n_participants = n(),\n    all_64 = all(n == 64)\n    )\n#> `summarise()` has grouped output by 'condition'. You can override using the `.groups` argument.\n#> # A tibble: 4 x 3\n#>   condition n_participants all_64\n#>   <fct>              <int> <lgl> \n#> 1 -$20/+$40             84 TRUE  \n#> 2 -$20/+$20             96 TRUE  \n#> 3 -$40/+$40             87 TRUE  \n#> 4 -$40/+$20             91 TRUE"},{"path":"tidyverse-intro.html","id":"dplyr-summary","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.5.3.6 dplyr Summary","text":"seen dplyr powerful tool working data.frames tibbles. five verbs, mutate(), summarise(), filter(), select(), arrange(), relatively intuitive syntax solving one common data manipulation problem. commonly used verbs mutate() (calculating new variables changing existing variables observations) summarise() (calculates new variables reduces number observations, usually one). One common use summarise() n() function allows counting number observations. Finally, filter() tool one needs selecting subsets observations.full power dplyr comes combining functions group_by(), performs operations conditional one several grouping variables. example, one important analysis steps calculating summary statistics level factor combination factor levels. can easily done dplyr using powerful data %>% group_by(factor1, factor2) %>% summarise(...) combination. addition getting informative summaries, also used regularly check whether data set matches assumptions one .","code":""},{"path":"tidyverse-intro.html","id":"summary-2","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.6 Summary","text":"chapter, began list resources provided general introduction R. , provided brief example read data change variables factors base R. main functions introduced read.csv(), str(), factor(), head(). factor() function also shown can use different optional arguments, level labels.yet done , maybe now good time check internal R help factor(), see understanding arguments matches actual description. , either type ?factor R console press F1 key cursor factor() function RStudio. brings help page (exists R function) can read description argument. able read understand R help pages one important R programming skills, one usually develops time, don’t demoralised understand little help page now.short recap base R, provided brief introduction tidyverse packages, tibble, readr, dplyr, introduced tidyverse pipe, %>%. backbone tidyverse tibble, modern variant base R data.frame. tibble also returned using readr functions reading data, read_csv().pipe, saw changes order can write commands. Whereas base R order inside outside (innermost function first), pipe can write pipe functions left write. pipe makes easy chain different operations together, otherwise difficult base R lead unreadable code. Pipingoften avoids need create intermediate variables generally leads readable code.Finally, introduced functionality dplyr. dplyr, important functions three “verbs,” mutate(), summarise(), filter(), solves one specific data manipulation problem. mutate() creates/changes variables within current data, summarise() creates new variables summarising current data, filter() selects observations (.e., reduces number rows). One powerful features dplyr verbs can combined group_by(), performs operations conditional one multiple categorical variables.help get going R tidyverse particular, next page offers number examples exercises.","code":""},{"path":"tidyverse-exercises.html","id":"tidyverse-exercises","chapter":"tidyverse: Example, Quiz, and Exercises","heading":"tidyverse: Example, Quiz, and Exercises","text":"get habit regularly starting new task analysis, now probably good time restart R session. reminder, RStudio menu Session - Restart R. , need load tidyverse :","code":"\nlibrary(\"tidyverse\")"},{"path":"tidyverse-exercises.html","id":"extended-walasek-example","chapter":"tidyverse: Example, Quiz, and Exercises","heading":"3.7 Extended tidyverse Example: Walasek & Stewart (2015) Exp. 1a & 1b","text":"","code":""},{"path":"tidyverse-exercises.html","id":"reading-both-data-sets","chapter":"tidyverse: Example, Quiz, and Exercises","heading":"3.7.1 Reading Both Data Sets","text":"extended tidyverse example, let us look data Walasek Stewart (2015). described Section 3.4.1, two similar data sets, Experiments 1a 1b, can consider together (done Section 1.2.2). follow extended examples, download folder data current working directory: Experiment 1a Experiment 1b. , can load separate objects using read_csv().use new names resulting data objects (ws1a ws1b) help us remember data Walasek Stewart (2015), Experiments 1a 1b. ’s generally good idea use descriptive object names makes code easier read later . Especially code long, one even use descriptive names walasek_stewart_2015_1a walasek_stewart_2015_1b. However, example stays relatively short, first letter last names year suffice.Note previous chapter, problems downloading two files setting correct working directory, can load directly internet well. However, recommended shown make sure can follow steps example.now two data sets structure (.e., columns data types) can see just printing :output shows two data sets columns order, also appear condition names. check, let’s print sorted unique values condition column:confirms indeed, condition names data sets.","code":"\nws1a <- read_csv(\"data/ws2015_exp1a.csv\")\n#> Rows: 22912 Columns: 6\n#> -- Column specification ------------------------------------\n#> Delimiter: \",\"\n#> chr (1): response\n#> dbl (5): subno, loss, gain, condition, resp\n#> \n#> i Use `spec()` to retrieve the full column specification for this data.\n#> i Specify the column types or set `show_col_types = FALSE` to quiet this message.\nws1b <- read_csv(\"data/ws2015_exp1b.csv\")\n#> Rows: 27072 Columns: 6\n#> -- Column specification ------------------------------------\n#> Delimiter: \",\"\n#> chr (1): response\n#> dbl (5): subno, loss, gain, condition, resp\n#> \n#> i Use `spec()` to retrieve the full column specification for this data.\n#> i Specify the column types or set `show_col_types = FALSE` to quiet this message.\nws1a <- read_csv(\"https://github.com/singmann/stats_for_experiments/raw/master/data/ws2015_exp1a.csv\")\nws1b <- read_csv(\"https://github.com/singmann/stats_for_experiments/raw/master/data/ws2015_exp1a.csv\")\nws1a\n#> # A tibble: 22,912 x 6\n#>   subno  loss  gain response condition  resp\n#>   <dbl> <dbl> <dbl> <chr>        <dbl> <dbl>\n#> 1     8     6     6 accept        20.2     1\n#> 2     8     6     8 accept        20.2     1\n#> 3     8     6    10 accept        20.2     1\n#> 4     8     6    12 accept        20.2     1\n#> 5     8     6    14 accept        20.2     1\n#> 6     8     6    16 accept        20.2     1\n#> # ... with 22,906 more rows\nws1b\n#> # A tibble: 27,072 x 6\n#>   subno  loss  gain response condition  resp\n#>   <dbl> <dbl> <dbl> <chr>        <dbl> <dbl>\n#> 1     1     6    12 accept        40.2     1\n#> 2     1     6    16 accept        40.2     1\n#> 3     1     6    20 accept        40.2     1\n#> 4     1     6    24 accept        40.2     1\n#> 5     1     6    28 accept        40.2     1\n#> 6     1     6    32 accept        40.2     1\n#> # ... with 27,066 more rows\nws1a$condition %>% \n  unique() %>% \n  sort()\n#> [1] 20.2 20.4 40.2 40.4\nws1b$condition %>% \n  unique() %>% \n  sort()\n#> [1] 20.2 20.4 40.2 40.4"},{"path":"tidyverse-exercises.html","id":"combining-both-data-sets","chapter":"tidyverse: Example, Quiz, and Exercises","heading":"3.7.2 Combining Both Data Sets","text":"analyse two data sets together, done Section 1.2.2, now combine two tibbles one. Specifically, need add rows one data sets data sets (contrast adding additional columns one data sets). tidyverse function combine multiple tibbles rows bind_rows(). code combining two data sets simply bind_rows(ws1a, ws1b). work case like columns, also order .However, combining data, consider one complication. can see data sets participant identifier column, subno, appears consist whole numbers. means number may used different participants two experiments. account possibility combining data, may end data set data different participants treated coming participant – clear error. example, output shows participant subno 8 Experiment 1a. also participant subno 8 Experiment 1b, data two different participants (participant participated studies) treated participant – clear error. see whether indeed potential problem, print individual subno values experiments.output confirms suspicion. subno values unique across experiments, need make combining data. One straightforward way adding string start subno, either e1a_ e1b_, indicates data set participant belongs . use paste0() function pastes strings together one (paste0() without adding extra separation character combined strings).call, also convert subno variable factor (categorical variable). can combine data sets one called ws1 using bind_rows() call discussed . get overview combined data using glimpse() function tidyverse equivalent str() (glimpse() often produces less verbose output tibbles str()– try see).output shows subno first participant Experiment 1 now e1a_8, 8. words, subno’s now unique within experiment can combine data without making larger data set misleading.verify logic, can count number unique values subno data set using length() function unique() elements subno vectors. length() function returns number elements vector, can directly used calculations. calculation shows , indeed, combined data number participants, 781, Experiments 1a 1b together.continuing analysis combined data, makes sense take step back consider briefly discussed question combine two data sets rather extensively. One one hand, code uses new functions yet introduced bind_rows() length(). , want introduce new functions explicitly discussing functionality.hand, discussion shows us one way handling checking assumptions code. issue combining two data sets using bind_rows(), code assumes participant identifiers unique. necessarily obvious assumption writing code, consequence fact simply concatenate data sets. assumption obvious, may overlook , possibly leading problems later code. Specifically, treating data different participants coming participant, just coincidentally share subno, probably appropriate statistical analysis. checking whether number participants combined data set equal sum number participants separate data sets, ensure combining two data sets done appropriately.Another reason discussing issue detail paper co-author, Skovgaard-Olsen, Singmann, Klauer (2016), overlooked exact problem. experiment reported paper consisted three conditions needed combined analysis. problem participant identifiers unique within conditions across conditions combined data sets without considering fact. Consequently, statistical analysis incorrectly treated observations different participants belonging participant. fact, statistical results reported Skovgaard-Olsen, Singmann, Klauer (2016) affected recognised error publication article. Luckily co-authors , realised happened re-analysed data correctly combining , initial error substantive effects results. details changed (exact value numbers), qualitative pattern results. Consequently, fix error publishing straightforward correction (Skovgaard-Olsen, Singmann, Klauer 2018). larger point errors can happen. best way avoid regularly checking explicitly whether assumptions make data code hold.","code":"\nws1a$subno %>% \n  unique() %>% \n  sort()\n#>   [1]   5   8  13  24  35  36  38  40  42  48  52  53  59\n#>  [14]  60  61  62  63  64  70  73  76  80  82  83  85  86\n#>  [27]  87  88  89  90  92  93  94  95  96  97  99 100 103\n#>  [40] 104 106 107 109 110 112 113 114 115 118 119 120 122\n#>  [53] 123 124 125 128 129 130 131 132 133 134 135 136 137\n#>  [66] 139 141 142 143 144 145 147 148 149 150 152 154 155\n#>  [79] 156 157 158 159 160 161 162 163 164 165 166 167 168\n#>  [92] 169 170 171 172 173 174 175 176 177 178 180 181 183\n#> [105] 184 186 187 188 190 191 193 194 195 196 198 200 201\n#> [118] 202 204 206 207 208 209 210 211 212 213 214 215 216\n#> [131] 217 218 219 220 221 222 223 224 225 226 227 228 229\n#> [144] 230 231 232 233 234 235 236 237 238 240 242 243 244\n#> [157] 245 246 247 248 249 250 251 256 257 258 259 260 261\n#> [170] 262 263 264 265 266 267 268 269 270 271 272 273 274\n#> [183] 276 277 278 279 280 281 282 283 284 285 286 287 288\n#> [196] 289 290 291 292 293 294 295 296 297 298 299 300 301\n#> [209] 303 304 306 307 308 309 310 311 312 314 315 316 317\n#> [222] 319 320 322 323 325 326 327 328 330 331 332 333 334\n#> [235] 335 336 337 338 339 340 341 342 343 344 345 346 347\n#> [248] 348 349 350 351 352 353 354 355 356 357 358 359 360\n#> [261] 361 362 363 364 365 366 367 368 369 370 371 372 373\n#> [274] 374 375 377 378 379 380 381 382 383 384 385 386 387\n#> [287] 388 389 390 391 392 393 394 395 396 397 398 399 400\n#> [300] 401 402 403 404 405 406 407 408 409 410 411 412 413\n#> [313] 414 415 416 417 418 420 421 422 423 424 425 426 427\n#> [326] 429 430 431 432 434 435 437 438 440 441 442 443 444\n#> [339] 445 446 447 448 449 450 451 452 453 454 455 456 457\n#> [352] 458 459 460 461 462 463 464\nws1b$subno %>% \n  unique() %>% \n  sort()\n#>   [1]   1   2   3   4   5   7   8   9  10  11  12  13  14\n#>  [14]  15  16  18  19  20  21  22  25  26  27  28  29  30\n#>  [27]  31  32  33  34  35  36  37  38  39  40  41  42  43\n#>  [40]  44  45  46  47  48  49  50  51  52  53  54  55  56\n#>  [53]  57  58  59  60  61  62  63  64  65  66  67  68  69\n#>  [66]  71  73  74  77  78  79  80  81  82  83  84  85  86\n#>  [79]  87  88  89  90  91  92  93  94  95  96  98 101 102\n#>  [92] 103 104 105 106 107 108 109 110 111 112 113 114 115\n#> [105] 116 117 118 119 120 121 122 123 124 125 126 127 128\n#> [118] 129 131 132 133 134 135 136 137 138 139 140 141 142\n#> [131] 143 144 145 146 147 148 149 150 151 152 153 154 155\n#> [144] 156 157 158 159 160 161 162 163 164 165 166 167 168\n#> [157] 169 170 171 172 173 174 175 176 177 178 179 180 181\n#> [170] 182 183 184 185 186 187 188 189 190 191 193 199 200\n#> [183] 202 203 208 209 210 211 212 213 214 215 216 217 218\n#> [196] 219 220 221 222 223 224 225 226 227 228 229 230 231\n#> [209] 232 233 236 237 238 240 243 244 245 246 247 248 249\n#> [222] 250 251 252 253 254 255 256 257 258 259 260 261 262\n#> [235] 263 264 265 266 267 268 269 270 271 273 274 275 276\n#> [248] 277 279 280 281 282 283 284 285 286 287 288 289 290\n#> [261] 291 292 293 294 295 296 297 299 301 302 303 304 306\n#> [274] 307 309 311 317 318 322 323 325 326 328 329 330 331\n#> [287] 332 333 334 336 337 338 340 341 342 343 344 345 346\n#> [300] 347 348 350 351 353 354 355 356 357 358 359 360 361\n#> [313] 364 368 369 370 371 372 373 374 375 376 377 378 379\n#> [326] 380 381 382 383 384 385 386 387 388 389 390 391 392\n#> [339] 393 394 396 398 400 401 402 403 404 405 406 407 408\n#> [352] 409 410 411 412 415 416 417 418 419 420 421 422 423\n#> [365] 426 427 428 429 430 431 432 434 436 437 438 439 440\n#> [378] 441 442 443 444 445 446 447 448 449 450 451 452 453\n#> [391] 454 455 456 458 459 460 461 462 463 464 465 466 467\n#> [404] 469 471 472 473 474 475 476 478 480 481 484 485 486\n#> [417] 488 489 490 491 492 493 494\nws1a <- ws1a %>%\n  mutate(subno = factor(paste0(\"e1a_\", subno)))\nws1b <- ws1b %>%\n  mutate(subno = factor(paste0(\"e1b_\", subno)))\nws1 <- bind_rows(ws1a, ws1b)\nglimpse(ws1)\n#> Rows: 49,984\n#> Columns: 6\n#> $ subno     <fct> e1a_8, e1a_8, e1a_8, e1a_8, e1a_8, e1a_8~\n#> $ loss      <dbl> 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 8~\n#> $ gain      <dbl> 6, 8, 10, 12, 14, 16, 18, 20, 6, 8, 10, ~\n#> $ response  <chr> \"accept\", \"accept\", \"accept\", \"accept\", ~\n#> $ condition <dbl> 20.2, 20.2, 20.2, 20.2, 20.2, 20.2, 20.2~\n#> $ resp      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1~\nlength(unique(ws1a$subno)) + length(unique(ws1b$subno))\n#> [1] 781\nlength(unique(ws1$subno))\n#> [1] 781"},{"path":"tidyverse-exercises.html","id":"preparing-the-data-set-and-descriptive-analysis","chapter":"tidyverse: Example, Quiz, and Exercises","heading":"3.7.3 Preparing the Data Set and Descriptive Analysis","text":"Now combined two data sets one tibble, can prepare analysis. , turn categorical variables factors, shown .Next, calculate number participants per condition check every participant 64 trials. one way check assumptions data ensure data integrity (use code described Section 3.5.3.5).Finally, time perform descriptive analysis data. main result reported data mean acceptance rates symmetric lotteries appear conditions. code given Section @ref(group_by), now use combined data. code reproduces results reported Section 1.2.2.","code":"\nws1 <- ws1 %>% \n  mutate(\n    subno = factor(subno),\n    response = factor(response, levels = c(\"reject\", \"accept\")),\n    condition = factor(\n      condition, \n      levels = c(40.2, 20.2, 40.4, 20.4), \n      labels = c(\"-$20/+$40\", \"-$20/+$20\", \"-$40/+$40\", \"-$40/+$20\")\n    )\n  )\nws1\n#> # A tibble: 49,984 x 6\n#>   subno  loss  gain response condition  resp\n#>   <fct> <dbl> <dbl> <fct>    <fct>     <dbl>\n#> 1 e1a_8     6     6 accept   -$20/+$20     1\n#> 2 e1a_8     6     8 accept   -$20/+$20     1\n#> 3 e1a_8     6    10 accept   -$20/+$20     1\n#> 4 e1a_8     6    12 accept   -$20/+$20     1\n#> 5 e1a_8     6    14 accept   -$20/+$20     1\n#> 6 e1a_8     6    16 accept   -$20/+$20     1\n#> # ... with 49,978 more rows\nws1 %>% \n  group_by(condition, subno) %>% \n  summarise(n = n()) %>% \n  group_by(condition) %>% \n  summarise(\n    n_participants = n(),\n    all_64 = all(n == 64)\n    )\n#> `summarise()` has grouped output by 'condition'. You can override using the `.groups` argument.\n#> # A tibble: 4 x 3\n#>   condition n_participants all_64\n#>   <fct>              <int> <lgl> \n#> 1 -$20/+$40            191 TRUE  \n#> 2 -$20/+$20            202 TRUE  \n#> 3 -$40/+$40            190 TRUE  \n#> 4 -$40/+$20            198 TRUE\nws1 %>%\n  filter(loss == gain, loss %in% c(12, 16, 20)) %>% \n  group_by(condition) %>% \n  summarise(mean_acc = mean(resp)) %>% \n  ungroup()\n#> # A tibble: 4 x 2\n#>   condition mean_acc\n#>   <fct>        <dbl>\n#> 1 -$20/+$40    0.213\n#> 2 -$20/+$20    0.5  \n#> 3 -$40/+$40    0.454\n#> 4 -$40/+$20    0.710"},{"path":"tidyverse-exercises.html","id":"quiz","chapter":"tidyverse: Example, Quiz, and Exercises","heading":"3.8 Quiz","text":"Note: pull-menu selecting answer turns green selecting correct answer.Exercise 3.1  NA stand R?“available,” .e., missing data“answer,” .e, R couldn’t find answer command“nothing applicable,” .e., values/variables match requestAnswer: 123Exercise 3.2  following true R package?collection functions extend R.Needs installed CRAN every time use .Needs loaded every time use .Answer: 123Exercise 3.3  “bugs” programming languages?variables interest specific analysisthe code used multiple times different datasetserror code leads unexpected/wrong resultsAnswer: 123Exercise 3.4  tibble?variant data.framethe tiydverse version vectorthe result get data wranglingAnswer: 123Exercise 3.5  operator %>% means using…base Ra pipea data.frameAnswer: 123Exercise 3.6  dplyr package useful :picking variables based namesadding new variablessummarizing multiple valueschanging ordering rowsreading file csvAnswer: 12345Exercise 3.7  Using dplyr package, function used adding/changing variables current data?mutate()summarise()filter()select()Answer: 1234Exercise 3.8  Using dplyr package, function used selecting subsets rows?mutate()summarise()filter()select()Answer: 1234Exercise 3.9  Using dplyr package, function used selecting subsets columns?mutate()summarise()`filter()select()Answer: 1234Exercise 3.10  using pipe operator %>%, R executes operations order?innermost outermost functionThat depends packages loaded.left right.Answer: 123","code":""},{"path":"tidyverse-exercises.html","id":"practical-helpers-cheatsheets","chapter":"tidyverse: Example, Quiz, and Exercises","heading":"3.9 Practical Helpers: Cheatsheets","text":"Trying remember applying tidyverse tools can bit overwhelming beginning. quite functions one remember syntax . Furthermore, far introduced important functions, dplyr tidyverse packages space introduce (see “R data science”).One way keep overview functions help cheat sheets (cheatsheets). Cheat sheets short (usually 1 2 pages) documents provide overview package tool common resource many programming languages. benefit compared learning resources books documentation provide concise overview can point quickly right tool.case R, know function use (e.g., thanks cheat sheet) might make sense follow looking documentation. documentation can obtained function entering ?functionname (e.g., ?summarise) R prompt. provides explanation function plus arguments, also usually helpful examples.tiydverse packages, RStudio provides almost complete list cheat sheets. relevant us moment dplyr cheat sheet. Furthermore, also cheat sheets RStudio IDE iteself, reading data, even base R. diving examples , might make sense download (even print) cheat sheets side trying solve problems.","code":""},{"path":"tidyverse-exercises.html","id":"earworms-task1","chapter":"tidyverse: Example, Quiz, and Exercises","heading":"3.10 tiydverse Exercise: Earworms and Sleep","text":"exercise work data set Scullin, Gao, Fillmore (2021) investigating research question relationship listening music sleep: “Many people listen music hours every day, often near bedtime. investigated whether music listening affects sleep, focusing rarely explored mechanism: involuntary musical imagery (earworms).” ’m sure experience earworm – hearing particular piece music head, able ‘stop playing.’ seems alien , Wikipedia provides longer introduction., looking data Scullin, Gao, Fillmore (2021)’s Study 2, sleep-lab experiment. Participants came sleep-lab stayed one night controlled environment number objective sleep parameters measured via polysomnography. Throughout night several physiological parameters electroencephalography (EEG) breathing recorded continuously. Participants assigned one two conditions determining music heard just going bed: either condition original versions three pop songs, included lyrics, instrumental version three pop-songs (without lyrics). three songs (“Don’t Stop Believin’” Journey, “Call Maybe” Carly Rae Jepsen, “Shake ” Taylor Swift) chosen previous research shown induce earworms. participants woke morning, asked whether experienced earworms different points night morning.researchers interested two research questions:listening original version pop song (lyrics) instrumental version affect whether participants develop earworms? research question based previous results authors expected instrumental version induce earworms lyrical versions.listening original version pop song (lyrics) instrumental version affect whether participants develop earworms? research question based previous results authors expected instrumental version induce earworms lyrical versions.sleep quality differ participants lyrical music condition instrumental music condition (participants expected earworms)?sleep quality differ participants lyrical music condition instrumental music condition (participants expected earworms)?looking data, take guess . believe sleep better listening music makes likely develop earworm?version original data file made available Scullin, Gao, Fillmore (2021) can downloaded . recommend download data folder working directory can read data via following code. makes sense restart R , also reload tidyverse.Alternatively, can also download directly internet exercise, really recommend getting comfortable files, folders, R working directory. code downloading internet :Let’s take first look data:addition variables relevant research questions, data contains number control variables (original data file included even ). can see total 19 variables:id: Participant identifiergroup: experimental condition two values: “Lyrics” versus “Instrumental”relaxed: question asking participants relaxed felt scale 0 (= relaxed) 100 (= relaxed) listening music.sleepy: question asking participants sleepy felt scale 0 (= sleepy) 100 (= sleepy) listening music.\nnext 5 variables concerned whether participants reported earworm different times:earworm_falling_asleep: earworm trying fall asleep last night? 0 = ; 1 = Yesearworm_night: earworm woke night? 0 = ; 1 = Yesearworm_falling_asleep: earworm woke night? 0 = ; 1 = Yesearworm_morning: earworm woke morning? 0 = ; 1 = Yesearworm_control: earworm control time point (.e., getting ready leave lab)? 0 = ; 1 = Yessleep_efficiency: sleep efficiency percentage score obtained polysomnography (0 = low sleep efficiency 100 = high sleep efficiency)sleep_time: total sleep time minutesThe remaining variables demographic ones whose meaning evident, except, perhaps, classrank. uses US nomenclature year 4 student participant (.e., freshman = year 1, sophomore = year 2, junior = year 3, senior = year 4) plus additional values.Exercise 3.11  research questions 1 2 described , variables independent dependent variables?answer question require coding. Instead come solution just thinking description experiment variables.Also, remember experiment test effect independent variable dependent variable.independent variable research questions experimental condition, variable group.research question 1, three dependent variables, beginning earworm_: earworm_falling_asleep, earworm_night, earworm_morning. earworm_control dependent variable research question, control variable (.e., see effect group dependent variable). [Note categorisation earworm_control control variable decision original authors. think principle still effect manipulation variable collected experimental manipulation. However, declared like , go categorisation .]research question 2, one main dependent variable, sleep_efficiency measure sleep quality. also consider time participant sleeps, sleep_time, secondary measure sleep quality.Exercise 3.12  can categorise variables, relaxed, age, etc., neither independent dependent variables? results pattern expect variables?variables can understood control variables. two explicitly psychological measures, relaxed sleepy, taken right experimental manipulation (.e., listening either lyrical instrumental music). hope type music affect measure. control variables, age following, demographic variables. can use check whether randomisation led unbalanced conditions. discussed , participant identifier, id, also control variable.Exercise 3.13  starting analysis, prepare data transform important categorical variables factors avoid problems. task: Transform id group factor. group make sure original version lyrics factor level 1.Sounds like job mutate() factor().Exercise 3.14  first step analysis, need get overview data. , first need learn something structure data. requires two steps:many rows data participant occupy?many participants condition?Sounds like job summarise()Looks like every id one row. However, let us make sure indeed case.Yes, everyone one row. discussed , one observation per participant distinction wide long format, can use data format.one observation per participant, can directly count number participants per condition counting number rows per condition.25 participants lyrics condition 23 instrumental condition.Exercise 3.15  looking results, ’s good idea get overview demographics data. first step, let us provide basic overview participants ignoring condition put :Provide summaries age participants (min, max, mean, sd).gender distribution?ethnicity distribution participants?calculating distribution categorical variable, calculate frequencies, also proportions (.e., relative frequencies).getting summary age, summarise() seems like good idea.questions, also use summarise() conditional categorical variables alternatively count(). can calculate proportions frequencies using formula proportion = frequency / total sum.age distribution look age variable:Looks like participants student age range.get absolute frequencies categorical variable can just :can add proportions resulting tibble using mutate():Around 71% participants women rest men.ethnicity item can use logic/code:Around 60% Caucasian, 19% Hispanic, rest Asian (12%) African American (8%).Exercise 3.16  Let us now calculate conditional demographic distributions; , demographic distribution conditional experimental condition group. reason check randomisation created balanced unbalanced groups control variables collected.can re-use code previous exercise, need add group_by() .age distribution can essentially copy code just need add group_by(group):Participants instrumental condition average around 1 year older participants lyrics condition, seems small given SD much larger. However, difference mostly due larger maximum value, also explain difference SDs. check , can split data two tibbles, one condition, take look sorted ages.suggests one particularly old participant instrumental condition, several (.e., four oldest participants instrumental condition). might age imbalance two conditions.calculate conditional gender distributions, first calculate number participants group gender combination. , need group experimental condition group calculate relative frequencies within condition. , can also use mutate() conditional grouping factor created group_by() (.e., sum(n) part mutate() call executed separately experimental condition group_by() part).output shows gender ratio balanced groups. Whereas see roughly 50-50 gender ratio instrumental condition, lyrical condition closer 80-20.Finally, let’s use logic ethnicity question. Note resulting table long shown fully, pipe print(n = Inf) done .show major differences two conditions. proportion Asian participants somewhat imbalanced.Overall, results show two groups perfectly balanced, even though groups randomly created. can happen experiments, fact expected, especially group sizes small (.e., maximum 25 participants per condition). larger groups, likely balanced.groups unbalanced, problem? necessarily. remember, problem randomisation attempts address confounders – third variables can affect dependent variable. Imbalance experiments thus problematic imbalance variables known also affect dependent variable. question age gender variables. much sleep researcher, assume effect gender sleep efficiency extremely large. However, age surely effect sleep, doubt whether true age range participants . age range larger (e.g., one group participants 40s older ), concerned. differences seem rather unimportant overall. Nevertheless, main analysis Scullin, Gao, Fillmore (2021) adjust difference gender, statistical technique get back later.Exercise 3.17  addition demographic variables, also get overview two control variables measuring participants’ responses state listening music, variables relaxed sleepy. Check whether look similar across condition calculating mean SD.job group_by() summarise() combination one frequent combination use tidyverse.differences relaxed sleepiness question listening music seem minor given variability (.e., SD) data.Exercise 3.18  now carefully checked explored data, ’s time take look research question 1: listening original version pop song (lyrics) instrumental version affect whether participants develop earworms?Let us look research question multiple steps:Calculate whether experimental condition (group) affects three earworm dependent variables well earworm control variable. , calculate conditional means well standard deviations.Calculate two new dependent variables three dependent variables (.e., earworm_... variables exception earworm_control): earworm (0 participant report earworm 1 otherwise) proportion earworms (sum three earworm dependent variables divided 3). calculate whether group affects two new dependent variables.analysis support prediction instrumental music leads earworms lyrical music?already know drill: Calculating conditional means standard deviations done group_by() summarise(). calculate new variables (analysis 2), need use mutate().Analysis 1: Separate analysis dependent variable:Whereas correct code, show variables returned data. One easy way ensure variables shown convert tibble back data.frame.can see two three dependent variables, earworms falling asleep morning, proportion earworms noticeably larger instrumental lyrical condition. proportion earworms night control time, see much smaller difference even small difference opposite direction.Analysis 2: Analysis new dependent variables.Step 1: Calculate new variablesWe first calculate new variables using mutate(). , realise any_earworm variable function prop_earworm (= proportion earworm) variable, calculate latter first. calculate proportion variable just add three variables divide 3, mindful need put parentheses around + operations.can calculate any_earworm variable prop_variable. Note new mutate() call. reason need refer prop_variable variable can lead problems using variables created mutate() call. might work, guaranteed, better avoid .calculate any_earworm variable, need check prop_earworm variable 0 assign 0 variable 1 otherwise. use if_else() function allows us apply -else logic, , vector (.e., called vector, if_else() also returns vector). , -part checks prop_earworm == 0; , return 0, 1 otherwise.save new variable back earworm data can use .non-trivial calculation, end printing first rows results see calculated variables show . make sure make errors. printing tibble per default now show variables want ot see, first select relevant variables, convert tibble data.frame, use head() get first rows. shows seems work. However, see enough rows 1s, repeat code print last rows using tail(). output telling logic worked.Step 2: Calculate conditional means SDsThis shows new dependent variables, see difference expected direction.Overall answer: analysis overall appears support prediction listening instrumental compared lyrical music leads earworms. However, still need, point, proper statistical analysis get better understanding.Exercise 3.19  Let’s now take look research question 2: sleep quality differ participants lyrical instrumental music condition (participants expected earworms)?Check effect independent variable main measure sleep quality, sleep_efficiency, also length sleep, sleep_time. Don’t forget calculate means SD judge size difference given level noise.addition just calculating conditional means, also calculate exact difference sleep efficiency sleeping time conditions.data overall support prediction?Conditional means job group_by() summarise() combination.analysis can straightforwardly adapted previous code. However, also want calculate difference exactly, save result new object, print .shows small difference sleep efficiency well time slept, predicted direction (.e., higher sleep quality lyrical compared instrumental music). However, SD sleep efficiency much smaller time slept, perhaps surprising given different magnitudes two variables (differing factor 5).Let us now calculate observed difference understand pattern better. tidyverse can done :shows can use normal vector indexing (.e., [1] [2]) vector inside summarise().results also obtained base R :Overall, results show difference sleep quality around 50% observed SD measures. research question 1, statistical analysis necessary better understand pattern.However, can already say differences large absolute terms. example, difference time slept around 10 minutes average sleep time corresponds 8 hours:Exercise 3.20  analysis far supports prediction research questions: Participants listen instrumental compared lyrical songs report earworms sleep worse. However, analysis address question whether participants report earworms sleep worse participants report earworms.last bit analysis, let’s take look question: participants report earworm sleep better participants , ignoring experimental condition. :Create new factor, has_earworm, yes, whenever participants report least earworm one three earworm dependent variables (.e., within experimental time frame, ignoring control time).Calculate conditional N (number participants) well conditional means SD two sleep variables, sleep_efficiency sleep_time, function has_earworm.can learn analysis?create new factor, has_earworm, can transform any_earworm factor:, can use previous code calculate conditional means SDs.analysis shows around 40% report earworm experimental time frame. also interesting pattern: Participants report earworm lower sleep efficiency scores sleep slightly longer, compared participants report earworm.can also calculate difference. Note need change order get interpretation (.e., positive means line prediction).can learn analysis looks earworm leads reduced sleep efficiency. pattern sleep length difficult interpret (also lot smaller compared variability data measured SD).However, analysis lose benefit randomisation. really know earworm something related earworm drives effect. Nevertheless, really another way introducing earworm playing different types music, still important result.","code":"\nlibrary(\"tidyverse\")\nearworm <- read_csv(\"data/earworm_study.csv\")\n#> Rows: 48 Columns: 19\n#> -- Column specification ------------------------------------\n#> Delimiter: \",\"\n#> chr  (7): group, gender, raceethnicity, nativeenglishspe...\n#> dbl (12): id, relaxed, sleepy, earworm_falling_asleep, e...\n#> \n#> i Use `spec()` to retrieve the full column specification for this data.\n#> i Specify the column types or set `show_col_types = FALSE` to quiet this message.\nearworm <- read_csv(\"https://github.com/singmann/stats_for_experiments/raw/master/data/earworm_study.csv\")\nglimpse(earworm)\n#> Rows: 48\n#> Columns: 19\n#> $ id                     <dbl> 102, 103, 104, 105, 106, 10~\n#> $ group                  <chr> \"Lyrics\", \"Lyrics\", \"Instru~\n#> $ relaxed                <dbl> 37, 96, 62, 83, 68, 78, 60,~\n#> $ sleepy                 <dbl> 49, 10, 44, 97, 42, 52, 41,~\n#> $ earworm_falling_asleep <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n#> $ earworm_night          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n#> $ earworm_morning        <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, ~\n#> $ earworm_control        <dbl> 1, 1, 0, 0, 0, 0, 1, 0, 0, ~\n#> $ sleep_efficiency       <dbl> 90.0, 95.7, 89.8, 94.1, 98.~\n#> $ sleep_time             <dbl> 487.6, 519.4, 472.1, 494.8,~\n#> $ age                    <dbl> 19, 21, 19, 20, 27, 18, 22,~\n#> $ gender                 <chr> \"Female\", \"Female\", \"Female~\n#> $ raceethnicity          <chr> \"Caucasian\", \"Caucasian\", \"~\n#> $ nativeenglishspeaker   <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\",~\n#> $ bilingual              <chr> \"No\", \"No\", \"No\", \"No\", \"No~\n#> $ height_inches          <dbl> 66, 68, 64, 69, 53, 62, 76,~\n#> $ weight_lbs             <dbl> 145, 183, 135, 155, 89, 120~\n#> $ handedness             <chr> \"R\", \"L\", \"R\", \"R\", \"R\", \"R~\n#> $ classrank              <chr> \"Sophomore\", \"Junior\", \"Fre~\nearworm <- earworm %>% \n  mutate(\n    id = factor(id),\n    group = factor(group, levels = c(\"Lyrics\", \"Instrumental\"))\n  )\nearworm %>% \n  group_by(id) %>% \n  summarise(n = n()) \n#> # A tibble: 48 x 2\n#>   id        n\n#>   <fct> <int>\n#> 1 102       1\n#> 2 103       1\n#> 3 104       1\n#> 4 105       1\n#> 5 106       1\n#> 6 107       1\n#> # ... with 42 more rows\nearworm %>% \n  group_by(id) %>% \n  summarise(n = n()) %>% \n  summarise(all_1 = all(n == 1))\n#> # A tibble: 1 x 1\n#>   all_1\n#>   <lgl>\n#> 1 TRUE\nearworm %>% \n  group_by(group) %>% \n  summarise(n = n()) \n#> # A tibble: 2 x 2\n#>   group            n\n#>   <fct>        <int>\n#> 1 Lyrics          25\n#> 2 Instrumental    23\nearworm %>% \n  summarise(mean = mean(age),\n            min = min(age),\n            max = max(age),\n            sd = sd(age))\n#> # A tibble: 1 x 4\n#>    mean   min   max    sd\n#>   <dbl> <dbl> <dbl> <dbl>\n#> 1  21.0    18    28  2.20\nearworm %>% \n  group_by(gender) %>% \n  summarise(n = n())\n#> # A tibble: 2 x 2\n#>   gender     n\n#>   <chr>  <int>\n#> 1 Female    34\n#> 2 Male      14\nearworm %>% \n  group_by(gender) %>% \n  summarise(n = n()) %>% \n  mutate(prop = n / sum(n))\n#> # A tibble: 2 x 3\n#>   gender     n  prop\n#>   <chr>  <int> <dbl>\n#> 1 Female    34 0.708\n#> 2 Male      14 0.292\nearworm %>% \n  group_by(raceethnicity) %>% \n  summarise(n = n()) %>% \n  mutate(prop = n / sum(n))\n#> # A tibble: 4 x 3\n#>   raceethnicity        n   prop\n#>   <chr>            <int>  <dbl>\n#> 1 African American     4 0.0833\n#> 2 Asian                6 0.125 \n#> 3 Caucasian           29 0.604 \n#> 4 Hispanic             9 0.188\nearworm %>% \n  group_by(group) %>% \n  summarise(mean = mean(age),\n            min = min(age),\n            max = max(age),\n            sd = sd(age))\n#> # A tibble: 2 x 5\n#>   group         mean   min   max    sd\n#>   <fct>        <dbl> <dbl> <dbl> <dbl>\n#> 1 Lyrics        20.4    18    24  1.63\n#> 2 Instrumental  21.6    18    28  2.59\nearworm_lyrics <- earworm %>% \n  filter(group == \"Lyrics\")\nearworm_instrumental <- earworm %>% \n  filter(group == \"Instrumental\")\nearworm_lyrics$age %>% \n  sort()\n#>  [1] 18 18 18 19 19 19 19 19 20 20 20 20 20 20 21 21 21 21\n#> [19] 21 22 22 22 23 23 24\nearworm_instrumental$age %>% \n  sort()\n#>  [1] 18 19 19 19 20 20 20 20 21 21 21 21 21 21 21 22 22 22\n#> [19] 22 25 26 27 28\nearworm %>% \n  group_by(group, gender) %>% \n  summarise(n = n()) %>%\n  group_by(group) %>%   \n  mutate(prop = n / sum(n))\n#> `summarise()` has grouped output by 'group'. You can override using the `.groups` argument.\n#> # A tibble: 4 x 4\n#> # Groups:   group [2]\n#>   group        gender     n  prop\n#>   <fct>        <chr>  <int> <dbl>\n#> 1 Lyrics       Female    21 0.84 \n#> 2 Lyrics       Male       4 0.16 \n#> 3 Instrumental Female    13 0.565\n#> 4 Instrumental Male      10 0.435\nearworm %>% \n  group_by(group, raceethnicity) %>% \n  summarise(n = n()) %>%\n  group_by(group) %>%   \n  mutate(prop = n / sum(n)) %>% \n  print(n = Inf)\n#> `summarise()` has grouped output by 'group'. You can override using the `.groups` argument.\n#> # A tibble: 8 x 4\n#> # Groups:   group [2]\n#>   group        raceethnicity        n   prop\n#>   <fct>        <chr>            <int>  <dbl>\n#> 1 Lyrics       African American     3 0.12  \n#> 2 Lyrics       Asian                5 0.2   \n#> 3 Lyrics       Caucasian           13 0.52  \n#> 4 Lyrics       Hispanic             4 0.16  \n#> 5 Instrumental African American     1 0.0435\n#> 6 Instrumental Asian                1 0.0435\n#> 7 Instrumental Caucasian           16 0.696 \n#> 8 Instrumental Hispanic             5 0.217\nearworm %>% \n  group_by(group) %>% \n  summarise(m_relaxed = mean(relaxed),\n            sd_relaxed = sd(relaxed),\n            m_sleepy = mean(sleepy),\n            sd_sleep = sd(sleepy))\n#> # A tibble: 2 x 5\n#>   group        m_relaxed sd_relaxed m_sleepy sd_sleep\n#>   <fct>            <dbl>      <dbl>    <dbl>    <dbl>\n#> 1 Lyrics            72.4       19.6     68.7     23.2\n#> 2 Instrumental      70.7       20.7     66.2     18.3\nearworm %>% \n  group_by(group) %>% \n  summarise(m_fall = mean(earworm_falling_asleep),\n            sd_fall = sd(earworm_falling_asleep),\n            m_night = mean(earworm_night),\n            sd_night = sd(earworm_night),\n            m_morning = mean(earworm_morning),\n            sd_morning = sd(earworm_morning),\n            m_cont = mean(earworm_control),\n            sd_cont = sd(earworm_control)\n  )\n#> # A tibble: 2 x 9\n#>   group m_fall sd_fall m_night sd_night m_morning sd_morning\n#>   <fct>  <dbl>   <dbl>   <dbl>    <dbl>     <dbl>      <dbl>\n#> 1 Lyri~  0.12    0.332  0.08      0.277     0.12       0.332\n#> 2 Inst~  0.261   0.449  0.0435    0.209     0.391      0.499\n#> # ... with 2 more variables: m_cont <dbl>, sd_cont <dbl>\nearworm %>% \n  group_by(group) %>% \n  summarise(m_fall = mean(earworm_falling_asleep),\n            sd_fall = sd(earworm_falling_asleep),\n            m_night = mean(earworm_night),\n            sd_night = sd(earworm_night),\n            m_morning = mean(earworm_morning),\n            sd_morning = sd(earworm_morning),\n            m_cont = mean(earworm_control),\n            sd_cont = sd(earworm_control)\n  )  %>% \n  as.data.frame()\n#>          group    m_fall   sd_fall    m_night  sd_night\n#> 1       Lyrics 0.1200000 0.3316625 0.08000000 0.2768875\n#> 2 Instrumental 0.2608696 0.4489778 0.04347826 0.2085144\n#>   m_morning sd_morning    m_cont   sd_cont\n#> 1 0.1200000  0.3316625 0.4000000 0.5000000\n#> 2 0.3913043  0.4990109 0.4782609 0.5107539\nearworm <- earworm %>% \n  mutate(\n    prop_earworm = (earworm_falling_asleep + earworm_night + \n                      earworm_morning) / 3\n  ) %>% \n  mutate(any_earworm = if_else(prop_earworm == 0, 0, 1)) \n\nearworm %>% \n  select(id, prop_earworm, any_earworm, starts_with(\"earworm\"))  %>% \n  as.data.frame() %>% \n  head()\n#>    id prop_earworm any_earworm earworm_falling_asleep\n#> 1 102            0           0                      0\n#> 2 103            0           0                      0\n#> 3 104            0           0                      0\n#> 4 105            0           0                      0\n#> 5 106            0           0                      0\n#> 6 107            0           0                      0\n#>   earworm_night earworm_morning earworm_control\n#> 1             0               0               1\n#> 2             0               0               1\n#> 3             0               0               0\n#> 4             0               0               0\n#> 5             0               0               0\n#> 6             0               0               0\nearworm %>% \n  select(id, prop_earworm, any_earworm, starts_with(\"earworm\"))  %>% \n  as.data.frame() %>% \n  tail()\n#>     id prop_earworm any_earworm earworm_falling_asleep\n#> 43 145    0.3333333           1                      1\n#> 44 146    0.3333333           1                      1\n#> 45 147    0.0000000           0                      0\n#> 46 148    0.3333333           1                      0\n#> 47 149    0.0000000           0                      0\n#> 48 151    0.3333333           1                      1\n#>    earworm_night earworm_morning earworm_control\n#> 43             0               0               0\n#> 44             0               0               1\n#> 45             0               0               0\n#> 46             0               1               1\n#> 47             0               0               0\n#> 48             0               0               1\nearworm %>% \n  group_by(group) %>% \n  summarise(m_any = mean(any_earworm),\n            sd_any = sd(any_earworm),\n            m_prop = mean(prop_earworm),\n            sd_prop = sd(prop_earworm)\n  )\n#> # A tibble: 2 x 5\n#>   group        m_any sd_any m_prop sd_prop\n#>   <fct>        <dbl>  <dbl>  <dbl>   <dbl>\n#> 1 Lyrics       0.28   0.458  0.107   0.186\n#> 2 Instrumental 0.522  0.511  0.232   0.274\nsummary_sleep <- earworm %>% \n  group_by(group) %>% \n  summarise(m_efficiency = mean(sleep_efficiency),\n            sd_efficiency = sd(sleep_efficiency),\n            m_time = mean(sleep_time),\n            sd_time = sd(sleep_time)\n  )\nsummary_sleep\n#> # A tibble: 2 x 5\n#>   group        m_efficiency sd_efficiency m_time sd_time\n#>   <fct>               <dbl>         <dbl>  <dbl>   <dbl>\n#> 1 Lyrics               94.3          2.97   507.    24.5\n#> 2 Instrumental         91.9          4.75   498.    26.0\nsummary_sleep %>% \n  summarise(diff_efficiency = m_efficiency[1] - m_efficiency[2],\n            diff_time = m_time[1] - m_time[2])\n#> # A tibble: 1 x 2\n#>   diff_efficiency diff_time\n#>             <dbl>     <dbl>\n#> 1            2.43      9.68\nsummary_sleep$m_efficiency[1] - summary_sleep$m_efficiency[2]\n#> [1] 2.425391\n\nsummary_sleep$m_time[1] - summary_sleep$m_time[2]\n#> [1] 9.67687\n500 / 60  ## sleep time in minutes / 60 gives time in hours\n#> [1] 8.333333\nearworm <- earworm %>% \n  mutate(has_earworm = factor(\n    any_earworm,\n    levels = c(1, 0),\n    labels = c(\"yes\", \"no\")\n  ))\nsummary_earworms <- earworm %>% \n  group_by(has_earworm) %>% \n  summarise(n = n(),\n            m_efficiency = mean(sleep_efficiency),\n            sd_efficiency = sd(sleep_efficiency),\n            m_time = mean(sleep_time),\n            sd_time = sd(sleep_time)\n  )\nsummary_earworms %>% \n  mutate(prop = n / sum(n))\n#> # A tibble: 2 x 7\n#>   has_earworm     n m_efficiency sd_efficiency m_time\n#>   <fct>       <int>        <dbl>         <dbl>  <dbl>\n#> 1 yes            19         91.4          4.17   508.\n#> 2 no             29         94.3          3.64   500.\n#> # ... with 2 more variables: sd_time <dbl>, prop <dbl>\n\n## want to see all variables:\nsummary_earworms %>% \n  mutate(prop = n / sum(n)) %>% \n  as.data.frame()\n#>   has_earworm  n m_efficiency sd_efficiency   m_time\n#> 1         yes 19     91.41579      4.174428 507.5421\n#> 2          no 29     94.27931      3.635183 499.6586\n#>    sd_time      prop\n#> 1 26.35405 0.3958333\n#> 2 24.71534 0.6041667\nsummary_earworms %>% \n  summarise(diff_efficiency = m_efficiency[2] - m_efficiency[1],\n            diff_time = m_time[2] - m_time[1])\n#> # A tibble: 1 x 2\n#>   diff_efficiency diff_time\n#>             <dbl>     <dbl>\n#> 1            2.86     -7.88"},{"path":"ggplot2-intro.html","id":"ggplot2-intro","chapter":"4 Data Visualisation with ggplot2","heading":"4 Data Visualisation with ggplot2","text":"far considered data analysis exclusively happens R console. , put numbers text console get numbers text . However, one part comprehensive data analysis. Another part data analysis transformation numbers pictures – data visualisation.One way create data visualisation R base R graphics engine primarily using plot() function. plot(), creating plot done drawing individual graphical elements, points() lines(). functions generally accept individual data points vectors arguments.Instead introducing base R plotting, introduce tidyverse approach data visualisation, ggplot2. ggplot2 system declaratively creating graphics, based book “Grammar Graphics” (Wilkinson 1999).32 ggplot2 also perhaps prominent member tidyverse. creation pre-dates term tidyverse several years, can also seen one founding packages.contrast base R plot functionality works vectors, ggplot2 requires data passed either data.frame tibble. hand, “provide data, tell ggplot2 map variables aesthetics, graphical primitives use, takes care details.” (quote official documentation).","code":""},{"path":"ggplot2-intro.html","id":"first-ggplot2-example","chapter":"4 Data Visualisation with ggplot2","heading":"4.1 First ggplot2 Example","text":"means practice first question need answer plotting ggplot2 variables data want show two axes. decided , consider graphical elements (called geoms ggplot2 terminology), want use show data. Let’s exemplify first example Walasek Stewart (2015) data also used Chapter 3 . Let’s begin loading data done (suppress status messages output ):One intuition probability lottery accepted related size potential gain, average independent variables (.e., potential loss condition). investigate , first need calculate average acceptance probability possible gain. , can create plot average accept probability y-axis possible gain x-axis. want plot average acceptance probabilities raw data (.e., individual observations), need summarise raw data first. , use now well-known combination group_by() summarise().Looking first rows returned tibble suggests intuition probably far mean acceptance rates appear increase gain. understand pattern better, let us plot data. , pass newly created tibble ggplot() function shown next. first interpret see plot describing call created detail.plot shows clear relationship mean acceptance rate lottery potential gain. average holds larger gain larger mean acceptance probability.Let us now describe call produces plot detail:create plot ggplot2 usually call ggplot() function. first argument ggplot() data want plot. data first argument, can also directly pipe tibble ggplot() function, see later.second argument aes() function, used map variables data onto aesthetics. Aesthetics ggplot2 terminology means graphical element feature plot can change function variable. , consider two aesthetics, x y axes, gain mapped onto x axis mean_acceptance mapped onto y axis. words, data point larger potential gain larger value x-axis (.e., appears right) data point lower potential gain. Likewise, data point larger mean_acceptance larger value y-axis data point lower mean_acceptance. Whereas x y axes probably important aesthetics, can also use others colour, shape, size graphical elements. map variable aesthetics, graphical elements plot vary function variable corresponding aesthetic. example, map variable onto size aesthetics, data points value variable larger plotted larger data points data points value variable smaller (see example ). experience, understanding use aes() function conceptually difficult part learning ggplot2, discouraged immediately fully clear. understanding develop throughout chapter examples. problem seems “aesthetics” represent rather abstract way thinking creating plots best learned examples. now, remember aes() allows specify aesthetics plot change function variable data. example, x = gain means position data point x-axis depends value gain, y = mean_acceptance means position data point y-axis depends value mean_acceptanceAfter passing data specifying aesthetics, close ggplot() call (.e., close opening parenthesis) add arguments, passed function calls, plot using +.important set arguments pass geoms (geometric objects). pass geom_point() indicate want plot points data.Another important aspect ggplot2 need use one geom, can specify multiple ones. example, add line top connecting data points:33","code":"\nlibrary(\"tidyverse\")\nws1a <- read_csv(\"data/ws2015_exp1a.csv\")\nws1b <- read_csv(\"data/ws2015_exp1b.csv\")\nws1a <- ws1a %>%\n  mutate(subno = factor(paste0(\"e1a_\", subno)))\nws1b <- ws1b %>%\n  mutate(subno = factor(paste0(\"e1b_\", subno)))\nws1 <- bind_rows(ws1a, ws1b)\nws1 <- ws1 %>% \n  mutate(\n    response = factor(response, levels = c(\"reject\", \"accept\")),\n    condition = factor(\n      condition, \n      levels = c(40.2, 20.2, 40.4, 20.4), \n      labels = c(\"-$20/+$40\", \"-$20/+$20\", \"-$40/+$40\", \"-$40/+$20\")\n    )\n  )\nglimpse(ws1)\n#> Rows: 49,984\n#> Columns: 6\n#> $ subno     <fct> e1a_8, e1a_8, e1a_8, e1a_8, e1a_8, e1a_8~\n#> $ loss      <dbl> 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 8~\n#> $ gain      <dbl> 6, 8, 10, 12, 14, 16, 18, 20, 6, 8, 10, ~\n#> $ response  <fct> accept, accept, accept, accept, accept, ~\n#> $ condition <fct> -$20/+$20, -$20/+$20, -$20/+$20, -$20/+$~\n#> $ resp      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1~\nlot_sum <- ws1 %>% \n  group_by(gain) %>% \n  summarise(mean_acceptance = mean(resp))\nlot_sum\n#> # A tibble: 13 x 2\n#>    gain mean_acceptance\n#>   <dbl>           <dbl>\n#> 1     6          0.0938\n#> 2     8          0.134 \n#> 3    10          0.229 \n#> 4    12          0.239 \n#> 5    14          0.358 \n#> 6    16          0.345 \n#> # ... with 7 more rows\nggplot(lot_sum, aes(x = gain, y = mean_acceptance)) +\n  geom_point()\nggplot(lot_sum, aes(x = gain, y = mean_acceptance)) +\n  geom_point() +\n  geom_line()"},{"path":"ggplot2-intro.html","id":"ggplot-2-continuous","chapter":"4 Data Visualisation with ggplot2","heading":"4.2 Two Continuous Variables","text":"examples show simple case plotting two continuous variables, dependent variable y-axis independent variable x-axis. ordering, dependent variable y-axis independent variable x-axis, common convention scientific plots generally follow.Let’s now consider cases plotting two continuous variables. plots show data first plot. plots , averaged possible potential losses considered effect size potential gain average acceptance rates. now plot case consider individual lottery – , unique combination potential gain loss – one data point (.e., averaging potential losses one potential gain). simplify matters, consider two symmetric conditions range losses equal range gains. Let’s begin analysis preparing corresponding data.first plot, begin call change data set passed ggplot() function.plot difficult interpret. see lotteries low acceptance rates (0 around 0.2) well another group acceptance rates much higher (around 0.3 0.85). Overall, look larger potential gain clearly associated larger mean acceptance rate.One possibility differing visual impressions might overlap data points near 0; , evidence -plotting. means current plot, differentiate one multiple data points share approximately x y coordinates. -plotting common problem plots show data points.","code":"\nlot_sum2 <- ws1 %>% \n  filter(condition %in% c(\"-$20/+$20\", \"-$40/+$40\")) %>% \n  group_by(loss, gain) %>% \n  summarise(mean_acceptance = mean(resp))\n#> `summarise()` has grouped output by 'loss'. You can override using the `.groups` argument.\nlot_sum2\n#> # A tibble: 119 x 3\n#> # Groups:   loss [13]\n#>    loss  gain mean_acceptance\n#>   <dbl> <dbl>           <dbl>\n#> 1     6     6           0.520\n#> 2     6     8           0.614\n#> 3     6    10           0.777\n#> 4     6    12           0.881\n#> 5     6    14           0.876\n#> 6     6    16           0.916\n#> # ... with 113 more rows\nggplot(lot_sum2, aes(x = gain, y = mean_acceptance)) +\n  geom_point()"},{"path":"ggplot2-intro.html","id":"addressing-over-plotting","chapter":"4 Data Visualisation with ggplot2","heading":"4.2.1 Addressing Over-Plotting","text":"One way address diagnose -plotting alpha blending. Alpha blending computer graphics effect creates visual impression semi-transparency. means alpha blending, overlapping points appear darker whereas non-overlapping points . Thus, alpha blending helpful technique investigating -plotting judging degree -plotting. example, can set alpha = 0.25 geom_point() shown .Maybe wondering set alpha = 0.25 value? answer trial--error. just tried different values saw plot looked good (alpha must lie 0 1, usually start value 0.5) .plot provides evidence -plotting. Especially mean acceptance rates near 0 (especially low values potential gains), see several points clearly darker rest data points. darkness means several points exactly position – -plotting. However, plot still clear. example, still difficult judge many points points appear clearly darker.One possibility improve figure introducing random jitter plotted points using geom_jitter(). Given points discrete x-axis positions – even whole numbers act potentially gains – makes sense add small amount jitter x-axis. done specifying width argument geom_jitter(). Specifically, width requires number specifies amount horizontal jitter units x-axis. Trial--error led conclude width = 0.4 produces appealing result.34 jitter x-axis, points retain exact y-axis positions still shown near original x-axis position.important thing know using geom_jitter() due randomness used add jitter, plot look slightly different every time created (.e., every time code executed). Try running code times RStudio see mean. , feel free try different values amount jitter happens remove width = 0.4 geom_jitter() call. can also try see happens add value height (e.g., height = 0.05).plot , combining jitter alpha blending, makes easier see first plot showed clear relationship potential gain mean acceptance rates averaging losses. many data points low mean acceptance rates left side plot many data points medium high acceptance rates right side plot. imagine taking mean point x-axis position can imagine essentially monotonically increasing mean acceptance rate. (curve monotonically increasing one line can go , remain value, move left right – go .)","code":"\nggplot(lot_sum2, aes(x = gain, y = mean_acceptance)) +\n  geom_point(alpha = 0.25)\nggplot(lot_sum2, aes(x = gain, y = mean_acceptance)) +\n  geom_jitter(width = 0.4, alpha = 0.25)"},{"path":"ggplot2-intro.html","id":"adding-a-third-variable","chapter":"4 Data Visualisation with ggplot2","heading":"4.2.2 Adding a Third Variable","text":"One question plot leaves open question see qualitatively different pattern mean acceptance rates lotteries. mean acceptance rates low others larger? provide answer question going create new factor, ev (expected value), separates expected value lottery three bins: expected value = 0 (.e., symmetric lotteries), negative expected value (.e., potential loss larger potential gain), positive expected value (potential loss smaller potential gain).expected value term introduced far, let’s now. lottery, expected value expected long term payout per lottery. , play lottery , expected value average gain loss play. calculated multiplying potential outcome probability taking sum . example, 50-50 lottery potential outcomes -/$18 +$20, expected value \\(0.5 \\times -18 + 0.5 \\times 20 = 1\\). lotteries Walasek Stewart (2015) 50-50 lotteries (potential outcome probability 50%), can check whether expected value positive negative directly comparing size potential outcome size potential loss.create ev variable, use another dplyr function, case_when(). case_when() vectorised variant multiple branching (.e., -else). allows us create new variable based multiple logical conditions convenient way. argument case_when() consists logical statement, ~ operator (call tilde-operator), return value returned case logical condition true. , three logical cases mapped onto one label describing sign expected value. convert ev variable factor using factor(). call factor() also specify order factor levels.can use new ev variable print points different colour depending value ev. just need map ev variable colour aesthetic aes() call, get interesting plot.clearly informative, plot uses green red colours can difficult distinguish individuals colour blindness (). Therefore, can make plot better two regards: (1) using somewhat nicer ggplot2 theme removes grey background (favourite theme_bw(), ) (2) can use colour-blind-friendly colour palette. use ggthemes::scale_colour_colorblind() (ggthemes:: just means can use function ggthemes package without loading package explicitly beforehand). gives us informative, also visually appealing clear plot.clear, plot shows exactly data previous one, just uses different theme different colours. least eyes, plot lot visually appealing clearer previous one. describing theme detail, let us first discuss see plot.plot shows expected value negative, participants rather unlikely accept lottery (mean acceptance rates 0.2). expected value 0 (.e., symmetric lotteries), acceptance rates start around 0.5 small potential losses gains generally decreases bit increasing loss/gain (just predicted loss aversion, see Section 1.2.1). Finally, lotteries positive expected value (average gain money), acceptance rates 0.5. Overall, plot shows results one expect see, make lot sense. , anyone willing accept bet expect win money compared expect lose money.moving next plot, let us quickly discuss “theme” . ggplot2, theme setting (function call) determines non-data display elements plot fonts, font size, background foreground colours, forth. default ggplot2 theme uses grey background, personally like much. prefer theme_bw() white background. ggplot2 comes themes can see online documentation (scroll examples see ).call shows first example powerful ggplot2 idea mapping variables aesthetics using aes() function . wanted improve plot distinguishing different types data points. just need variable data represents distinction – , created ev variable . can just map variable appropriate aesthetic. , use colour aesthetic allows us distinguish different data points colour. However, also use different aesthetic distinguish different data points. Can think one? Take seconds think clicking solution .Another way distinguish different points shape. examples seen far, used circles show data points. However, also use shapes display data points squares triangles.ggplot2 aesthetic distinguish data points belonging different categories using different shapes called (surprise!) shape. Thus, can easily can change plot changing corresponding argument aes() call:ggplot2 also allows us combine different aesthetics, shape colour. words, can map one variable multiple aesthetics:","code":"\nlot_sum2 <- lot_sum2 %>% \n  mutate(ev = case_when(\n    gain == loss ~ \"EV = 0\",\n    gain < loss ~ \"EV negative\",\n    gain > loss ~ \"EV positive\"\n  )) %>% \n  mutate(\n    ev = factor(ev, levels = c(\"EV negative\", \"EV = 0\", \"EV positive\"))\n  )\nlot_sum2\n#> # A tibble: 119 x 4\n#> # Groups:   loss [13]\n#>    loss  gain mean_acceptance ev         \n#>   <dbl> <dbl>           <dbl> <fct>      \n#> 1     6     6           0.520 EV = 0     \n#> 2     6     8           0.614 EV positive\n#> 3     6    10           0.777 EV positive\n#> 4     6    12           0.881 EV positive\n#> 5     6    14           0.876 EV positive\n#> 6     6    16           0.916 EV positive\n#> # ... with 113 more rows\nggplot(lot_sum2, aes(x = gain, y = mean_acceptance, colour = ev)) +\n  geom_jitter(width = 0.25, alpha = 0.5)\nggplot(lot_sum2, aes(x = gain, y = mean_acceptance, colour = ev)) +\n  geom_jitter(width = 0.25, alpha = 0.5) +\n  ggthemes::scale_colour_colorblind() +\n  theme_bw()\nggplot(lot_sum2, aes(x = gain, y = mean_acceptance, shape = ev)) +\n  geom_jitter(width = 0.25, alpha = 0.5) +\n  ggthemes::scale_colour_colorblind() +\n  theme_bw()\nggplot(lot_sum2, aes(x = gain, y = mean_acceptance, \n                     shape = ev, colour = ev)) +\n  geom_jitter(width = 0.25, alpha = 0.5) +\n  ggthemes::scale_colour_colorblind() +\n  theme_bw()"},{"path":"ggplot2-intro.html","id":"adding-a-fourth-variable","chapter":"4 Data Visualisation with ggplot2","heading":"4.2.3 Adding a Fourth Variable","text":"done data set? yet. one trick can use make plot even informative. can map loss variable onto size data points. , need change aes() part call. Specifically, add size = loss aes() call. allows us understand pattern shown data.Admittedly, adding loss fourth variable leads comparatively complicated plot. Let’s discuss addition adds see really helps understanding data. can now see something , hindsight, surprising. potential loss small, now indicated small data point, average acceptance rate comparatively high.specifically, can see interesting pattern focus one potential gain (.e., one x-axis position) data. example, focus 8 data points potential gain 32 (.e., just x-axis tick 30). look data way, can nicely see size-based ordering: small losses largest mean acceptance rates large losses smallest.35In sum, plot provides comprehensive summary data Walasek Stewart (2015) (least two symmetric conditions displayed ). plot now allows us see happening data across different lotteries, also shows number patterns expect see data (participants’ sensitivity size gains losses). combination allows plot serve two functions:allows us gain new interesting insights data. example, can see participants average decrease willingness accept symmetric lotteries increasing gain/loss, just predicted loss aversion. plot allows us address research question.fact see many patterns expect see provides us kind ‘confidence check’ obvious problems data. discussed , many things can go wrong study design, data collection, analysis. Therefore, fact data shows several patterns intuitively make lot sense (difference whether expected values negative, neutral, positive) increases confidence much went wrong.words, plot shows us new insights research question, also allows us judge quality evidence provided. learn addition theoretically interesting pattern, data looks reasonable trustworthy (passes intuitive confidence checks). means evidence data provides research question stronger evidence data permit confidence checks data aspects contradict expectations.One recommendation follows discussion always explore data using graphical means – , try various different ways plotting data – moving statistical analysis. process, also known exploratory data analysis, really first step proper data analysis.36As seen example , exploratory data analysis generally iterative process. usually start one simple plot based initial idea, often focusing part data. Based plot, gradually develop (e.g., adding additional elements) incorporating additional parts data. Immediately starting complex plot comes mind can frequently bit overwhelming.Graphical exploratory data analysis can also involve quite bit trial--error, mention creativity. Sometimes, especially simple studies, really clear start best way plot data . often, need try different things can see happening.Taken together, graphical exploration data one important ways increasing confidence quality data. graphical exploration, can perform visual confidence checks assure us obvious problems errors data. Especially data set large, difficult see problems just looking spreadsheet data. contrast, graphical data exploration versatile helpful tool really figuring data trying tell us.","code":"\nggplot(lot_sum2, aes(x = gain, y = mean_acceptance, \n                     colour = ev, size = loss)) +\n  geom_jitter(width = 0.25, alpha = 0.5) +\n  ggthemes::scale_colour_colorblind() +\n  theme_bw()"},{"path":"ggplot2-intro.html","id":"faceting-intro","chapter":"4 Data Visualisation with ggplot2","heading":"4.3 Faceting: Creating Sub-Plots Across a Variable","text":"plots focussed two symmetric conditions. However, might also interested extent pattern saw happens four different conditions (.e., two asymmetric loss/gain-range conditions well). , might make sense create one plot condition. One naive way create sub plots split data different data sets create one plot sub data set. better way provided ggplot2 technique faceting.Faceting easy way get ggplot2 create sub-plots (often called panels) based one categorical variable. show present data, need prepare new tibble similar one . However, now aggregate data one average mean acceptance value lottery (.e., unique combination loss gain) four conditions. also need ev variable tibble.create faceted plot, need use yet another ggplot2 function, facet_wrap(), pass variable plot faceted . specify variables facet call facet_wrap() using vars() function. create faceted plot, need use correct data set, tibble lot_sum_all, simply add facet_wrap(vars(condition)) previous call (simplify plot, remove mapping loss size).plot bit busy details clear. One problem legend takes rather large space right-hand side. can change position legend using call theme() function specifying legend.position argument \"bottom\".looks little better, although legend isn’t completely visible. now, .plot shows quite noticeable differences across conditions, largest differences two asymmetric conditions. discussed , can now clearly see many lotteries positive expected value (-$20/+$40 condition), participants dislike symmetric lotteries expected value 0. contrast, many lotteries negative expected value (-$40/+$20 condition), symmetric lotteries relatively attractive participants tend like nearly much lotteries positive expected value. results compatible original notion loss aversion values lotteries matter, context appear.","code":"\nlot_sum_all <- ws1 %>% \n  group_by(condition, loss, gain) %>% \n  summarise(mean_acceptance = mean(resp)) %>% \n  mutate(ev = case_when(\n    gain == loss ~ \"EV = 0\",\n    gain < loss ~ \"EV negative\",\n    gain > loss ~ \"EV positive\"\n  )) %>% \n  mutate(\n    ev = factor(ev, levels = c(\"EV negative\", \"EV = 0\", \"EV positive\"))\n  )\n#> `summarise()` has grouped output by 'condition', 'loss'. You can override using the `.groups` argument.\nlot_sum_all\n#> # A tibble: 256 x 5\n#> # Groups:   condition, loss [32]\n#>   condition  loss  gain mean_acceptance ev         \n#>   <fct>     <dbl> <dbl>           <dbl> <fct>      \n#> 1 -$20/+$40     6    12           0.613 EV positive\n#> 2 -$20/+$40     6    16           0.702 EV positive\n#> 3 -$20/+$40     6    20           0.874 EV positive\n#> 4 -$20/+$40     6    24           0.890 EV positive\n#> 5 -$20/+$40     6    28           0.916 EV positive\n#> 6 -$20/+$40     6    32           0.942 EV positive\n#> # ... with 250 more rows\nggplot(lot_sum_all, aes(x = gain, y = mean_acceptance, \n                        colour = ev)) +\n  geom_jitter(width = 0.25, alpha = 0.5) +\n  ggthemes::scale_colour_colorblind() +\n  theme_bw() +\n  facet_wrap(vars(condition)) \nggplot(lot_sum_all, aes(x = gain, y = mean_acceptance, \n                        colour = ev)) +\n  geom_jitter(width = 0.25, alpha = 0.5) +\n  ggthemes::scale_colour_colorblind() +\n  theme_bw() +\n  facet_wrap(vars(condition)) + \n  theme(legend.position = \"bottom\")"},{"path":"ggplot2-intro.html","id":"changing-ggplot2-theme-for-r-session","chapter":"4 Data Visualisation with ggplot2","heading":"4.4 Changing ggplot2 Theme for R Session","text":"last plots always changed plot theme default theme theme_bw() passing explicitly plot. Although continue plot, efficient change theme rest R session. call theme_set() function theme_bw() argument. call, also make changes theme feel makes plots generally look better – using larger axis labels, removing unnecessary grid lines, changing default position legend.Note, global option affect ggplot2 plots created executing command within R session. reset default theme, restart R session run different theme_set() command.","code":"\ntheme_set(theme_bw(base_size = 15) + \n            theme(legend.position=\"bottom\", \n                  panel.grid.major.x = element_blank()))"},{"path":"ggplot2-intro.html","id":"ggplot-cat-cont","chapter":"4 Data Visualisation with ggplot2","heading":"4.5 One Continuous and one Categorical Variable","text":"far, looked two continuous variables seen create appealing figures mapping different variables different aesthetics. Now, trying situation potentially common experimental data – situation one continuous dependent variable one categorical independent variable.creating plot, need create appropriate data set. plots , compared lotteries aggregating data Walasek Stewart (2015) level individual lotteries (.e., unique combinations potential losses gains). Whereas made sense purposes , ignored differences across participants.Remember, general goal find signal among noise responses dependent variable. data human participants, participants usually considered major source noise. discussed , different people number reasons may related research question. producing data visualisations, means usually good idea show distribution responses participants. provides visual impression level noise produced one main sources noise data. plot thus provides us visual opportunity compare signal noise.Therefore, instead aggregating across participants , create new data set calculate one score per participant condition. , plot distribution across participants condition. begin creating data set lotteries. Whereas might informative plot respect research question (e.g., compared plotting shared symmetric lotteries), instructive goal learn different ways make plots. later section, produce plot shared lotteries interesting research question loss aversion.already clear output individual mean acceptance rates even first participants one condition quite variable. also shows returned tibble still grouped (See message Groups ...):. perform operations , instead just plotting, might necessary add ungroup() call pipe remove grouping. However, just passing ggplot() function, necessary .","code":"\npart_sum <- ws1 %>%\n  group_by(condition, subno) %>%   # aggregate for both, condition and subno\n  summarise(mean_acc = mean(resp)) \n#> `summarise()` has grouped output by 'condition'. You can override using the `.groups` argument.\npart_sum\n#> # A tibble: 781 x 3\n#> # Groups:   condition [4]\n#>   condition subno   mean_acc\n#>   <fct>     <fct>      <dbl>\n#> 1 -$20/+$40 e1a_109    0.203\n#> 2 -$20/+$40 e1a_113    0.859\n#> 3 -$20/+$40 e1a_125    0.516\n#> 4 -$20/+$40 e1a_129    0.844\n#> 5 -$20/+$40 e1a_13     0.5  \n#> 6 -$20/+$40 e1a_133    0.5  \n#> # ... with 775 more rows"},{"path":"ggplot2-intro.html","id":"displaying-all-data-points","chapter":"4 Data Visualisation with ggplot2","heading":"4.5.1 Displaying All Data Points","text":"plots following convention, plot dependent variable, average acceptance rate per participant, y-axis independent variable, experimental condition, x-axis. , begin plotting individual data values points using point geom (.e., geom_point()).resulting plot bit difficult interpret. can see seems differences conditions (e.g., fewer large mean acceptance rates two rightmost conditions), difficult judge points top – , whether -plotting . , difficult get good impression distribution points per condition.’ve done , let’s add jitter points using geom_jitter() well use alpha blending (.e., alpha = 0.25). Differently , let’s specify particular amount horizontal (.e., width) vertical (.e., height) jitter. case, geom_jitter() automatically adds horizontal vertical jitter.resulting plot lot clearer, visually appealing. amount horizontal jitter large, making difficult see x-axis position (.e., experimental condition) point belongs . improve visual impression, better choose amount jitter hand (done ), passing specific value width argument.difficulty choosing amount horizontal jitter x-axis shows categorical variable, easily identifiable unit. fact, categorical variable, x-axis position shown one whole number, starting 1, difference 1 levels categorical variable. Thus, plot , actual x-axis positions four factor levels 1, 2, 3, 4 factor labels just – labels. Let’s try improve plot specifying amount horizontal jitter, width = 0.2 ( found trial--error) height argument (means height = 0 width specified).plot provides better overview distributions. can, example, clearly see right-condition -$40/+$20 majority data points 0.25.","code":"\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_point()\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_jitter(alpha = 0.25)\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_jitter(width = 0.2, alpha = 0.25)"},{"path":"ggplot2-intro.html","id":"the-bee-swarm-and-related-plots","chapter":"4 Data Visualisation with ggplot2","heading":"4.5.2 The Bee Swarm and Related Plots","text":"Alternatives geom_jitter() provide structure individual data points displayed offered two geoms ggbeeswarm package, geom_beeswarm() geom_quasirandom(). use geoms, first need load ggbeeswarm package (installing first via install.packages() necessary).can replace call geom_jitter() call one two geoms.can see, geoms provide lot structure tohow overlapping data points distributed. geom_beeswarm() produces -called bee swarm plot; , plot -plotted points displaced shown adjacent next (case x-axis). geom_quasirandom() uses similar approach adds random noise points also provide good visual representation shape distribution., choice ggbeeswarm geoms usually depends amount data degree overlap among data points. present case, number data points large prefer visual impression geom_quasirandom(). situations fewer data points, geom_beeswarm() can preferred.plots allow interesting conclusions data. example, -$20/+$40 condition, distribution rather wide data points along whole range mean accuracies. , clear visual peak shape cluster data. noticeable pattern somewhat fewer data points low average acceptance rates (.e., 0.25). contrast, conditions less clearly identifiable centre cluster data points (around 0.5 two symmetric distributions near bottom -$40/+$20 condition).","code":"\nlibrary(\"ggbeeswarm\")\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_beeswarm()\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_quasirandom()"},{"path":"ggplot2-intro.html","id":"box-plot","chapter":"4 Data Visualisation with ggplot2","heading":"4.5.3 Box Plot","text":"Alternatives visualising distribution data points geoms plot data point, summary distribution. popular box plot violin plot, geom_boxplot() geom_violin().Box plots, also known box whiskers plots, visualise distribution several summary statistics, plus potential “outliers.” provides compact summary data can also used case many data points. Let’s show looks like data, explaining visual elements detail.can see several graphical elements plot. explain , following image annotated variant box plot gives names different elements.\nFigure 4.1: Annotated variant box plot.\ncan see box plot consists box, thicker line somewhere inside, well two whiskers. cases, see additional data points outside whiskers, known “outliers” (reason word quotes questionable appropriate term – see discussion ).thick line inside box measure central tendency data values (.e., measure centre distribution). typically use mean summary value central tendency, box plots generally use median (sometimes box plots show mean addition median). median value separates cuts distribution points lower upper half. Technically, means median 50% quantile, data point 50% data points smaller 50% data points larger.37The upper lower bound box, two hinges, show 25% 75% quantiles data. , data points either 25% 75% data points smaller. consequence, box encompasses 50% data points. allows one judge data . example, line earlier visual impression -$20/+$40 distribution widest, box encompassing 50% data also largest. conditions clearly defined centre, box noticeably smaller.two whiskers ends box, hinges, extend hinges largest value 1.5 times size box. size box given 75% quantile minus 25% quantile, also known interquartile range. idea whiskers represent, sense, typical range distribution. Therefore, data points outside typical range can considered untypical. untypical data points often called \"outliers’’, unclear sense terminology appropriate. example, quite data points per condition (190 200) seem unlikely observe cases look somewhat different cases (.e., untypical).issue “outliers,” identified box plot, trivial. “outlier” genuine response participant, simply removing seem appropriate. specifically, omitting data fit idea data can seen instance data fabrication. hand, single response unusually large influence results (e.g., single data point far away others solely responsible observed effect), also problematic. usually want results represent data overall just single observation. Thus, many issues can happen statistical analysis, deal “outliers” depends specific context situation. generally reasonable strategy see qualitative pattern results changes extreme untypical data points included . , shows results robust “outliers” unusually large influence results. case, truthful reporting measures taken address extreme data points essential.","code":"\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_boxplot()"},{"path":"ggplot2-intro.html","id":"violin-plots","chapter":"4 Data Visualisation with ggplot2","heading":"4.5.4 Violin Plots","text":"Another possibility visualising distribution puts less emphasis summary statistics, shape. One popular way violin plot, -called shapes result sometimes look like violin. create violin plot ggplot2, simply need change geom geom_violin().clear plot, violin plot makes easy see whether distribution relatively flat, like -$20/+$40 condition, one multiple peaks, call modes statistical terminology.38 absence clear mode, call distribution uniform. present case, might say relatively uniform, fewer data points lower values mean acceptance (.e., distribution completely uniform).distributions three conditions, clear single mode, called unimodal (meaning one mode). conditions, modes located expect , given previous plots. symmetric conditions modes around 0.5 -$40/+$20 condition mode near bottom, around 0.13. distribution two clear modes peaks, call bimodal, appear justified distributions shown .can also see commonality geom_quasirandom() plot violin plot. shape data points distributed “quasi randomly” geom_quasirandom(), shape violin plot.One way increase amount information shown violin plot adding lines correspond different quantiles distribution. example, add 25%, 50% (.e., median), 75% quantile box plot, follows:","code":"\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_violin()\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75))"},{"path":"ggplot2-intro.html","id":"plotting-the-mean","chapter":"4 Data Visualisation with ggplot2","heading":"4.5.5 Plotting The Mean","text":"section discussed plots one continuous dependent variable one categorical independent variable. Given one basic structures used experiments, probably common type data visualisation scientific literature. already experience literature, might noticed plots created far diverge quite considerably plots seen . particular, many published plots show data points visualisation full distribution data, done . Instead, published plots focus one particular summary statistic data, condition means.example, one common plot type -called dynamite plot means shown terms bar graph. addition, error bar added mean measure uncertainty (provide thorough explanation error bars later chapters). example using present data set looks like .\nFigure 4.2: Example dynamite plot. plot type never really recommended.\ncan seen, bar graph together error bar looks like cartoon dynamite plunger, plot got name. Even though plot can created R, hiding code learn create ! can probably tell, big fan type plot neither many people interested statistics (e.g., Vanderbilt biostatistics department).problem dynamite plots? Clearly issue plot shows means. remember discussion specific results (e.g., Chapter 1), always focussed condition means (e.g., mean acceptance rate symmetric lotteries 21% -$20/+$40 condition, 71% -$40/+$20 condition). Clearly, means important. see coming chapters, statistical reason . even go far say mean generally important summary statistic data.problem dynamite plot shows mean, shows mean.39 mean tells partial story, mean can result many different underlying sets data. fully understand data, need see full distribution, either showing data points visualisation distribution discussed .best illustration means summary statistics hide distribution data shown “datasaurus plot” (Matejka Fitzmaurice 2017). Note plot shows two continuous variables plotted one another (scatter plot) one continuous one categorical variable, nicely illustrates point. can see data can dramatically change shape – cycling 13 qualitatively different patterns, one dinosaur – maintaining means plus summary statistics (two decimal points) x-axis y-axis.\nFigure 4.3: “datasaurus dozen.” Two variables maintain summary statistics dramatically changing shape data points. Justin Matejka George Fitzmaurice: https://www.autodesk.com/research/publications/-stats-different-graphs\nshort, one just focuses means summary statistics, good chance missing important features data. Consequently, good approach plotting one’s data combining visualisation full distribution data mean. approach using throughout book.Another benefit showing visualisation full distribution data instead just means, provides realistic picture noise data. Remember said one goal statistics help us distinguish signal noise. focus means, even including measure uncertainty error bars, can readily forget actual level noise data. may thus inclined draw overly optimistic conclusions data, ones less likely correct conclusions consider actual level noise.Combining visualisation full data distribution mean can achieved adding call stat_summary() function plot. stat_summary() allows us add summary function plot. called without additional arguments, adds mean data plus error bar (default behaviour). Let’s add call quasi-random data cloud plot see happens.However, ’s pretty difficult see anything much different plots, except perhaps larger black circle middle left-distribution. noticeable difference status message telling us default summary function, mean_se(), used. want add mean fine error bars (show standard error, hence se), can generally ignore message. problem plot added black summary points top black data points. One way improve plot plotting data points background semi-transparent manner using alpha = 0.2.plot now makes easy see full distribution data well mean. principle, also see error bars, might small actually visible (meaning error bars smaller diameter point mean). mentioned , ’ll leave technical definition error bars later now accept represent uncertainty means.alternative change geom_quasirandom() plot, stat_summary() plot (e.g., passing colour = \"red\"). left exercise reader. Likewise, might want see happens change order two calls (.e., stat_summary() first geom_quasirandom() second). said , important part learning R trying different things seeing happens. code R script, literally nothing can go wrong trying things. break R goofing around ! can always restart R start beginning script get now. , please play around code provided .plot allows us interesting conclusion regards distributions means: three conditions clearly visible mode peak (-$20/+$40 condition), mode differs noticeably mean. happens distributions asymmetric around mode, long tail towards one end distribution. long tail pushes mean away mode towards long tail, exactly pattern seen . generally, statistical analysis focuses mean (statistical analyses discussed book) provides sense imperfect picture data. Whereas mean, definition, represents average values, might always represent typical value distribution (understand typical value one near mode).40 provides another reason want show distribution data addition mean. allows us judge well mean actually summarise data.Let clear: message previous paragraph focussing mean summary statistic case rather asymmetric distribution necessarily wrong. message something else: focus mean, show actual data, miss nuances real data usually . course ’s interesting see happens mean important summary statistic. absence additional information, mean best prediction new observation distribution, even asymmetric distribution. However, mean typical distribution asymmetric shape, important result addition whatever happens mean. fully understand evidence provided data set, understand data fully, including level noise, peculiarities, nuances. Therefore, always plot full distribution plotting mean.","code":"\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_quasirandom() +\n  stat_summary()\n#> No summary function supplied, defaulting to `mean_se()`\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_quasirandom(alpha = 0.2) +\n  stat_summary()\n#> No summary function supplied, defaulting to `mean_se()`"},{"path":"ggplot2-intro.html","id":"one-continuous-variable-histograms","chapter":"4 Data Visualisation with ggplot2","heading":"4.6 One Continuous Variable: Histograms","text":"far, always created plots involving two variables. However, sometimes want see distribution one variable, one conditional another variable, discussed . important type plot task histogram.example, instead considering distribution mean acceptability ratings participant across conditions, done , also look distribution across conditions. call ggplot2 :plot shows number adjacent bars showing frequency (“count”) individual participants’ mean acceptability value within specific range bin data. width bar indicates range bin, height bar determines count values within range. words, higher bar specific x-axis range, values within range.also helpful clearly distinguish histogram bar graph dynamite plot used displaying means (Figure 4.2). histogram, height bar corresponds frequency value, clear interpretation. means can use shaded area visual cue allows us infer much data given range values. case bar graph showing mean. bar graph, data mean, shaded area provide good visual representation data.can interpret histogram? absence clear research question, usual way describing visual impression distribution. , see one clear mode (peak) around 0.6 well another mode somewhere around 0.15. modes surrounded observations bell shape – amount observations decreases move away peaks. However, around second peak (around 0.15) distribution symmetrical bell shaped. Around main peak, distribution asymmetric observations lower values larger values (.e., almost “shoulder” right side main peak). Finally, distribution rather flat overall much data values around 0.7.","code":"\nggplot(part_sum, aes(mean_acc)) +\n  geom_histogram()\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`."},{"path":"ggplot2-intro.html","id":"choosing-the-number-of-bins","chapter":"4 Data Visualisation with ggplot2","heading":"4.6.1 Choosing the Number of Bins","text":"One important issue consider interpreting histograms whether visual impression affected much width bins. stated status message appears calling geom_histogram() without additional arguments, default setting use 30 bins. width bins function number bins bins equally wide need extend across full range values data set.One way change width bins change number bins using bins argument. example, change number bins 50 get following plot.contrast first histogram, mode around 0.6 now clearly dominating distribution also looks “peaky” (.e., bell shape; bins directly adjacent peak similarly high, like rest distribution). Furthermore, second mode around 0.15 much less clear appears might another mode around 0.4. However, can still clearly see asymmetric distribution data left right main mode.Yet another visual impression achieved using 20 bins. plot clearly shows main mode distribution looks comparatively flat sides . also maintains asymmetric impression.Instead letting number bins determine width bins, also directly specify width using binwidth argument. , also often makes sense either specify center boundary argument. names arguments suggest, allow specifying either centre boundary bins (one arguments can specified). makes sense specify boundary 0 – means first bin lower bound 0 (compare histograms first bin lower bound 0). binwidth use 0.0333… (.e., 0.1 / 3) result three bins per 0.1 interval. produces plot 30 bins, shown .plot complements one clearly shows main mode, asymmetry, well relatively flat distributions sides main mode. can also see trough (valley, opposite mode) 0.475.exploration different bin numbers bin widths shows generally obvious bin width choose. best approach use iterative process, done : Try different values less systematic fashion see visual impression changes. goal figure important robust patterns data . summarised geom_histogram documentation discussing choose bin width (see ?geom_histogram) : “may need look options uncover full story behind data.”","code":"\nggplot(part_sum, aes(mean_acc)) +\n  geom_histogram(bins = 50)\nggplot(part_sum, aes(mean_acc)) +\n  geom_histogram(bins = 20)\nggplot(part_sum, aes(mean_acc)) +\n  geom_histogram(binwidth = 0.1 / 3, boundary = 0)"},{"path":"ggplot2-intro.html","id":"proportion-instead-of-frequency-plots","chapter":"4 Data Visualisation with ggplot2","heading":"4.6.2 Proportion Instead of Frequency Plots","text":"histograms far shown frequency value falls within bin. Whereas makes easy interpret results, can difficult comparing histograms across different groups markedly differ total number observations. problem case frequency observation appears specific bin can mean different things depending many cases data. words, problem frequencies normalised. , consider alternative variants histogram problem.example, suppose wanted plot distribution mean acceptability values separately across conditions using histogram. lead situation markedly different numbers observations, let us pool two conditions symmetric ranges gains losses one. Let’s call new variable condition2. Let’s also transform variable factor.Let’s see resulting histogram looks like produce histogram (using binwidth = 0.1 / 3, boundary = 0), now facet plot three panels using new condition variable, condition2. Note, explicitly use aes(x = mean_acc) make clear mean_acc mapped onto x-axis (now done implicitly).plot shows clear differences conditions, difficult compare symmetric condition two. specifically, symmetric condition bars larger individual bars -$20/+$40 condition. course really possible conditions similar amounts data. Thus, plot misleading informative.One alternative plot density instead frequency. density measure normalised within panel. specifically, density defined total area within panel equal 1.41 get plot, need map density (instead count) y-axis, can done y = after_stat(density). perfectly honest, need understand fully y = after_stat(density) . one less intuitive ggplot2 options one just needs remember (know look ), necessarily understand fully.look plot, can see pattern within panel looks previous plot, now scale non-symmetric conditions comparable symmetric ones. happens panel normalised density 1. words, heights bars panel relative total number observations within panel. stressed , permits comparison across panels, even though panels markedly different numbers observations.One problem density histogram density quantity particularly intuitive interpret. might better density? One alternative display proportion data bin. can also achieved ggplot2 using variant code . , need map density times width plot y-axis. Using approach can achieved via y = after_stat(density * width):plot exactly identical previous one, except scale y-axis changed now shows proportion data bin. reflect better plot, can change y-axis label adding label(y = \"proportion\") plot.Note trick passing y = after_stat(density * width) works convert histogram frequency proportion. far know, mentioned official documentation. already discussed, whenever one wants plot multiple histograms across conditions differ number observations, count (.e., frequency) can hard interpret. Therefore, trick shown something use quite regularly.final paragraph section, let’s quickly sum can see histograms across conditions. symmetric conditions, see one large mode (one large one slightly smaller mode) around 0.5, data left right peak. -$40/+$20 condition, one large mode near left end scale much data 0.5. Finally, -$20/+$40 condition data clear shape. can interpreted multiple modes, also rather uniform distribution values around 0.25.","code":"\npart_sum <- part_sum %>% \n  mutate(\n    condition2 = if_else(condition %in% c(\"-$20/+$20\", \"-$40/+$40\"), \n                         \"symmetric\", as.character(condition))\n  ) %>% \n  mutate(condition2 = factor(\n    condition2, \n    levels = c(\"-$20/+$40\", \"symmetric\", \"-$40/+$20\"))\n  )\nggplot(part_sum, aes(x = mean_acc)) +\n  geom_histogram(binwidth = 0.1 / 3, boundary = 0) +\n  facet_wrap(vars(condition2))\nggplot(part_sum, aes(x = mean_acc, y = after_stat(density))) +\n  geom_histogram(binwidth = 0.1 / 3, boundary = 0) +\n  facet_wrap(vars(condition2))\nggplot(part_sum, aes(x = mean_acc, y = after_stat(density * width))) +\n  geom_histogram(binwidth = 0.1 / 3, boundary = 0) +\n  facet_wrap(vars(condition2))\nggplot(part_sum, aes(x = mean_acc, y = after_stat(density * width))) +\n  geom_histogram(binwidth = 0.1 / 3, boundary = 0) +\n  facet_wrap(vars(condition2)) +\n  labs(y = \"proportion\")"},{"path":"ggplot2-intro.html","id":"summary-3","chapter":"4 Data Visualisation with ggplot2","heading":"4.7 Summary","text":"chapter provided brief introduction data visualisation ggplot2 also squeezed crucial theoretical point – serious data analysis begin exploratory data analysis using variety different plots data. goal producing plots twofold: one hand, graphical data exploration can tell us whether data looks okay confidence checks. One easiest ways spot obvious errors ensure data coherent matches assumptions, produce series plots. example, relatively clear expectations specific plot show, finding something else usually extremely informative. hand, data often contains important information going beyond implications explicit empirical hypothesis interested . Graphical data exploration probably best approach getting full story data wants tell.also discussed exploratory data analysis plot creation generally iterative process involving trial error. often good strategy begin subset data, focus simple pattern, create comparatively simple plot. idea plot relatively easy understand. shows expected pattern, makes sense make complex gradually adding additional graphical elements expanding data shown.terms practical introduction ggplot2, covered data visualisation three common cases: one continuous variable , one continuous one categorical variable (experiment), two continuous variables. pedagogical reasons, order three cases presented chapter way round, beginning two continuous variables.plots one continuous variable, introduced different variants histograms. histogram shows distribution one variable displaying count across different bins (subranges) variable. provides easy interpret visual representation distribution variable. One problem applying histograms data appropriate choice bin width often obvious. means one usually needs try different values. also shown create histograms normalised variant count – using either density probability/proportion – can used comparing histograms across conditions different numbers observations.plots one continuous one categorical variable – typical situation experimental data – considered two different levels data. can plot observations, full distribution, just mean.Firstly, considered plot observations conditional categorical variable. shown plotting individual observations,common problem -plotting, observations appear top . address -plotting, can draw points semi-transparently (using alpha < 1) add random jitter using geom_jitter(). principled approach avoiding -plotting two geoms provided ggbeeswarm package: geom_beeswarm() creates ‘bee swarm’ plotting overlapping points adjacent preferable situations lower numbers observations; geom_quasirandom() plots points shape distribution adding quasi-random white space points preferable situations observations. Instead plotting observations, can also plot graphical summaries distribution, either using box plot, violin plot also shows shape distribution, shape used geom_quasirandom().Secondly, considered plot mean distribution observations across conditions shown can done using stat_summary() without need aggregate data beforehand. also discussed plotting mean, also visualisation full distribution data, can hide important information avoided. Thus, ideally always combine plot mean full distribution (either observations graphical summaries, depending number observations). Ideally, mean made visually salient, example making larger different colour, compared visualisation distribution observations.plots two continuous variables, shown plot observations using either geom_point() geom_jitter(). addition, introduced number ggplot2 basic functionalities. Specifically, shown map variables aesthetics using aes() function use distinguish data points different ways.addition graphical options specific certain combinations variable, introduced general ggplot2 functionalities: themes means change overall look plot faceting create sub-plots panels split plot along categorical variable.sum , ggplot2 powerful tool creating visually appealing data visualisations. basic functionality pass data ggplot() function specify mapping variables aesthetics using aes() function (e.g., aes(x = indepdenent_variable, y = dependent_variable)). add elements plot using +.42 important element add plot geoms, geometric objects produce visualisation observations. mainly looked geom_point(), geom_jitter(), geom_beeswarm(), geom_quasirandom() (last two package ggbeeswarm), geom_boxplot(), geom_violin(), geom_histogram(). can also add summary statistic data plot using stat_summary().addition basic graphical elements, seen can also change plot ways. example, create faceted plot panels add call function facet_wrap(). change look graphical elements can use pre-specified themes, add call theme_bw() change individual elements theme() function. change text axis labels, can add call labs() function.ggplot2 extremely comprehensive tool, remembering details immediately bit much ask. Therefore, tidyverse packages, RStudio provides ggplot2 cheat sheet. highly encourage take look cheat sheet moving next page.","code":""},{"path":"ggplot-exercises.html","id":"ggplot-exercises","chapter":"ggplot2: Example, Quiz, and Exercises","heading":"ggplot2: Example, Quiz, and Exercises","text":", let’s get habit restarting R session starting new analysis. reminder, RStudio menu Session - Restart R. , begin loading tidyverse.43 create plots, also set global theme without grey background tweaks.","code":"\nlibrary(\"tidyverse\")\ntheme_set(theme_bw(base_size = 15) + \n            theme(legend.position=\"bottom\", \n                  panel.grid.major.x = element_blank()))"},{"path":"ggplot-exercises.html","id":"ggplot-walasek-example","chapter":"ggplot2: Example, Quiz, and Exercises","heading":"4.8 Extended ggplot2 Example: Walasek & Stewart (2015) Exp. 1a & 1b","text":"","code":""},{"path":"ggplot-exercises.html","id":"reading-and-combining-both-data-sets","chapter":"ggplot2: Example, Quiz, and Exercises","heading":"4.8.1 Reading and Combining Both Data Sets","text":"Let’s last time get back data Walasek Stewart (2015). , load data sets way previous tidyverse extended example (Section 3.7) combine one data set, ws1. , assumes two data files, Experiment 1a Experiment 1b, data sub-folder working directory. convert categorical variables factors.discussed , two ways plot data. One way aggregate -lottery – , -item – level. aggregate -participant level. , , beginning -lottery level.","code":"\nws1a <- read_csv(\"data/ws2015_exp1a.csv\")\nws1b <- read_csv(\"data/ws2015_exp1b.csv\")\nws1a <- ws1a %>%\n  mutate(subno = factor(paste0(\"e1a_\", subno)))\nws1b <- ws1b %>%\n  mutate(subno = factor(paste0(\"e1b_\", subno)))\nws1 <- bind_rows(ws1a, ws1b)\nws1 <- ws1 %>% \n  mutate(\n    subno = factor(subno),\n    response = factor(response, levels = c(\"reject\", \"accept\")),\n    condition = factor(\n      condition, \n      levels = c(40.2, 20.2, 40.4, 20.4), \n      labels = c(\"-$20/+$40\", \"-$20/+$20\", \"-$40/+$40\", \"-$40/+$20\")\n    )\n  )\nglimpse(ws1)\n#> Rows: 49,984\n#> Columns: 6\n#> $ subno     <fct> e1a_8, e1a_8, e1a_8, e1a_8, e1a_8, e1a_8~\n#> $ loss      <dbl> 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 8~\n#> $ gain      <dbl> 6, 8, 10, 12, 14, 16, 18, 20, 6, 8, 10, ~\n#> $ response  <fct> accept, accept, accept, accept, accept, ~\n#> $ condition <fct> -$20/+$20, -$20/+$20, -$20/+$20, -$20/+$~\n#> $ resp      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1~"},{"path":"ggplot-exercises.html","id":"by-lottery-plot","chapter":"ggplot2: Example, Quiz, and Exercises","heading":"4.8.2 By-Lottery Plot","text":"-lottery plot, use exact strategy . , plot average acceptance rate per lottery condition. also split lotteries three bins – negative expected value, expected value 0, positive expected value. difference earlier analysis slightly different factor level names plot.data hand, can produce plot Section 4.3. already set global theme, need pass themes calls plot. However, still want create appealing legend, adding call guides() function. also specify better y-axis label using labs() function, ensure y-axis stretches full range data, 0 1 using coord_cartesian() function. function allows us specify y-axis (ylim) x-axis (xlim) limits.call contains detailed specifications (e.g., nicer axes labels better organised legends) typically needed polished plot publication presentation conference. way set elements therefore something one can easily forget. One way remember taking look ggplot2 cheat sheet. Another way simply googling want . example, googled “ggplot2 legend two rows” remember needed use guides() exact syntax. , don’t forget google friend comes R programming, especially tidyverse.plot shows pattern across lotteries seen , one highly questions original idea loss aversion. clear relative attractiveness symmetric lotteries differs considerably across conditions, people evaluate magnitudes losses wins similarly across .","code":"\nlot_sum_all <- ws1 %>% \n  group_by(condition, loss, gain) %>% \n  summarise(mean_acceptance = mean(resp)) %>% \n  mutate(ev = case_when(\n    gain == loss ~ \"= 0\",\n    gain < loss ~ \"negative\",\n    gain > loss ~ \"positive\"\n  )) %>% \n  mutate(\n    ev = factor(ev, levels = c(\"negative\", \"= 0\", \"positive\"))\n  )\n#> `summarise()` has grouped output by 'condition', 'loss'. You can override using the `.groups` argument.\nlot_sum_all\n#> # A tibble: 256 x 5\n#> # Groups:   condition, loss [32]\n#>   condition  loss  gain mean_acceptance ev      \n#>   <fct>     <dbl> <dbl>           <dbl> <fct>   \n#> 1 -$20/+$40     6    12           0.613 positive\n#> 2 -$20/+$40     6    16           0.702 positive\n#> 3 -$20/+$40     6    20           0.874 positive\n#> 4 -$20/+$40     6    24           0.890 positive\n#> 5 -$20/+$40     6    28           0.916 positive\n#> 6 -$20/+$40     6    32           0.942 positive\n#> # ... with 250 more rows\nggplot(lot_sum_all, aes(x = gain, y = mean_acceptance, \n                        colour = ev, size = loss)) +\n  geom_jitter(width = 0.25, alpha = 0.5) +\n  ggthemes::scale_colour_colorblind() +\n  facet_wrap(vars(condition)) + \n  guides(size=guide_legend(nrow=2,byrow=TRUE), \n         colour = guide_legend(title = \"EV\", nrow=2,byrow=TRUE)) +\n  labs(y = \"mean acceptance\") +\n  coord_cartesian(ylim = c(0, 1))"},{"path":"ggplot-exercises.html","id":"by-participant-plot","chapter":"ggplot2: Example, Quiz, and Exercises","heading":"4.8.3 By-Participant Plot","text":"One important piece information -lottery plots show, variability accepting symmetric lotteries across participants. visualise , need plot distribution mean acceptability scores across participants condition separately. particular, interested values three symmetric lotteries shared across three conditions, -$12/+$12, -$14/+$14, -$16/+$16 lotteries.plot values across participants, first need calculate -participant mean acceptability scores three symmetric lotteries. , can use filter introduced earlier group condition subno.One problem plotting data three shared symmetric lotteries, four distinct values mean acceptability scores.. individual participant can accept 0, 1, 2 3 lotteries. Calculating mean acceptability dividing 0 3 3 gives four possible values:code , used : operator create sequence integers (.e., whole numbers) 0 3. result obtained explicitly creating vector, example, using c(0, 1, 2, 3).Plotting data like unique values bit tricky. usual approaches – alpha blending, jitter, bee swarm plots, like – produce satisfactory results. following code produce plots show problem.code, also introduce another ggplot2 functionality. plot created ggplot2 R object . far, printed object produces plot plot pane. However, save plot R object using <-, plot shown plot pane. Instead, can use another function, plot_grid() cowplot package, create one plot consists multiple individual plots. Note install cowplot running code (e.g., via install.packages(\"cowplot\")).opinion, six plots somewhat unsatisfactory. Perhaps best one plot using jitter, even , pattern data clear.alternative particular situation show individual data points, count data points share one position. can achieved using geom_count. resulting plot, size points show many points position.Instead showing count directly, can also show proportion responses within condition. , can make plot nicer changing labels. can also adapt grid lines background exactly 4 possible positions mean acceptabilities.plot shows distribution individual mean acceptability scores mostly bimodal. -$20/+$40 condition, vast majority participants either accepts symmetric lotteries rejects . -$20/+$40 condition see one mode, participants reject symmetric lotteries. Taken together, indicates , overall, participants quite consistent behaviour across different symmetric lotteries. reject accept one symmetric lottery likely make choice another symmetric lottery. words, amount noise within participants comparatively low whereas considerable noise across participants.addition, plot clearly shows main pattern results. original notion loss aversion matters acceptance rates symmetric lotteries magnitude potential losses gains. present results clearly contradict notion. -$20/+$40 condition, symmetric lotteries unattractive, participants reject . contrast, -$40/+$20 condition, symmetric lotteries attractive, participants accept . two conditions symmetric lotteries neither particularly attractive unattractive, around half participants accept symmetric lotteries around half reject .Let’s discuss bit meant “clearly shows” start paragraph . plot shows main results pattern well variability data way plot, sense, makes statistical analysis really necessary. Given relatively large number participants per condition (around 200), fact one condition vast majority never accepts symmetric lottery, another condition vast majority always accepts symmetric lottery, means clear difference conditions. reasonable statistical test must come conclusion, otherwise begin question validity statistical test look problems data.means visual representation results can provide empirical evidence addition statistical results. fact, two types evidence – figures derived results statistical analyses derived results – complementary always part communication found experiment. Without figure, properly judge much evidence statistical result actually provides.present case, visual impression suggests clear effect, statisticians say data pass inter-ocular trauma test: results hit right eyes. case, confidence data support empirical hypothesis grows compared situation shows outcome statistical analysis. statistical tests suggest effect figures much, also important outcome suggests epistemic status empirical hypothesis less clear. case, statistical analyses always accompanied figures., leave loss aversion study Walasek Stewart (2015) behind us. example shown rather extensively finding evidence appears support theoretical idea mean theory actually supported. Put bluntly, really look original notion loss aversion tenable. mean rank-based account preferred Walasek Stewart (2015) true? necessarily. data shows participants change behaviour context-dependent manner. consistent rank-based account, sure accounts possible well. larger point making strong inferences great theoretical ideas generally difficult.","code":"\npart_sum <- ws1 %>%\n  filter(loss == gain, loss %in% c(12, 16, 20)) %>% \n  group_by(condition, subno) %>% \n  summarise(mean_acc = mean(resp)) %>% \n  ungroup()\n#> `summarise()` has grouped output by 'condition'. You can override using the `.groups` argument.\npart_sum\n#> # A tibble: 781 x 3\n#>   condition subno   mean_acc\n#>   <fct>     <fct>      <dbl>\n#> 1 -$20/+$40 e1a_109        0\n#> 2 -$20/+$40 e1a_113        0\n#> 3 -$20/+$40 e1a_125        0\n#> 4 -$20/+$40 e1a_129        0\n#> 5 -$20/+$40 e1a_13         0\n#> 6 -$20/+$40 e1a_133        0\n#> # ... with 775 more rows\n0:3 / 3\n#> [1] 0.0000000 0.3333333 0.6666667 1.0000000\n\n## these are exactly the only values that occur in this data set:\nsort(unique(part_sum$mean_acc))\n#> [1] 0.0000000 0.3333333 0.6666667 1.0000000\n## load packages required here:\nlibrary(\"cowplot\")\nlibrary(\"ggbeeswarm\")\n\np1 <- ggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_point(alpha = 0.1) +\n  stat_summary()\np2 <- ggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_jitter(alpha = 0.1, width = 0.3, height = 0.1) +\n  stat_summary()\n## we set groupOnX = FALSE to avoid a warning otherwise\np3 <- ggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_beeswarm(alpha = 0.1, groupOnX = TRUE, cex = 0.1) +\n  stat_summary()\np4 <- ggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_quasirandom(alpha = 0.1, groupOnX = TRUE) +\n  stat_summary()\np5 <- ggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_boxplot() +\n  stat_summary()\np6 <- ggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_violin(draw_quantiles = c(0, 0.25, 0.5, 0.75)) +\n  stat_summary()\nplot_grid(p1, p2, p3, p4, p5, p6, ncol = 2)\n#> No summary function supplied, defaulting to `mean_se()`\n#> No summary function supplied, defaulting to `mean_se()`\n#> No summary function supplied, defaulting to `mean_se()`\n#> No summary function supplied, defaulting to `mean_se()`\n#> No summary function supplied, defaulting to `mean_se()`\n#> No summary function supplied, defaulting to `mean_se()`\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_count(alpha = 0.2) +\n  stat_summary()\n#> No summary function supplied, defaulting to `mean_se()`\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_count(aes(size = after_stat(prop)), alpha = 0.2) +\n  stat_summary() +\n  scale_y_continuous(breaks = c(0, 0.33, 0.67, 1)) +\n  guides(size=guide_legend(title = \"proportion\")) +\n  labs(y = \"mean acceptance\")\n#> No summary function supplied, defaulting to `mean_se()`"},{"path":"ggplot-exercises.html","id":"quiz-1","chapter":"ggplot2: Example, Quiz, and Exercises","heading":"4.9 Quiz","text":"Note: pull-menu selecting answer turns green selecting correct answer.Exercise 4.1  data format ggplot2 accept?individual datapointsdata.frame / tibblenumerical vectorsAnswer: 123Exercise 4.2  role aes() function?tells R data.frame want plotIt allows adding individual data points lines plotIt used map variables data onto aestheticsAnswer: 123Exercise 4.3  variable typically plotted y-axis?independent variableThe dependent variableThe control variableAnswer: 123Exercise 4.4  “-plotting” refer ?least two data points exactly position visually differentiatedIt means created many plots need restart R menu (Session - restart R)occurs incorrect aesthetics used ggplot2 create plotAnswer: 123Exercise 4.5  “alpha blending?”synonym “aesthetics” ggplot2A technique creates visual impression semi-transparencyAn argument ggplot() indicating want plot points dataAnswer: 123Exercise 4.6  geom_jitter() ?introduces random jitter plotted pointsIt allows change colour data points (.e., colour jittered)allows combination several plots one bigger plot jittered mannerAnswer: 123Exercise 4.7  “theme” ggplot2?refers colour palette used plotIt title plotThe theme determines overall look plot: e.g., font, font size, backgroundAnswer: 123Exercise 4.8  “faceted plot?”plot two variables displayedA plot data originally -plotted, moreA plot consisting several sub-plots/panelsAnswer: 123Exercise 4.9  following show individual data points?geom_boxplot()geom_beeswarm()geom_quasirandom()Answer: 123Exercise 4.10  25% quantile mean?’s data point 25% data points largerIt’s data point 25% data points smallerIt’s data point 25% data points can larger smallerAnswer: 123Exercise 4.11  dynamite plots useful?show meanThey show distribution data data pointsThey contain legends plot labelsAnswer: 123Exercise 4.12  name function adds mean data plot?mean()geom_point()stat_summary()Answer: 123Exercise 4.13  plots involve two variables?histogrambox plotbar graphAnswer: 123Exercise 4.14  uniform distribution look like?one clear mode symmetricIt least two modes heightIt clear modeAnswer: 123Exercise 4.15  Consider data experiment dependent variable score independent variable condition. aes() call plot data conventional way?aes(condition = x, score = y)aes(condition = y, score = x)aes(x = condition, y = score)aes(y = condition, x = score)Answer: 1234Exercise 4.16  appropriate way handle outliers identified box-plot?Outliers removed analysis can influence results large degree.real data point collected needs included analysis. Removing outlier can seen instance data fabrication.repeat analysis without outlier, see results robust presence absence outlier.Answer: 123","code":""},{"path":"ggplot-exercises.html","id":"ggplot2-exercise-earworms-and-sleep","chapter":"ggplot2: Example, Quiz, and Exercises","heading":"4.10 ggplot2 Exercise: Earworms and Sleep","text":"exercise, let’s return study Scullin, Gao, Fillmore (2021) investigating relationship earworms sleep quality. Let’s quickly recap study design research question . details see Section 3.10.study sleep lab experiment participants heard music going sleep. lyrics condition, songs original versions pop songs lyrics. instrumental condition, instrumental versions pop songs.researchers interested two research questions:listening original version pop song (lyrics) instrumental version affect whether participants develop earworms? research question based previous results authors expected instrumental version induce earworms lyrical versions.investigate question, researchers asked participants whether experienced earworms various times experiment (.e., falling asleep, night, morning, getting ready leave sleep lab)sleep quality differ participants lyrical music condition instrumental music condition (participants expected earworms)?investigate question, researchers used polysomnography measured various objective sleep parameters. , specifically looking sleep efficiency sleep time.version original data file made available Scullin, Gao, Fillmore (2021) can downloaded . , recommend download data folder working directory can read data following code. makes sense restart R , also reload tidyverse. Let’s prepare data set discussed previous exercise take look .addition variables relevant research questions, data contains number control variables (original data file included even ). can see total 19 variables:id: Participant identifiergroup: experimental condition two values: “Lyrics” versus “Instrumental”relaxed: question asking participants relaxed felt scale 0 (= relaxed) 100 (= relaxed) listening music.sleepy: question asking participants sleepy felt scale 0 (= sleepy) 100 (= sleepy) listening music.\nnext 5 variables concerned whether participants reported earworm different times:earworm_falling_asleep: earworm trying fall asleep last night? 0 = ; 1 = Yesearworm_night: earworm waking night? 0 = ; 1 = Yesearworm_morning: earworm waking morning? 0 = ; 1 = Yesearworm_control: earworm control time point (getting ready leave lab)? 0 = ; 1 = Yessleep_efficiency: percentage score obtained polysomnography (0 = low sleep efficiency 100 = high sleep efficiency)sleep_time: total sleep time minutesThe remaining variables demographic ones whose meaning evident, except, perhaps, classrank. uses US nomenclature year four student participant (Freshman = year 1, Sophomore = year 2, Junior = year 3, Senior = year 4) plus additional values staff, post-graduate students (presumably) one missing value (‘-’).Exercise 4.17  Let’s begin taking look distribution sleep quality sleep time, two dependent variables second research question, using histograms. Ideally, combine two histograms one plot shown previously using plot_grid(). Remember one important part making histograms picking appropriate bin width. distributions look like?Histograms produced geom_histogram(). pick correct bin width, just try .Let’s start basic histogram without changing setting geom_histogram(). , pass earworm data ggplot(), set x aesthetic variable want plot just combine plots one.plots bit difficult interpret rather noisy, giving clear visual impression. One possibility many bins. Remember total 48 data points, 30 bins means observations per bin.Therefore, let’s try get informative visual impression reducing number bins. Given scale data see , let’s start bin width 2 sleep efficiency 10 sleep time. Let’s also set boundary 0 bins always begin whole numbers.resulting plots much easier interpret. distribution sleep efficiency scores unimodal asymmetric long tail left side (-called left-skewed). distribution sleep times looks bi-modal symmetric around midpoint two modes.make sure impression fluke, let’s make two variants plot. one, halve bin width, , increase 50%.sleep efficiency, pattern relatively stable. halve bin width, mode less well defined. However, 1.5 times bin width, pattern similar.sleep time, pattern essentially unchanged.Let’s make final plot using initial bin width, bit nicer. particular, harmonise y-axis limits use better x-axis labels.Exercise 4.18  data contains two control variables, relaxed sleepy, subjective judgements collected participants listened music. variables answers relatively similar questions, might suspect participants provided similar responses. Make plot control variables see whether related.control variables continuous variables. Hence, make plot two continuous variables. See Section 4.2 details.can create plot directly passing earworm data ggplot() mapping one variables x-axis y-axis. need use geom_point() see data.plot suggests seems relationship, although certainly strong. response relaxed large, response sleepy also appears large.can improve plot make pattern somewhat clearer. Note data exactly scale variables, 0 100. Thus, makes sense plot data way reflects . particular, can make sure axes show full range scale, visually-judged distances mean axes. can use coord_fixed() ratio =1. . ratio refers aspect ratio plot, meaning width divided height. value 1 therefore results plot square:plot makes clearer seems relationship, also two data points violating general trend. one case, sleepiness large relaxed small. , relaxed large, sleepiness small.Exercise 4.19  might also interested whether two control variables, relaxed sleepy, show relationship main dependent variable, sleep efficiency. seen, two variables also appear least somewhat related. Therefore, might also interested relationship combined measure two control variables (average relaxed sleepy) sleep efficiency.check , make three plots, one control variable plus one average two control variables. plots, one now three control variables shown together sleep efficiency. obvious relationships control variables sleep efficiency?control variables sleep efficiency continuous variables. plot two continuous variables plot. , see Section 4.2.also makes sense first create combined control variable, say control_combined plotting . , can use mutate().Let’s begin creating combined control variable, control_combined using mutate., can create one plot comparing three control variables sleep efficiency.difficult make clear judgement . relaxed sleepy, appear data points top right plot elsewhere. However, plots also one data point large sleep efficiency value time smallest value control variable. Note two unusual points different participants.sleepy however, also one data point small sleep efficiency low sleepiness. relaxed point missing.combined score, also see clear relationship. Whereas looks like data points main diagonal (.e., data points lower right corner) much say.Exercise 4.20  Let’s now take look research question (1): listening original version pop song (lyrics) instrumental version affect whether participants develop earworms?, let’s look two different dependent variables created tidyverse exercise: earworm (0 participant report earworm 1 otherwise) proportion earworms (sum three earworm dependent variables divided 3). calculate whether group affects two new dependent variables.Note follow Scullin, Gao, Fillmore (2021) calculate variables three earworm variables (.e., earworm_... variables exception earworm_control).Make one plot two dependent variables distribution variable across participants well mean shown, separately two experimental conditions.plot support prediction instrumental music leads earworms lyrical music?Now make plots involving one categorical variable, experimental condition, one continuous variable, dependent variable. yet ring bell, see Section 4.5.begin creating new variables. can use solution corresponding tidyverse exercise:, can start making plots beginning prop_earworm. Note prop_earworm structure mean acceptance rate symmetric lotteries discussed example . means, possible values, four.Hence, can use essentially code plot count (proportion) individual data points background. need make sure change variables correctly.plot provides evidence hypothesis instrumental songs induce earworms. ‘lyrics’ condition, vast majority participants report earworms report . contrast, ‘instrumental’ condition, participants likely report earworms. even report earworms measured time points. Consequently, also see mean difference proportion earworms .Let’s now take look second dependent variable, any_earworm. variable can take two values, 0 1, per participant. Hence, can use similar piece code.provides similar picture ‘earworm’ variable. ‘lyrics’ condition, vast majority reports earworms, whereas ‘instrumental condition’ split roughly 50-50. course, similarity surprising. variables use exact information, slightly different ways.sum, data supports prediction listening instrumental music leads earworms listening lyrical music.Exercise 4.21  Now learned something relationship different versions pop songs earworms, let’s link main point interest study, sleep quality. specifically, research question (2) : sleep quality differ participants lyrical music condition instrumental music condition, participants expected earworms?Make one plot two dependent variables relevant research question, one sleep efficiency one sleep time. , plot make sure show distribution variable across participants well mean, separately two experimental conditions.plot support hypothesis type music heard sleeping affects sleep quality?make similar plot previous exercise. However, time probably want show individual data points somewhat differently.exercise, need manipulate data participant one observation dependent variables. important question display data background. Let’s begin using geom_beeswarm(),make two plots, one dependent variable, combine one figure.figure shows two dependent variables, sleep quality average lower instrumental music group compared lyrical music group. looking individual data points background, looks like many value dependent variable, distribution points clear. Therefore, let’s also try using geom_quasirandom():plot shows clearly sleep efficiency, difference means also reflected distribution individual points. instrumental condition, distribution points seems bigger range, well shifted downwards compared lyrical condition. suggests might actual effect type music listened sleep sleep efficiency.pattern results clear sleep time. Whereas also difference means, distributions look fairly similar. Furthermore, two groups one data point far away rest distribution (potential “outlier”) opposite direction mean difference (.e., particularly low value lyrics group mean larger). Hence, whether observed difference reflects genuine difference lot less clear.Speaking generally, geom_quasirandom seems provide interesting easy interpret data visualisation data set. shows seems effect experimental condition sleep efficiency, effect less clear sleep time.Exercise 4.22  last exercise, make plot shows distribution means sleep efficiency sleep time conditional whether participants earworms night (.e., time, excluding control time point). Thus, analysis ignore experimental condition.perform analysis, need begin step corresponding analysis tidyverse exercises create one set plots:Create new factor, has_earworm, yes, whenever participants report least earworm one three earworm dependent variables (.e., within experimental time frame, ignoring control time).Make plot sleep_efficiency sleep_time function has_earworm.ensure forget look , also calculate many participants versus earworms.can see? participants report earworm sleep better participants , ignoring experimental condition?Take look last tidyverse exercise get going.create new factor, has_earworm, can transform any_earworm factor:can make plot last analysis, new independent variable.plot suggests sleep efficiency lower one reports least one earworm night compared . Interestingly, participants report earworms show unimodal distribution mode larger one sleep efficiency score participants report earworms. However, distributions relatively long tail towards lower scores, difference means huge.sleep time, difference opposite direction noticeably smaller.plot also shows fewer participants report earworm . Let’s take look large groups :shows around 40% participants report earworm, indicating imbalance large.","code":"\nlibrary(\"tidyverse\")\nlibrary(\"cowplot\")  ## for plot_grid()\nlibrary(\"ggbeeswarm\") ## for bee swarm plot\ntheme_set(theme_bw(base_size = 15) + \n            theme(legend.position=\"bottom\", \n                  panel.grid.major.x = element_blank()))\nearworm <- read_csv(\"data/earworm_study.csv\")\n#> Rows: 48 Columns: 19\n#> -- Column specification ------------------------------------\n#> Delimiter: \",\"\n#> chr  (7): group, gender, raceethnicity, nativeenglishspe...\n#> dbl (12): id, relaxed, sleepy, earworm_falling_asleep, e...\n#> \n#> i Use `spec()` to retrieve the full column specification for this data.\n#> i Specify the column types or set `show_col_types = FALSE` to quiet this message.\nearworm <- earworm %>% \n  mutate(\n    id = factor(id),\n    group = factor(group, levels = c(\"Lyrics\", \"Instrumental\"))\n  )\nglimpse(earworm)\n#> Rows: 48\n#> Columns: 19\n#> $ id                     <fct> 102, 103, 104, 105, 106, 10~\n#> $ group                  <fct> Lyrics, Lyrics, Instrumenta~\n#> $ relaxed                <dbl> 37, 96, 62, 83, 68, 78, 60,~\n#> $ sleepy                 <dbl> 49, 10, 44, 97, 42, 52, 41,~\n#> $ earworm_falling_asleep <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n#> $ earworm_night          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, ~\n#> $ earworm_morning        <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, ~\n#> $ earworm_control        <dbl> 1, 1, 0, 0, 0, 0, 1, 0, 0, ~\n#> $ sleep_efficiency       <dbl> 90.0, 95.7, 89.8, 94.1, 98.~\n#> $ sleep_time             <dbl> 487.6, 519.4, 472.1, 494.8,~\n#> $ age                    <dbl> 19, 21, 19, 20, 27, 18, 22,~\n#> $ gender                 <chr> \"Female\", \"Female\", \"Female~\n#> $ raceethnicity          <chr> \"Caucasian\", \"Caucasian\", \"~\n#> $ nativeenglishspeaker   <chr> \"Yes\", \"Yes\", \"Yes\", \"Yes\",~\n#> $ bilingual              <chr> \"No\", \"No\", \"No\", \"No\", \"No~\n#> $ height_inches          <dbl> 66, 68, 64, 69, 53, 62, 76,~\n#> $ weight_lbs             <dbl> 145, 183, 135, 155, 89, 120~\n#> $ handedness             <chr> \"R\", \"L\", \"R\", \"R\", \"R\", \"R~\n#> $ classrank              <chr> \"Sophomore\", \"Junior\", \"Fre~\np1 <- ggplot(earworm, aes(x = sleep_efficiency)) +\n  geom_histogram()\np2 <- ggplot(earworm, aes(x = sleep_time)) +\n  geom_histogram()\nplot_grid(p1, p2)\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\n#> `stat_bin()` using `bins = 30`. Pick better value with\n#> `binwidth`.\np1 <- ggplot(earworm, aes(x = sleep_efficiency)) +\n  geom_histogram(binwidth = 2, boundary = 0)\np2 <- ggplot(earworm, aes(x = sleep_time)) +\n  geom_histogram(binwidth = 10, boundary = 0)\nplot_grid(p1, p2)\n## half bin width\np1 <- ggplot(earworm, aes(x = sleep_efficiency)) +\n  geom_histogram(binwidth = 1, boundary = 0)\np2 <- ggplot(earworm, aes(x = sleep_time)) +\n  geom_histogram(binwidth = 5, boundary = 0)\nplot_grid(p1, p2)\n## 1.5 times bin width\np1 <- ggplot(earworm, aes(x = sleep_efficiency)) +\n  geom_histogram(binwidth = 3, boundary = 100)\np2 <- ggplot(earworm, aes(x = sleep_time)) +\n  geom_histogram(binwidth = 15, boundary = 0)\nplot_grid(p1, p2)\np1 <- ggplot(earworm, aes(x = sleep_efficiency)) +\n  geom_histogram(binwidth = 2, boundary = 0) +\n  coord_cartesian(ylim = c(0, 11)) + \n  labs(x = \"sleep efficiency\")\np2 <- ggplot(earworm, aes(x = sleep_time)) +\n  geom_histogram(binwidth = 10, boundary = 0) +\n  coord_cartesian(ylim = c(0, 11)) + \n  labs(x = \"sleep time (minutes)\")\nplot_grid(p1, p2)\nggplot(earworm, aes(x = relaxed, y = sleepy)) +\n  geom_point() \nggplot(earworm, aes(x = relaxed, y = sleepy)) +\n  geom_point()  +\n  coord_fixed(ratio = 1, xlim = c(0, 100), ylim = c(0, 100))\nearworm <- earworm %>% \n  mutate(control_combined = (relaxed + sleepy) / 2 )\n\npe3_1 <- ggplot(earworm, aes(x = relaxed, y = sleep_efficiency)) +\n  geom_point() \npe3_2 <- ggplot(earworm, aes(x = sleepy, y = sleep_efficiency)) +\n  geom_point() \npe3_3 <- ggplot(earworm, aes(x = control_combined, y = sleep_efficiency)) +\n  geom_point() \nplot_grid(pe3_1, pe3_2, pe3_3)\nearworm <- earworm %>% \n  mutate(\n    prop_earworm = (earworm_falling_asleep + earworm_night + \n                      earworm_morning) / 3\n  ) %>% \n  mutate(any_earworm = if_else(prop_earworm == 0, 0, 1)) \nsort(unique(earworm$prop_earworm))\n#> [1] 0.0000000 0.3333333 0.6666667 1.0000000\nggplot(earworm, aes(x = group, y = prop_earworm)) +\n  geom_count(aes(size = after_stat(prop)), alpha = 0.2) +\n  stat_summary() +\n  scale_y_continuous(breaks = c(0, 0.33, 0.67, 1)) +\n  guides(size=guide_legend(title = \"prop. participants\")) +\n  labs(y = \"prop. earworms\")\n#> No summary function supplied, defaulting to `mean_se()`\nggplot(earworm, aes(x = group, y = any_earworm)) +\n  geom_count(aes(size = after_stat(prop)), alpha = 0.2) +\n  stat_summary() +\n  scale_y_continuous(breaks = c(0, 0.33, 0.67, 1)) +\n  guides(size=guide_legend(title = \"prop. participants\")) +\n  labs(y = \"any earworm\")\n#> No summary function supplied, defaulting to `mean_se()`\npr2_1a <- ggplot(earworm, aes(x = group, y = sleep_efficiency)) +\n  geom_beeswarm(alpha = 0.2) +\n  stat_summary() \npr2_2a <- ggplot(earworm, aes(x = group, y = sleep_time)) +\n  geom_beeswarm(alpha = 0.2) +\n  stat_summary() \nplot_grid(pr2_1a, pr2_2a)\n#> No summary function supplied, defaulting to `mean_se()`\n#> No summary function supplied, defaulting to `mean_se()`\npr2_1b <- ggplot(earworm, aes(x = group, y = sleep_efficiency)) +\n  geom_quasirandom(alpha = 0.2) +\n  stat_summary() \npr2_2b <- ggplot(earworm, aes(x = group, y = sleep_time)) +\n  geom_quasirandom(alpha = 0.2) +\n  stat_summary() \nplot_grid(pr2_1b, pr2_2b)\n#> No summary function supplied, defaulting to `mean_se()`\n#> No summary function supplied, defaulting to `mean_se()`\nearworm <- earworm %>% \n  mutate(has_earworm = factor(\n    any_earworm,\n    levels = c(1, 0),\n    labels = c(\"yes\", \"no\")\n  ))\npr3_1b <- ggplot(earworm, aes(x = has_earworm, y = sleep_efficiency)) +\n  geom_quasirandom(alpha = 0.2) +\n  stat_summary() \npr3_2b <- ggplot(earworm, aes(x = has_earworm, y = sleep_time)) +\n  geom_quasirandom(alpha = 0.2) +\n  stat_summary() \nplot_grid(pr3_1b, pr3_2b)\n#> No summary function supplied, defaulting to `mean_se()`\n#> No summary function supplied, defaulting to `mean_se()`\nearworm %>% \n  group_by(has_earworm) %>% \n  summarise(n = n()) %>% ## calculate N\n  mutate(prop = n/sum(n)) ## transform N into %\n#> # A tibble: 2 x 3\n#>   has_earworm     n  prop\n#>   <fct>       <int> <dbl>\n#> 1 yes            19 0.396\n#> 2 no             29 0.604"},{"path":"standard1.html","id":"standard1","chapter":"5 The Standard Approach for One Independent Variable","heading":"5 The Standard Approach for One Independent Variable","text":"chapter introducing standard statistical approach analysing experimental data one independent variable (.e., one factor). simple case study comparing two experimental conditions one dependent variable. exemplify standard approach design using recent straightforward experiment.","code":""},{"path":"standard1.html","id":"ex:urry","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.1 Example Data: Note Taking Experiment","text":"Heather Urry 87 undergraduate graduate students (Urry et al. 2021) (yes, 87 students co-authors!) compared effectiveness taking notes laptop versus longhand (.e., pen paper) learning lectures. 142 participants (differed 88 authors) first viewed one several 15 minutes lectures (TED talks) asked take notes either laptop pen paper. proper experiment, participants randomly assigned either laptop (\\(N = 68\\)) longhand condition (\\(N = 74\\)). 30 minutes delay, participants quizzed content lecture. answers participant independently rated several raters (agreed strongly ; .e., showed high inter-rater reliability) using standardised scoring key. procedure produced one memory score per participant representing percentage information remembered ranging 0 (= memory) 100 (= perfect memory).44 Figure 5.1 shows memory scores across note taking conditions.\nNote: code included advanced readers general interest. Understanding code necessary understanding content chapter.\n\nFigure 5.1: Distribution memory scores Urry et al. (2021) across two note taking conditions. Black points show individual scores, red point shows mean. three lines show, top bottom, 75%, 50% (.e., median), 25% quantile.\nFigure 5.1, black point shows memory score one participant full distribution data visible. shape distribution also shown via violin plot (.e., black outline around points) added three lines representing three summary statistics data. top bottom lines 75% quantile, 50% quantile (.e., median), 25% quantile. red points show mean associated error bars show standard error mean45. see two means quite similar, although mean laptop condition slightly larger, 2.0 points (mean laptop = 68.2, mean longhand = 66.2).","code":"\nlibrary(\"tidyverse\")\nlibrary(\"afex\")\nlibrary(\"ggbeeswarm\")  ## only needed later, but already loaded here\ntheme_set(theme_bw(base_size = 15) + \n            theme(legend.position=\"bottom\", \n                  panel.grid.major.x = element_blank()))\ndata(\"laptop_urry\", package = \"afex\")\nset.seed(1234566) ## needed to ensure jitter is the same each time plot is created\np1 <- ggplot(laptop_urry, aes(x = condition, y = overall)) +\n  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +\n  ggbeeswarm::geom_quasirandom() +\n  stat_summary(colour = \"red\") +\n  coord_cartesian(ylim = c(0, 100)) +\n  labs(y = \"memory score\")\n# to show figure execute (without comment):\n# p1"},{"path":"standard1.html","id":"generalising-from-sample-to-population","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.2 Generalising From Sample to Population","text":"previous paragraph provide us descriptive statistics describing results experiment Urry et al. (2021): memory difference 2.0 points scale 0 100 laptop longhand condition sample 142 participants. However, researchers usually primarily interested happens sample. like know results generalise population sample drawn.Let’s clarify mean “population” “sample,” two key concepts statistics, even though discussion leads us bit tangent. One way thinking terms population encompasses individual principle take part study whereas sample particular individuals data. sense, population potential participants whereas sample actual participants study. Consequently, sample always subset population; say sample drawn population.case Urry et al. (2021), probably understand population undergraduate students Tufts University, research intensive (-called R1) US university. technically appropriate definition participants undergraduates Tufts university (.e., participants drawn pool Tufts University undergraduates). problem understanding population possible generalisation results somewhat limited. researchers really interested whether memory difference note taking laptop versus longhand holds Tufts undergraduates, whether holds students general.population actually interested students, including students different institutions (e.g., students Universities non-Western countries) future students might take notes either longhand format laptop (even students yet born). Ideally, hope results generalise population students. undergraduates Tufts also subset larger population, might wonder whether can also generalise population. Whether generalisation permitted statistical question, question external validity (see Section 2.5.3). depends representative Tufts undergraduates respect research question overall population students. , example, something special Tufts students take notes remember information (students US east coast, students R1 US Universities, …), generalisation hold sub-population students students. stay within terminology introduced book, question whether plausible alternative explanations preventing us generalise one population .sum , statistical methods introduced permit generalisation sample actual population sample drawn. sample undergraduate students specific institution (commonly case), strictly speaking statistical methods permit generalise population students institution.46 Whether can generalise larger population usually interested (e.g., students humans), question statistical methods can provide answer . can seen yet another epistemic gap needs bridged using non-statistical arguments researcher. example, one way make convincing case finding generalises larger population collect data larger population show generalise population. example, instead collecting data Western undergraduate students, attempt collect data students parts world.Interestingly, recent large-scale analysis replicating (.e., repeating) 28 classical psychological phenomena across 36 countries territories found overall differences countries cultures surprisingly small specific phenomena investigated, exceptions (Klein et al. 2018). One caveat study even though participants came many different parts world, still mostly undergraduate students. Nevertheless, study provides assurances many phenomena studied psychology least generalise undergraduate populations specific institutions undergraduate populations world wide. course, whether also holds research question investigated Urry et al. (2021) conclusively answered one phenomena investigated Klein et al. (2018).","code":""},{"path":"standard1.html","id":"nhst-logic","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.3 The Logic of Inferential Statistics","text":"Going beyond present sample goal inferential statistics. different inferential statistical approaches, focussing popular one, null hypothesis significance testing (NHST). describe NHST thoroughly later chapter, already introduce main ideas . NHST, question generalising sample population two different possible true states world: either difference conditions means population – case mean difference observed sample due chance – difference condition means population.study Urry et al. (2021), two possible true states refer difference memory scores students take note laptop longhand format population students. one possibility difference memory scores note taking conditions. case, observed difference 2.0 points just chance occurrence (.e., noise). possibility difference memory scores depending students take note. case, observed difference 2.0 due chance, also affected true difference population (.e., signal plus noise).going forward describing test two possible states world, important understand importance distinguishing difference sample difference population. reality, practically every study shows difference two conditions. extremely unlikely observed difference exactly 0. example, case Urry et al. (2021), even though distributions memory scores look similar (5.1), still observed difference 2.0 points. seems unlikley two groups different participants mean scores exactly conditions, even type note taking true effect. words, just difference sample mean generalises population. need inferential statistic help us decision.decide two possibile states world, set statistical model data. statistical model allows us assess difference population – call possible state world null hypothesis. specifically, statistical model allows us test compatible observed data null hypothesis difference. statistical test null hypothesis proceeds follows:assume state world difference population means true.Based assumption calculate likely observe difference least large one observed sample. done comparing observed level noise observed difference conditions. observed difference small given observed level noise, likely difference comes state world difference population means. observed difference large given observed level noise, unlikely difference comes state world difference.probability observing difference least large one observed small, take evidence null hypothesis difference true – reject null hypothesis.reject null hypothesis, act difference population. case, common say experimental manipulation effect.can see, logic underlying inferential statistics using NHST trivial. make clearer, let us apply logic example data. want know whether observed mean memory difference note taking laptop note taking long hand format sample generalises population students take note either formats. , set statistical model data. model allows us test compatible data null hypothesis mean memory difference population. done comparing overall noise data (.e., variability visible Figure 5.1) observed difference conditions.Specifically, model allows us calculate probability obtaining memory difference large one observed assuming mean memory difference population. data incompatible null hypothesis – , unlikely obtain memory difference least large one observed null hypothesis true – reject null hypothesis mean memory difference population. reject null hypothesis, act mean memory difference population. words, act type note taking effect memory lectures population.Even though logic NHST necessarily intuitive, clearly helpful researchers. running experiment really like know difference observed experiment (.e., sample) meaningful sense generalises population drawn sample. said , almost every actual experiment mean difference condition (.e., extreme unlikely conditions exactly mean). Thus, pretty much always face question. NHST allows us test whether observed difference compatible world difference. unlikely, decide – , act – difference.can see spelling logic detail inferential steps make get answer whether can generalise sample population. design experiments goal mind find difference different experimental conditions. However, test directly. Instead, test compatibility data converse actually interested – null hypothesis effect. test “fails” – , test suggests data incompatible null hypothesis – make two inferential steps. First reject null hypothesis act difference.inferential steps necessitated logically. specifically, number get NHST probability indicating unlikely observed data null hypothesis difference. probability low necessitate null hypothesis false. Low probability events can happen. Furthermore, statistical model representing null hypothesis can also false ways necessitate difference conditions. means already discussed discussing second inferential gap (Section 1.3.2), inferences based NHST never provide 100% certainty.NHST de facto standard procedure inferential statistics across empirical sciences (.e., psychology related disciplines). Understanding logic NHST enable understand majority empirical papers also allow apply inferential statistics research. Nevertheless, exist long list popular criticisms NHST (e.g., Rozeboom 1960; Meehl 1978; Cohen 1994; Nickerson 2000; Wagenmakers 2007). Despite criticisms, NHST (likely remain) popular statistical framework. reason alternative approaches (Bayesian approach; Wagenmakers et al. (2017)) also issues complicating use practice.discuss criticisms NHST detail later chapters, now important realise NHST allow test, prove, whether mean difference population. thing NHST calculates probability compatible data null hypothesis. probability low necessarily mean difference. Likewise, probability high necessarily mean difference. inferences draw based NHST results probabilistic (.e., can false). important rule interpreting results NHST humble. NHST never “proves” “confirms” anything. Instead, NHST results “suggest” “indicate” certain interpretations. -interpret results, instead stay humble interpretations, unlikely fall prey common (often justifiable) criticisms NHST framework.","code":""},{"path":"standard1.html","id":"basic-stats-model","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.4 The Basic Statistical Model","text":"apply inferential statistics NHST framework data, begin setting statistical model data. statistical model attempts explain predict observed values dependent variable (DV) independent variable (IV).47 experimental context means predicting observed outcome, DV, experimental manipulation, IV.basic statistical model partitions observed DV three parts: overall mean, reasons become clear later called intercept, effect IV, part data explained model, residuals. summing three parts together, result observed value. mathematical form can express (used reading mathematical expressions, point end equation simply full stop ends sentence mathematical meaning.) someone without mathematics background , know equations text often intimidating immediately useful. Consequently, moving makes sense go equation detail. Furthermore, statistical analyses discussed book applications Equation (5.1). equation forms foundation statistical analysis experimental data thus understanding unlock analyses discussed book. Consequently, makes sense spend time .Le’s consider variables Equation (5.1) detail. , also consider many different possible values variable can take .48 following Figure, variant Figure 5.1, shows elements graphically explain text just .\nNote: code included advanced readers general interest. Understanding code necessary understanding content chapter.\n\nFigure 5.2: Data Urry et al. (2021) showing overall mean (intercept, blue dotted line), condition specific effects (difference dashed red lines condition means blue line), residuals (grey lines condition means data points).\n\\(\\text{DV}\\): dependent variable, DV, observed values, one observation/participant. example data 142 black data points shown Figure 5.2. Thus, statistical model tries explain individually observed values.\\(\\text{DV}\\): dependent variable, DV, observed values, one observation/participant. example data 142 black data points shown Figure 5.2. Thus, statistical model tries explain individually observed values.\\(\\text{intercept}\\): intercept represents overall mean. Consequently, one intercept (.e., intercept observation). experimental designs define mean condition means. example data intercept average laptop mean (68.2) longhand mean (66.2), (68.2 + 66.2) / 2 = 67.2, shown blue dotted line Figure 5.2.\\(\\text{intercept}\\): intercept represents overall mean. Consequently, one intercept (.e., intercept observation). experimental designs define mean condition means. example data intercept average laptop mean (68.2) longhand mean (66.2), (68.2 + 66.2) / 2 = 67.2, shown blue dotted line Figure 5.2.\\(\\text{IV-effect}\\): IV-effect represents signal, effect independent variable define difference condition means intercept (.e., deviation condition means intercept). Thus, always many different IV-effects conditions. example data two conditions, two different IV-effects, magnitude differ sign, 1.0 laptop condition (.e., 68.2 - 67.2 = 1.0) -1.0 longhand condition (.e., 66.2 - 67.2 = -1.0). add values intercept, get condition means. discuss , relevant part answering statistical question interest. Figure 5.2, red dashed line (red points) show condition means, thus condition effects differences blue line red lines.\\(\\text{IV-effect}\\): IV-effect represents signal, effect independent variable define difference condition means intercept (.e., deviation condition means intercept). Thus, always many different IV-effects conditions. example data two conditions, two different IV-effects, magnitude differ sign, 1.0 laptop condition (.e., 68.2 - 67.2 = 1.0) -1.0 longhand condition (.e., 66.2 - 67.2 = -1.0). add values intercept, get condition means. discuss , relevant part answering statistical question interest. Figure 5.2, red dashed line (red points) show condition means, thus condition effects differences blue line red lines.\\(\\text{residual}\\): residuals idiosyncratic aspects data left unexplained statistical model. words, noise data. model predicts condition means (.e., intercepts plus independent variable), deviations individual observations condition means. Thus, DV, many residuals values DV. Figure 5.2, residuals shown grey lines condition means data point. information (variability data) model explain. discussed , observed level noise – function size residuals number observations – important quantity applying NHST. Therefore, residuals play central role NHST.\\(\\text{residual}\\): residuals idiosyncratic aspects data left unexplained statistical model. words, noise data. model predicts condition means (.e., intercepts plus independent variable), deviations individual observations condition means. Thus, DV, many residuals values DV. Figure 5.2, residuals shown grey lines condition means data point. information (variability data) model explain. discussed , observed level noise – function size residuals number observations – important quantity applying NHST. Therefore, residuals play central role NHST.","code":"\nset.seed(1234566)  ## needed to ensure jittered points are at same position\np1_de <- ggplot_build(p1)\n#> No summary function supplied, defaulting to `mean_se()`\n\nlap2 <- laptop_urry %>% \n  mutate(xpoint = p1_de$data[[2]]$x)\n\nlap3 <- laptop_urry %>% \n  group_by(condition) %>% \n  summarise(overall = mean(overall))\n\nlap4 <- lap3 %>% \n  summarise(overall = mean(overall))\n\nlap5 <- p1_de$data[[1]] %>% \n  group_by(x) %>% \n  summarise(xmin = first(xmin),\n            xmax = first(xmax)) %>% \n  bind_cols(lap3)\n\n\nlap6 <- p1_de$data[[2]] %>% \n  group_by(group) %>% \n  mutate(overall = mean(y)) %>% \n  as_tibble() %>% \n  select(-size, - fill, -alpha, - stroke)\n\nset.seed(1234566)\np2 <- ggplot(laptop_urry, aes(x = condition, y = overall)) +\n  geom_violin() +\n  geom_segment(data = lap6, \n               mapping = aes(x = x, xend = x, y = overall, yend = y),\n               colour = \"grey\") +\n  ggbeeswarm::geom_quasirandom() +\n  geom_hline(yintercept = lap4$overall, linetype = 3, colour = \"blue\") +\n  geom_segment(data = lap5, mapping = aes(x = xmin, xend = xmax,\n                                          y = overall, yend = overall),\n               colour = \"red\", linetype = 2) +\n  stat_summary(colour = \"red\") +\n  labs(y = \"memory score\")\n# to show figure execute (without comment):\n# p2"},{"path":"standard1.html","id":"model-predictions","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.4.1 Model Predictions","text":"simplification Equation (5.1), makes clearer statistical model predicts, obtained ignore residuals moment. reminder, residuals part data remains unexplained. words, residuals represent noise – idiosyncratic parts data independent manipulation. example, participants may better memory others independent took notes.remains statistical model ignore idiosyncratic aspects predictions based IV. case experimental data, IV experimental condition. Thus, statistical model actually predicts means experimental conditions. can formalise asHere, hat symbol (\\(\\hat{}\\)) means predicted value. Thus, contrast actual DV , predicted DV equation.performing statistical analyses sometimes can helpful remind oneself regular statistical model predicts condition means. generally make predictions individual participants consider factors part model. predict, interested , condition means.","code":""},{"path":"standard1.html","id":"statistical-model-for-the-example-data","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.4.2 Statistical Model for the Example Data","text":"Let us take look first six participants values variables basic statistical model get better understanding Equation (5.1).first three columns show data. pid participant identifier (id) column. often case real data, ids missing various reasons (3 7). example, can happen potential participants interested study received id, finish start experiment. consequence, first 6 rows already go pid = 8. condition factor (.e., categorical variable) tells us note taking condition participant. overall memory score scale 0 100 serves numeric DV statistical model (.e., left-hand side Equation (5.1)).intercept, iv_effect, residual columns contain values variables right-hand side Equation (5.1). addition, prediction column shows left-hand side Equation (5.2).described , every observation (.e., row) idiosyncratic DV residual. also see values share one intercept, IV-effect condition specific. consequence, prediction column (sum intercept IV-effect) also two different values, one condition.can see data sum three values right-hand side Equation (5.1) equals observed value DV. example, consider pid = 4. enter values Equation (5.1) \\[\n50.0 = 67.2 + (-1) + (-16.2).\n\\]example data can also better understand residuals mean; difference observed value predicted value, \\(\\text{residual} = \\text{DV} - \\hat{\\text{DV}}\\). Consider pid = 4. participant residual \\[\n-16.2 = 50.0 - 66.2.\n\\]can also see residual captures idiosyncratic aspects data explained condition means. example, participants – pid = 5 pid = 8 – large positive residuals indicating good memory independent note taking condition. Likewise, pid = 4 large negative residual indicating comparatively worse memory (independent note taking condition). words, one part unexplained information data standard statistical model individual differences.","code":""},{"path":"standard1.html","id":"understanding-the-statistical-model","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.4.3 Understanding the Statistical Model","text":"Now described parts statistical model, almost ready fit model interpret output. , makes sense look parts individually goal trying understand set statistical model way . Remember, goal evaluate whether effect experimental manipulation – difference two note taking conditions – population data sampled. , set model partitions observed data three parts:intercept representing overall mean,condition specific effect, IV-effect, representing difference condition means intercept, can understood signal data,residuals representing idiosyncratic part explained model, can understood noise data.reason separating data way allows us zoom matters, signal noise. answer question difference conditions population, can now focus relevant parts model. overall level performance captured intercept can ignored question. Consequently, statistical test reported statistical test condition effect based level noise captured residuals. Thus, reason setting statistical model way make easy get answer question interests us: effect note taking manipulation/conditions memory? answer question need consider condition effect light observed level noise.","code":""},{"path":"standard1.html","id":"estimating-the-statistical-model-in-r","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.5 Estimating the Statistical Model in R","text":"theoretical introduction, now jump practical part. means, show set statistical model R perform interpret corresponding statistical test.","code":""},{"path":"standard1.html","id":"package-and-data-setup","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.5.1 Package and Data Setup","text":"statistical analyses reported book generally use afex package (Singmann et al. 2021) combination emmeans package (Lenth 2021). afex stands “analysis factorial experiments” simplifies many things want (full disclaimer: main developer afex). analyses can also performed different functions, often easiest use afex functions developed particularly cognitive behavioural researchers working experimental data. specifically, afex functions provide expected results experimental data sets ---box without need change settings (true corresponding non-afex functions). emmeans stands “estimated marginal means” package use statistical model estimated investigate results. afex emmeans fully integrated . integration makes easy test practically hypotheses interest combination two packages straight forward manner. already introduce interplay two packages , next chapters showcase full power combination.also regular use functions tidyverse package (e.g., plotting), introduced Chapters 3 4.begin analysis usual way restarting R. load three packages (use install.packages(c(\"afex\", \"emmeans\", \"tidyverse\")) case yet installed). also change default ggplot2 theme using theme_set() nicer one.next step loading data. made easy data Urry et al. (2021) part afex, name laptop_urry. means, can load data() function. Note calling data() function laptop_urry object yet exist workspace. Thus, need call data() function name character vector (.e., enclosed \"). loads laptop_urry object workspace (similar way using read_csv() ). get overview variables data set using str(), returns structure data.frame.str() function shows six variables, three already mentioned :pid: participant identifier, factor 142 levels, one participant.pid: participant identifier, factor 142 levels, one participant.condition: factor identifying note taking condition participant belongs , two levels, laptop longhand.condition: factor identifying note taking condition participant belongs , two levels, laptop longhand.talk: factor identifying TED talk participant saw, 5 level.talk: factor identifying TED talk participant saw, 5 level.overall: Numeric variable participants’ overall memory performance scale 0 (= memory) 100 (= perfect memory). variable called overall average two separate memory performance scores given .overall: Numeric variable participants’ overall memory performance scale 0 (= memory) 100 (= perfect memory). variable called overall average two separate memory performance scores given .factual: Numeric variable participants’ memory score factual questions (ignored chapter).factual: Numeric variable participants’ memory score factual questions (ignored chapter).conceptual: Numeric variable participants’ memory score conceptual questions (analysed Chapter 6).conceptual: Numeric variable participants’ memory score conceptual questions (analysed Chapter 6).","code":"\nlibrary(\"afex\")\nlibrary(\"emmeans\")\nlibrary(\"tidyverse\")\ntheme_set(theme_bw(base_size = 15) + \n            theme(legend.position=\"bottom\", \n                  panel.grid.major.x = element_blank()))\ndata(\"laptop_urry\")\nstr(laptop_urry)\n#> 'data.frame':    142 obs. of  6 variables:\n#>  $ pid       : Factor w/ 142 levels \"1\",\"2\",\"4\",\"5\",..: 1 2 3 4 5 6 7 8 9 10 ...\n#>  $ condition : Factor w/ 2 levels \"laptop\",\"longhand\": 1 2 2 1 2 2 1 2 2 1 ...\n#>  $ talk      : Factor w/ 5 levels \"algorithms\",\"ideas\",..: 4 4 2 5 1 3 5 2 5 4 ...\n#>  $ overall   : num  65.8 75.8 50 89 75.6 ...\n#>  $ factual   : num  61.7 68.3 33.3 85.7 69.2 ...\n#>  $ conceptual: num  70 83.3 66.7 92.3 82.1 ..."},{"path":"standard1.html","id":"estimating-the-statistical-model","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.5.2 Estimating the Statistical Model","text":"estimating basic statistical model using afex, can use aov_car() function. Let’s first show corresponding call look like example data, describing arguments aov_car() detail. Note following code save output R object res1 (stands “results 1”).call aov_car() consisted two arguments: first argument formula specifying statistical model, overall ~ condition + Error(pid). second argument identifies data.frame containing data (.e., variables appearing formula), laptop_urry. can also see calling aov_car() produces status message informing us contrasts set contr.sum IVs model. message shown information purposes mean anything ordinary. explain later chapters thoroughly contrasts .49Let’s take depth look first argument aov_car(), formula overall ~ condition + Error(pid). formula R defined presence tilde-operator ~ main way specifying statistical models. allows specifying statistical models way similar mathematical formulation, specifically prediction equation statistical model, Equation (5.2). Therefore, formula provides comparatively intuitive approach specifying statistical model. left hand side ~ dependent variable, overall. right hand side variables want use predict dependent variable.present case, right-hand side consists two parts concatenated +, independent variable condition Error() term participant identifier variable pid. Thus, two differences formula used prediction Equation (5.2). formula misses intercept specified Error() term missing Equation (5.2). Let us address two difference turn. intercept actually missing equation, implicitly included. specifically, intercept specified using 1 formula. However, unless intercept explicitly suppressed – can done including 0 formula rarely makes sense (.e., done really know exactly makes statistical sense case) – always assumed part model. Consequently, including explicitly produces equivalent results implicitly included. following code shows comparing previous result without explicit intercept, res1, aov_car result explicit intercept, res1b. comparison performed using .equal() function. function can used compare arbitrary R objects returns TRUE equal.Error() term mandatory part model formula using aov_car() used specify participant identifier variable (.e., pid case). Without , aov_car() throw error.50Before looking results, let us quickly explain function specifying models called aov_car(). regular statistical model solely includes factors (.e., categorical variables) independent variables also known analysis variance, usually shortened ANOVA.51 also exists base R functions analysis variance, called aov(). However, aov() can produce unexpected inappropriate results specific cases. contrast, aov_car() always produces expected appropriate results therefore preferrable situations. combining base R functions function car package (Fox Weisberg 2019) (car stands book title, “Companion Applied Regression” vehicle).52 aov_car() stands aov help car package.","code":"\nres1 <- aov_car(overall ~ condition + Error(pid), laptop_urry)\n#> Contrasts set to contr.sum for the following variables: condition\nres1b <- aov_car(overall ~ 1 + condition + Error(pid), laptop_urry)\n#> Contrasts set to contr.sum for the following variables: condition\nall.equal(res1, res1b)\n#> [1] TRUE"},{"path":"standard1.html","id":"interpreting-the-results","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.5.3 Interpreting the Results","text":"Let’s now take look results statistical model. , simply call object contains results res1 (get output calling print(res1) nice(res1)).default aov_car() output “Anova Table” see throughout book. can also see results table contains “Type 3 tests,” ignore meaning now.53 next line results table provides information response variable, also know dependent variable DV. Just intended, DV overall.next part output table effects corresponding statistical information. present case table one row, effect condition. row contains information null hypothesis significance test (NHST) condition effect.important column table effects last column, p.value, \\(p\\)-value. \\(p\\)-value main result NHST allows us judge compatibility data null hypothesis. specifically, \\(p\\)-value probability obtaining difference least large observed assuming null hypothesis difference true. \\(p\\)-value probability, 0 1. probability, 0 indicates impossibility 1 indicates absolute certainty. \\(p\\)-value, small values near 0 indicate data appears incompatible null hypothesis – probability obtain result least large observed low assume null hypothesis true. see case \\(p\\)-value small, .471. Thus, data incompatible null hypothesis. means data suggest memory difference note taking laptop longhand format lectures.general, researchers adopted significance level .05. means \\(p\\)-value smaller .05, treat evidence data incompatible null hypothesis. case say result “significant.” However, case result smaller .05 result “significant” (avoid saying “insignificant” \\(p\\)-value larger .05, “significant” technical term ). Thus, present case reject null hypothesis. Therefore, evidence either method note taking superior terms memory performance.two important columns whose results generally need reported, df, stands “degrees freedom” (df), F. Understanding columns detail beyond scope present chapter, introduce briefly.general meaning degrees freedom number independent pieces information used estimating something. independent information , better can estimate thing want estimate. case ANOVA – statistical model use mostly book – two degrees freedom, reported ANOVA table. first value, 1, numerator df, refers number independent data points estimating overall mean. always given number conditions (.e., factor levels) minus 1. present case, two conditions, laptop longhand, numerator df 2 - 1 = 1. second value denominator df, refers overall number independent data points. generally given number participants minus numerator df minus 1. , 142 participants therefore 142 - 1 - 1 = 140. general, larger denominator df (.e., participants ) better can detect incompatibility null hypothesis (.e., easier get small \\(p\\)-values).\\(F\\)-value measure signal versus noise ratio – expresses observed incompatibility data null hypothesis. larger \\(F\\), larger signal relative noise. \\(F \\leq 1\\), data compatible null hypothesis. \\(F > 1\\) data degree incompatible null hypothesis, larger values indicating incompatibility. \\(p\\)-value calculated df \\(F\\)-value. Consequently, results usually reported following way: \\(F(1, 140) = 0.52\\), \\(p = .471\\).54The output contains two columns discuss detail now, ges MSE. ges stands generalised eta-squared, using mathematical notation Greek letters, \\(\\eta^2_G\\), standardised effect size measure tells us something absolute magnitude observed effect (Olejnik Algina 2003; Bakeman 2005).55 Many journals psychology require reporting similar measures, included default output. MSE stands “mean squared errors” column mainly included historical reasons.56 Consequently, ignore .One thing might note results table contain information intercept. discussed , mean intercept included model, included. reason omitting intercept default output generally primary interest. experimental research, main interest usually effect independent variables, effect experimental manipulation. statistical model separates intercept (.e., overall mean) condition effect allows zoom relevant part. line , default output aov_car . Later chapters show can also get information intercept.sum , estimating statistical model aov_car() provides us inferential statistical results, null hypothesis tests IV-effects. get , just need call object containing results R prompt (e.g., calling res1 present case). However, can use results object also others parts statistical analyses, data visualisation follow-analyses.","code":"\nres1\n#> Anova Table (Type 3 tests)\n#> \n#> Response: overall\n#>      Effect     df    MSE    F  ges p.value\n#> 1 condition 1, 140 269.66 0.52 .004    .471\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1"},{"path":"standard1.html","id":"data-visualisation","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.5.4 Data Visualisation","text":"discussed , comprehensive statistical analysis experiment requires just statistical results shown ANOVA table. One important aspect data visualisation (Chapter @ref{#ggplot2-intro}). simplify data visualisation estimated statistical model, can use afex function afex_plot(), built top ggplot2 package.afex_plot() requires estimated model object (e.g., returned aov_car()) specification factors model want plot. present case, one factor, condition, can choose one. factors passed afex_plot() need passed character strings (.e., enclosed \"...\").\nFigure 5.3: afex_plot() figure data Urry et al. (2021)\nsimple call afex_plot() produces already good looking figure along lines discussion Section @ref{#ggplot-cat-cont}. figure combines individual-level data points (background grey) condition means (black). Individual data points background similar values displaced x-axis lie top . achieved geom_beeswarm geom package ggbeeswarm. plot also per default includes error bars around mean, depict 95% confidence intervals means. said , error bars indicate uncertainty estimate mean. full definition confidence interval shows given later chapter.afex_plot() returns ggplot2 plot object, can manipulate plot make nicer.\nFigure 5.4: Nicer afex_plot() figure data Urry et al. (2021)\nexample, code snippet first save plot object p1 call number ggplot2 function plot object alter plot appearance (discussed , graphical elements added ggplot2 plot using +). Function labs() used change axis labels, coord_cartesian() changes extent y-axis (.e., plot now show full possible range memory performance score), geom_line(aes(group = 1) adds line connecting two means. figure included results report, dissertation, manuscript.","code":"\nafex_plot(res1, \"condition\")\np1 <- afex_plot(res1, \"condition\")\np1 + \n  labs(x = \"note taking condition\", y = \"memory performance (0 - 100)\") +\n  coord_cartesian(ylim = c(0, 100)) +\n  geom_line(aes(group = 1))"},{"path":"standard1.html","id":"follow-up-analysis","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.5.5 Follow-Up Analysis","text":"Another important part comprehensive statistical analysis follow-analysis. Follow-analysis catch-term use describe additional statistical analyses performed estimating initial statistical model inspecting ANOVA table. Follow-analysis generally involves inspection predicted condition means statisrtical analysis relationships. case single independent variable two levels (e.g., laptop versus longhand) much investigate regard. can nevertheless illustrate general procedure.follow-analyses, generally begin function emmeans() package emmeans (Lenth 2021). afex_plot(), emmeans() requires estimated model object first argument specification factors model interested . emmeans() returns estimated marginal means, slightly complicated way saying condition means plus additional statistical information. Let’s try example data.now, focus estimates means column emmean ignore additional inferential statistical information columns SE upper.CL. can see reported means match means given text beginning chapter Section 5.1.power emmeans provide condition means, also allows us perform calculation condition means. example, case factor two levels, can easily calculate difference two condition means. difference condition means also measure size effect. However, compared ges column default output, standardised, simple effect size expressed units dependent variable. often easier understand simple, compared standardised effect sizes, generally prefer measure.calculate effect condition emmeans, first need save object returned emmeans(). can call pairs() function object. gives us pairwise comparisons conditions means; , difference among pairs condition means. present case, one pair:57The output shows mean difference 1.99 slightly differs 2.0 difference reported Section @ref{#ex:urry} (.e., mean laptop = 68.2 - mean longhand = 66.2). deviation model estimates descriptive results considered slightly concerning. Fortunately, difference real consequence differential rounding. results reported rounded one decimal. mode-based results, also get estimated difference 2.0 (explain code detail ):","code":"\nemmeans(res1, \"condition\")\n#>  condition emmean   SE  df lower.CL upper.CL\n#>  laptop      68.2 1.99 140     64.3     72.1\n#>  longhand    66.2 1.91 140     62.4     70.0\n#> \n#> Confidence level used: 0.95\nem1 <- emmeans(res1, \"condition\")\npairs(em1)\n#>  contrast          estimate   SE  df t.ratio p.value\n#>  laptop - longhand     1.99 2.76 140   0.722  0.4715\nem1 %>% \n  pairs() %>% \n  as.data.frame() %>% \n  format(digits = 1, nsmall = 1)\n#>            contrast estimate  SE    df t.ratio p.value\n#> 1 laptop - longhand      2.0 2.8 140.0     0.7     0.5"},{"path":"standard1.html","id":"summary-4","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.6 Summary","text":"goal chapter introduce standard statistical approach analysing experimental data one independent variable two levels – experiment two conditions. Practically every time run experiment, observe mean difference dependent variable two conditions. example data Urry et al. (2021), memory difference 2.0 points two note taking conditions (laptop versus longhand) response scale 0 100.important statistical question whether evidence suggesting observed difference sample generalises population. sample participants experiment population refers possible participants sampled. Urry et al. (2021), population can loosely described students taking notes precisely undergraduate students research intensive (R1) US universities. question like get statistical answer : believe generally memory difference note taking laptop versus longhand? words, genuine signal observed difference just noise? answer question need inferential statistics.inferential statistical approach using called null hypothesis significance testing NHST. NHST directly address question whether evidence difference population. Instead, NHST tests compatibility data null hypothesis – assumption difference condition population. important result NHST \\(p\\)-value. \\(p\\)-value measure compatibility data null hypothesis; probability obtaining results least extreme observed, assuming null hypothesis true. \\(p\\)-value smaller .05, reject null hypothesis difference. case, decide evidence difference (although follow logical necessity).apply NHST data, set statistical model partitions observed data three parts (Equation (5.1)): intercept representing overall mean, effect independent variable (.e., difference condition means intercept), residuals representing idiosyncratic aspects explained parts model. effect independent variable estimate signal residuals used derive estimate noise data. partitioning allows us zoom part data mainly interested , effect independent variable, experimental manipulation.estimate statistical model data, use function aov_car() afex package. aov_car() allows us specify statistical model using formula form dv ~ iv + Error(pid) (pid refers variable data participant identifier), resembling mathematical specification statistical model. default output returns ANOVA table provides null hypothesis significance test iv, independent variable. returned table called ANOVA table statistical models contain factors called analysis variance ANOVA. present case, statistical model single factor, note taking condition, two levels, laptop versus longhand. returned ANOVA table, \\(p\\)-value experimental factor, additional inferential statistical information degrees freedom, df, \\(F\\)-value.can use object returned aov_car() plotting using function afex_plot(). function produces plot combining individual-level data points condition means error bars indicating uncertainty estimate condition means. provides comprehensive display data experiment. function returns ggplot2 object, plot can easily modified create figure can used results report.can also use object returned aov_car follow-analyses using emmeans. emmeans, can easily obtain condition means (estimated marginal means) dependent variable. Based condition means, can calculate observed effect size (.e., mean difference).Applying statistical model data Urry et al. (2021) showed non significant difference, \\(F(1, 140) = 0.52\\), \\(p = .471\\). suggests difference memory performance watching talk taking notes either laptop longhand format.","code":""},{"path":"case-study-laptop.html","id":"case-study-laptop","chapter":"6 Case Study: More Results from Note Taking Studies","heading":"6 Case Study: More Results from Note Taking Studies","text":"chapter apply learned previous chapter - analyse experimental data one experimental manipulation two conditions. , take look data Urry et al. (2021). Additionally, analyse data Mueller Oppenheimer (2014). study first published study investigating question note taking laptop longhand format basis Urry et al. (2021) planned study. data Mueller Oppenheimer (2014) perform full analysis starting reading data. addition performing statistical hypothesis test, calculate descriptive statistics.start analysis chapter way previous chapter, loading three packages generally use, afex, emmeans, tidyverse, set nicer ggplot2 theme. probably good idea restart R (unless, course, just starting R). RStudio can conveniently done menu clicking Session Restart R. R environments might need restart program. benefit restarting R create blank R session packages loaded objects exist workspace. blank sessions ensures , obtained set results, can recreate later using code. , blank R session avoids potential problems due analyses performed previous session still lingering. Restarting R generally done starting new analysis one completely done analysis. latter case, makes sense restart R rerun code one saved ones script ensure results replicate based code script (require additional code saved).","code":"\nlibrary(\"afex\")\nlibrary(\"emmeans\")\nlibrary(\"tidyverse\")\ntheme_set(theme_bw(base_size = 15) + \n            theme(legend.position=\"bottom\", \n                  panel.grid.major.x = element_blank()))"},{"path":"case-study-laptop.html","id":"conceptual-memory-data-from-urry-et-al.-2021","chapter":"6 Case Study: More Results from Note Taking Studies","heading":"6.1 Conceptual Memory Data from Urry et al. (2021)","text":"quick reminder, Urry et al. (2021) showed participants short lectures (TED talks) video participants allowed take notes. One group participants, laptop condition, take notes laptop, whereas participants longhand condition take notes pen paper. lecture, participants quizzed two aspects content lecture, factual questions conceptual questions.example one videos Kevin Slavin “Algorithms Shape World”. factual question video : “New York City, Internet distributed ?” conceptual question video : “algorithms useful successful stock trading?”previous chapter (5), analysed overall memory score. overall memory score average memory scores factual questions conceptual questions. present analysis, concerned memory performance conceptual questions.begin analysis reading-data. part afex, can read-calling data() command (works afex attached library()). get overview variables using str()., participants identifier variable pid note taking condition variable condition. can also guess conceptual memory scores aptly name variable conceptual (unsure , also check documentation data ?laptop_urry).Usually, data sufficiently prepared (.e., performed coherence checks identified DV IV), first step analysis plotting data. done using ggplot2 directly. However, present case clear statistical model going estimate. case, using afex_plot() involves bit less typing. Thus, start estimating statistical model conceptual memory performance data Urry et al. (2021) save estimated model object mc_urry (mc = model conceptual scores) . , use aov_car() laptopt_urry data specify statistical model using formula interface. DV considering conceptual, IV condition, participant identifier pid. Consequently, formula conceptual ~ condition + Error(pid). , looking inferential statistical results, use model object plot data using afex_plot.\nFigure 6.1: Conceptual memory scores Urry et al. (2021) across note taking conditions\nreason beginning plotting data allows see whether data looks alright. , check whether features stand , single observations extreme values (“outliers”) unusual patterns. case, try figure can find reason issues want deal . , data looks alright, continue consider results significance test:ANOVA table reveals significance test effect condition significant, \\(p = .319\\). Thus, line finding evidence difference overall memory performance, also evidence difference memory conceptual information.also employ emmeans, investigating condition means (estimated marginal means). line Figure 6.1, memory score laptop condition descriptively around 3.5 points higher score longhand condition. surprising, afex_plot uses emmeans internally. Therefore, means shown figure exactly values means shown .moving next data set, let’s consider report analysis research report. example, write:shown Figure 6.1, participants’ conceptual memory scores (scale 0 100) descriptively slightly larger laptop condition compared longhand condition. analysed scores ANOVA one factor, note taking condition, two levels (laptop vs. longhand). effect note taking condition significant, \\(F(1, 140) = 1.00\\), \\(p = .319\\). indicates data provide evidence difference memory conceptual information based notes taken video lectures.","code":"\ndata(\"laptop_urry\")\nstr(laptop_urry)\n#> 'data.frame':    142 obs. of  6 variables:\n#>  $ pid       : Factor w/ 142 levels \"1\",\"2\",\"4\",\"5\",..: 1 2 3 4 5 6 7 8 9 10 ...\n#>  $ condition : Factor w/ 2 levels \"laptop\",\"longhand\": 1 2 2 1 2 2 1 2 2 1 ...\n#>  $ talk      : Factor w/ 5 levels \"algorithms\",\"ideas\",..: 4 4 2 5 1 3 5 2 5 4 ...\n#>  $ overall   : num  65.8 75.8 50 89 75.6 ...\n#>  $ factual   : num  61.7 68.3 33.3 85.7 69.2 ...\n#>  $ conceptual: num  70 83.3 66.7 92.3 82.1 ...\nmc_urry <- aov_car(conceptual ~ condition + Error(pid), laptop_urry)\n#> Contrasts set to contr.sum for the following variables: condition\nafex_plot(mc_urry, \"condition\")\nmc_urry\n#> Anova Table (Type 3 tests)\n#> \n#> Response: conceptual\n#>      Effect     df    MSE    F  ges p.value\n#> 1 condition 1, 140 441.76 1.00 .007    .319\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\nemmeans(mc_urry, \"condition\")\n#>  condition emmean   SE  df lower.CL upper.CL\n#>  laptop      73.5 2.55 140     68.5     78.6\n#>  longhand    70.0 2.44 140     65.2     74.8\n#> \n#> Confidence level used: 0.95"},{"path":"case-study-laptop.html","id":"why-are-experiments-replicated","chapter":"6 Case Study: More Results from Note Taking Studies","heading":"6.2 Why are Experiments Replicated?","text":"experiment Urry et al. (2021) first experiment investigating effect type note taking lectures memory. contrast, study replication Mueller Oppenheimer (2014). replication act repeating previous study goal checking one can obtain (replicate) similar results previous study.discussed , inferences NHST never conclusive probabilistic require multiple inferential steps. Replications one important tools science overcoming least probabilistic uncertainties associated inferences draw experimental data. example consider several independent otherwise similar possible experiments – , replications experiment – obtain significant result (.e., indicate data incompatible null hypothesis). pattern dramatically increase confidence null hypothesis likely false. words, replication studies main tool overcoming second inferential gap identified Section 1.3.2.addition gain confidence specific results, good practical reasons replicating existing experiment. example, beginning work new topic generally good idea replicate experiment one wants build . one already encounters problems replicating results reported literature, suggests topic may less well-understood portrayed literature.Another excellent reason performing replication believing existing result. Remember, one key components scientific method scepticism (least according Wikipedia). result difficult believe, reasonable sceptical position require evidence. replication one way – best way – produce additional evidence particular empirical finding. believing existing experiments also imply one questions integrity researchers experiment. many completely harmless reasons study might replicate. example, researchers might just obtained significant results chance (happens 5% cases, discussed next chapters).Sadly, replicating existing experiments publishing results, still norm psychology related disciplines. Quite contrary, situation dire many fields currently considered replication crisis. example, large scale effort replicate 100 studies psychology (Open Science Collaboration 2015) showed less 50% studies successfully replicated. Similarly sobering results since observed across social sciences (Camerer et al. 2018, 2016; Klein et al. 2018). Much written problem right place rehash arguments. best summary situation book Chris Chambers (Chambers 2017). important thing realise science cumulative endeavour. Every new experiment builds existing research. existing research never replicated, confidence research low. questions foundations new work builds non-replicated research. move forward researchers need replicate work important research, value replications done others (especially work), let findings replicate fade obscurity.","code":""},{"path":"case-study-laptop.html","id":"conceptual-memory-scores-from-mueller-and-oppenheimer-2014-experiment-2","chapter":"6 Case Study: More Results from Note Taking Studies","heading":"6.3 Conceptual Memory Scores from Mueller and Oppenheimer (2014) Experiment 2","text":"Urry et al. (2021) direct replication Mueller Oppenheimer (2014), design uses materials (.e., TED talks questions). Participants watched short lecture videos (projected onto screen) take notes either laptop longhand format. 30 minutes lecture asked factual conceptual questions lectures. answers coded first author. previous analysis, transform answers memory score 0 (= memory) 100 (= perfect memory). line analysis Urry et al. (2021) , interested conceptual memory scores .experiment Urry et al. (2021) direct replication Experiment 1 Mueller Oppenheimer (2014). , first focus Experiment 2 Mueller Oppenheimer (2014), also direct replication Experiment 1 included additional experimental manipulation ignore. reason focussing Experiment 2 first data bit straightforward interpret. get back Experiment 2 , providing complete overview results data sets investigating research question whether mode taking note lectures affects memory employ experimental design introduced Mueller Oppenheimer.Luckily us, data Mueller Oppenheimer (2014), including data Experiment 2, available online Open Science Framework (OSF). OSF one visible developments resulting replication crisis. free website allows researchers share data materials associated research. replication crisis OSF, rare get access data underlying published studies. Nowadays, many researchers depose (anonymised) data published studies OSF include links data papers. allows researcher, us, reanalyse existing data ensure reported results can reproduced.Note book distinguish reproducing result replicating result. Reproducing means able obtain exact results using exact data. , given data analysis script description analysis, can perform statistical analysis end result. contrast, replication means repeating study new participants able obtain qualitatively result. words, replicating necessarily entails collecting new data whereas reproducing based data.","code":""},{"path":"case-study-laptop.html","id":"preparing-the-data","chapter":"6 Case Study: More Results from Note Taking Studies","heading":"6.3.1 Preparing the Data","text":"get habit downloading data OSF reanalysing , going now. file need called Study 2 abbreviated data.csv can found following OSF link: https://osf.io/t43ua/ Please go ahead download now put data folder can access .use original data available OSF, preparing data analysis involves steps previous analyses. example, first need figure variables observations relevant. also perform simple coherence checks make sure assumptions variables data hold. involved data preparation phase, also requires knowledge experiment, paints realistic picture data preparation. ones data, one also spend time checking problems data.data preparation heavily draws tidyverse knowledge introduced Chapter 3. use function read_csv() read-data assign object mo2014_e2. reminder, read_csv() always returns tibble (tidyverse version data.frame). use glimpse() function (also tiydverse function) get overview data (similar str() less verbose tibbles).can see data 153 participants 22 columns. Many columns names immediately clear. uncommon. important task getting new data set trying figure variables mean. uncommon one also also data experiments. example, software running experiments often collect variables needed analysis. Consequently, one first step actually anything data usually figuring variables mean one needed ., can already see 153 participants final number participants reported Mueller Oppenheimer (2014). Instead, removed two participants analysis resulting 151 participants. Studying OSF repository detail – particular published SPPS script output, Output Syntax - Study 2.doc) – shows participants number 194 237 need removed (information can found variable filter_$ current data set). file also shows two relevant notetype conditions 1 = longhand 2 = laptop, remove observations notetype == 3 analysis (notetype == 3 additional condition ignore ). moving , filter data remove observations.filtering observations tidyverse skills introduced Chapter 3: pipe (%>%) data mo2014_e2 filter(). Importantly, overwrite mo2014_e2 filtered tibble need filtered observations (use , read-data ). see many participants remain using nrow(). nrow() function returns number rows data. every participant spans one row, can use get overall number participants.reported 99 participants matches 99 participants reported OSF two conditions, laptop versus longhand.Sadly, OSF include codebook describing variables particular data set (earlier version data set variables overlaps degree present one). However, looking data information OSF, things clear: participantid participant identifier variable, notetype condition identifier coding experimental condition, whichtalk identifies TED talk participants saw (mapping talks numbers also given SPPS output){target=“_blank”}.first step, can transform relevant indicator variables, participant experimental condition variable, factors. discussed , transforming variable factor guarantees none analyses incorrectly treats one factors (.e., categorical variables) numerical variable. example, taking mean numbers participant identifier column reasonable statistical operation. Instead overwriting existing variables, create new variable name analysis Urry et al. (2021), pid condition. also assign human understandable labels instead using 1 2 condition codes. make easier understand pattern results. use factor() inside mutate() tidyverse combination pipe operator %>%.Looking data reveals newly created variables added end tibble.next step calculate dependent variable, memory scores 0 100 used analysis Urry et al. (2021). can see two question types, factual conceptual, multiple measures. index score raw score well perfect variants types scores. perfect presumably means maximal possible value obtained score observation (.e., row). data also contains number \\(z\\)-transformed variants scores (variables Z...), ignore (original paper used z-scores main analysis, difficult interpret results qualitatively , ignore ). focus index score gives participant maximal 1 point per question (information given paper/OSF). Let us take look first six observations relevant variables.can see number questions per question type talk differs (indicated difference perfect-indexscore values across rows), total number items appears always ten (perfectfactindexscore + perfectconceptindexscore = 10 row). also aligns list items found OSF, shows ten questions per talk number factual conceptual questions differing across talks. information calculate memory scores. However, moving makes sense run quick coherence check verify number questions per row indeed ten rows. , create new variable sum two perfect index scores, using mutate() (adds variable existing data), see whether sum always equal 10, using summarise() (reduces data one row). number question per observation/row sums ten observations, return TRUE.Fortunately, check passes. therefore feel assumptions meaning variables supported. Consequently, can go ahead calculate memory score.preparing data analysis, running analysis, important regularly include coherence checks ones analysis. analysis involves assumptions underlying data – example, variable means, values variable can possibly take , observations included data. Based assumption calculate variables data perform analysis. However, humans fallible data analysis experience shows assumptions sometimes (regularly) false. Sometimes one misunderstood (misremembers) meaning variable, might data entry error, data still includes observations excluded (e.g., test runs researchers instead participants).example, study Lewandowsky, Gignac, Oberauer (2013) age one participants recorded 32,757 years error uncovered publication manuscript. Fortunately Lewandowsky, Gignac, Oberauer (2013), error affect substantive conclusion drawn data minor effects reported statistical results. correct error published correction (Lewandowsky, Gignac, Oberauer 2015). correction short note scientific journal describing error consequences well corrected result without error.Publishing correction nothing dramatic (also papers published corrections errors discovered publication), course prefer . errors affect conclusion substantially, sometimes correction enough paper retracted. retraction means scientific article removed journal. Regular coherence assumptions checks ones analysis way minimise chance errors final analysis.Based positive outcomes checks, somewhat confident understood variables data can calculate memory score 0 100. , divide index score perfect...indexscore multiply results 100. simplify coming analysis, create new tibble, mo2014_e2a, retains variables really need analysis. done using tidyverse function select(). take another look first six rows data using head(). shows data now ready reanalysis.","code":"\nmo2014_e2 <- read_csv(\"data/Study 2 abbreviated data.csv\")\nglimpse(mo2014_e2)\n#> Rows: 153\n#> Columns: 22\n#> $ participantid            <dbl> 103, 122, 142, 152, 172, ~\n#> $ notetype                 <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1~\n#> $ whichtalk                <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1~\n#> $ Wcount                   <dbl> 148, 273, 124, 149, 182, ~\n#> $ threeG                   <dbl> 0.08219178, 0.05535055, 0~\n#> $ factualindex             <dbl> 2.499, 2.833, 2.333, 4.49~\n#> $ conceptualindex          <dbl> 2.0, 1.5, 2.0, 2.0, 2.0, ~\n#> $ factualraw               <dbl> 6, 7, 5, 11, 12, 4, 7, 11~\n#> $ conceptualraw            <dbl> 4, 3, 4, 4, 4, 3, 4, 3, 4~\n#> $ perfectfactindexscore    <dbl> 7, 7, 7, 7, 7, 7, 7, 7, 7~\n#> $ perfectconceptindexscore <dbl> 3, 3, 3, 3, 3, 3, 3, 3, 3~\n#> $ perfectfactscore         <dbl> 14, 14, 14, 14, 14, 14, 1~\n#> $ perfectconceptscore      <dbl> 6, 6, 6, 6, 6, 6, 6, 6, 6~\n#> $ `filter_$`               <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1~\n#> $ ZFindexA                 <dbl> -0.17488644, 0.07568357, ~\n#> $ ZCindexA                 <dbl> 0.52136737, 0.03238307, 0~\n#> $ ZFrawA                   <dbl> 0.32848714, 0.68152314, -~\n#> $ ZCrawA                   <dbl> 0.9199338, 0.2942131, 0.9~\n#> $ ZFindexW                 <dbl> -0.76333431, -0.47725123,~\n#> $ ZCindexW                 <dbl> 0.79471336, 0.06811829, 0~\n#> $ ZFrawW                   <dbl> -0.6437995, -0.2476152, -~\n#> $ ZCrawW                   <dbl> 0.9647838, 0.1731663, 0.9~\nmo2014_e2 <- mo2014_e2 %>% \n  filter(notetype != 3, participantid != 194, participantid != 237)\nnrow(mo2014_e2)\n#> [1] 99\nmo2014_e2 <- mo2014_e2 %>% \n  filter(notetype != 3) %>% \n  mutate(\n    pid = factor(participantid),\n    condition = factor(notetype, \n                       levels = c(2, 1),\n                       labels = c(\"laptop\", \"longhand\"))\n  )\nglimpse(mo2014_e2)\n#> Rows: 99\n#> Columns: 24\n#> $ participantid            <dbl> 103, 122, 142, 152, 172, ~\n#> $ notetype                 <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1~\n#> $ whichtalk                <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1~\n#> $ Wcount                   <dbl> 148, 273, 124, 149, 182, ~\n#> $ threeG                   <dbl> 0.08219178, 0.05535055, 0~\n#> $ factualindex             <dbl> 2.499, 2.833, 2.333, 4.49~\n#> $ conceptualindex          <dbl> 2.0, 1.5, 2.0, 2.0, 2.0, ~\n#> $ factualraw               <dbl> 6, 7, 5, 11, 12, 4, 7, 11~\n#> $ conceptualraw            <dbl> 4, 3, 4, 4, 4, 3, 4, 3, 4~\n#> $ perfectfactindexscore    <dbl> 7, 7, 7, 7, 7, 7, 7, 7, 7~\n#> $ perfectconceptindexscore <dbl> 3, 3, 3, 3, 3, 3, 3, 3, 3~\n#> $ perfectfactscore         <dbl> 14, 14, 14, 14, 14, 14, 1~\n#> $ perfectconceptscore      <dbl> 6, 6, 6, 6, 6, 6, 6, 6, 6~\n#> $ `filter_$`               <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1~\n#> $ ZFindexA                 <dbl> -0.17488644, 0.07568357, ~\n#> $ ZCindexA                 <dbl> 0.52136737, 0.03238307, 0~\n#> $ ZFrawA                   <dbl> 0.32848714, 0.68152314, -~\n#> $ ZCrawA                   <dbl> 0.9199338, 0.2942131, 0.9~\n#> $ ZFindexW                 <dbl> -0.76333431, -0.47725123,~\n#> $ ZCindexW                 <dbl> 0.79471336, 0.06811829, 0~\n#> $ ZFrawW                   <dbl> -0.6437995, -0.2476152, -~\n#> $ ZCrawW                   <dbl> 0.9647838, 0.1731663, 0.9~\n#> $ pid                      <fct> 103, 122, 142, 152, 172, ~\n#> $ condition                <fct> longhand, longhand, longh~\nmo2014_e2 %>% \n  select(pid, condition, factualindex, conceptualindex, \n         perfectfactindexscore, perfectconceptindexscore) %>% \n  as.data.frame() %>% \n  head()\n#>   pid condition factualindex conceptualindex\n#> 1 103  longhand        2.499             2.0\n#> 2 122  longhand        2.833             1.5\n#> 3 142  longhand        2.333             2.0\n#> 4 152  longhand        4.499             2.0\n#> 5 172  longhand        4.999             2.0\n#> 6 183  longhand        1.833             1.5\n#>   perfectfactindexscore perfectconceptindexscore\n#> 1                     7                        3\n#> 2                     7                        3\n#> 3                     7                        3\n#> 4                     7                        3\n#> 5                     7                        3\n#> 6                     7                        3\nmo2014_e2 %>% \n  mutate(sum_p_index = perfectfactindexscore + perfectconceptindexscore) %>% \n  summarise(check = all(sum_p_index == 10))\n#> # A tibble: 1 x 1\n#>   check\n#>   <lgl>\n#> 1 TRUE\nmo2014_e2a <- mo2014_e2 %>% \n  mutate(\n    factual = factualindex / perfectfactindexscore * 100,\n    conceptual = conceptualindex / perfectconceptindexscore * 100\n  ) %>% \n  select(pid, condition, factual, conceptual)\nhead(mo2014_e2a)\n#> # A tibble: 6 x 4\n#>   pid   condition factual conceptual\n#>   <fct> <fct>       <dbl>      <dbl>\n#> 1 103   longhand     35.7       66.7\n#> 2 122   longhand     40.5       50  \n#> 3 142   longhand     33.3       66.7\n#> 4 152   longhand     64.3       66.7\n#> 5 172   longhand     71.4       66.7\n#> 6 183   longhand     26.2       50"},{"path":"case-study-laptop.html","id":"descriptive-statistics","chapter":"6 Case Study: More Results from Note Taking Studies","heading":"6.3.2 Descriptive Statistics","text":"performing inferential statistical analysis data, obtain descriptive statistics. provides overview data. addition, descriptive analysis another way checking data minimising chances errors problems.first, want calculate number participants per condition. can use tidyverse pipeline group_by() count():another data check, can compare numbers values reported OSF data (number participants condition reported original paper). numbers match, increases confidence data preparation.next descriptive statistic, calculate condition means DV interest, conceptual memory scores. also calculate standard deviation get idea spread data. use piping tidyverse get result. time final function pipe summarise() allows calculate summary statistics.shows conceptual memory 10 points higher longhand compared laptop condition. also see difference around 5 points SD.side note, added another calculation summarise() call. example, n = n() also calculated number participants per condition, previous code .One important part descriptive analysis always graphical data exploration. plot data points usually best way see something wrong data. , used afex_plot() estimated model aov_car(), can also invoke ggplot2 directly.build plot ggplot2, pipe data ggplot() build figure layer layer. important part mapping variables data aesthetics aes() function. call directly ggplot() call mimic figures seen far, mapping condition \\(x\\)-axis DV, conceptual memory scores, \\(y\\)-axis. Figure 5.1, begin violin plot (geom_violin()) different quantiles. violin plot shows shape distribution. combine individual data points, show using geom_beeswarm() ggbeeswarm package (call function without loading package beforehand using package::function()). add mean (standard error, explained later) using stat_summary() red. plot see data already spans full range \\(y\\)-axis, need use coord_cartesian(ylim = c(0, 100)).\nFigure 6.2: Conceptual memory scores Mueller Oppenheimer (2014). plot combines individual data points black means red.\nlooking figure, see got status message console, “summary function supplied, defaulting `mean_se()`.” message always shown using stat_summary() without additional argument can safely ignored (.e., just indicates red point shows mean red error bars show standard error).plot show anything unusual. plot just reinforces previous descriptive results: mean memory score, also three displayed quantiles, larger longhand laptop condition. Taken together, descriptive analysis suggests nothing preventing us running inferential analysis might difference conceptual memory scores might larger longhand condition.","code":"\nmo2014_e2a %>% \n  group_by(condition) %>% \n  count()\n#> # A tibble: 2 x 2\n#> # Groups:   condition [2]\n#>   condition     n\n#>   <fct>     <int>\n#> 1 laptop       51\n#> 2 longhand     48\nmo2014_e2a %>% \n  group_by(condition) %>% \n  summarise(\n    mean = mean(conceptual),\n    sd = sd(conceptual)\n  )\n#> # A tibble: 2 x 3\n#>   condition  mean    sd\n#>   <fct>     <dbl> <dbl>\n#> 1 laptop     35    23.5\n#> 2 longhand   46.5  28.9\nmo2014_e2a %>% \n  ggplot(aes(x = condition, y = conceptual)) +\n  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +\n  ggbeeswarm::geom_beeswarm() +\n  stat_summary(colour = \"red\")  \n#> No summary function supplied, defaulting to `mean_se()`"},{"path":"case-study-laptop.html","id":"inferential-analysis","chapter":"6 Case Study: More Results from Note Taking Studies","heading":"6.3.3 Inferential Analysis","text":"inferential analysis conceptual scores uses exactly call previous analysis, new data set, mo2014_e2a.Looking ANOVA table results shows \\(p\\)-value smaller .05; analysis reveals significant effect condition. indicates data provides evidence null hypothesis difference note taking conditions. Consequently, justified saying data provides evidence difference.make easy detect significant result, afex – like statistical software tools – indicates significant effect \\(p < .05\\) one * next \\(F\\)-value. case significant result \\(p < .01\\), indicted two **. case significant results \\(p < .001\\), indicated three ***. case effect significant, \\(p < .1\\), + shown position (however, indicate data almost significant).can use afex_plot() way produce results figure based estimated model, individual data points shown grey background.\nFigure 6.3: afex_plot() figure conceptual memory scores Mueller Oppenheimer (2014, Experiment 2) show significant difference two note taking conditions.\ncan also use emmeans() obtain estimated marginal means:Likewise, can use emmeans() combination `pairs() obtain estimated difference two conditions:report study research paper, following way:Figure 6.3 shows conceptual memory scores Mueller Oppenheimer (2014) suggests conceptual memory somewhat better longhand compared laptop note taking condition. line visual impression, ANOVA conceptual memory scores indicated significant effect note taking condition, \\(F(1, 97) = 4.77\\), \\(p = .031\\). mean scores longhand condition (mean = 46.5, SD = 28.9) significantly larger mean scores longhand condition (mean = 35.0, SD = 23.5), difference 11.5 (SE = 5.3). suggests type note taking lectures, either laptop loghand format, might effect conceptual memory content lectures.","code":"\nmc_mo <- aov_car(conceptual ~ condition + Error(pid), mo2014_e2a)\n#> Contrasts set to contr.sum for the following variables: condition\nmc_mo\n#> Anova Table (Type 3 tests)\n#> \n#> Response: conceptual\n#>      Effect    df    MSE      F  ges p.value\n#> 1 condition 1, 97 688.89 4.77 * .047    .031\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\nafex_plot(mc_mo, \"condition\")\nemmeans(mc_mo, \"condition\")\n#>  condition emmean   SE df lower.CL upper.CL\n#>  laptop      35.0 3.68 97     27.7     42.3\n#>  longhand    46.5 3.79 97     39.0     54.0\n#> \n#> Confidence level used: 0.95\nemmeans(mc_mo, \"condition\") %>% \n  pairs()\n#>  contrast          estimate   SE df t.ratio p.value\n#>  laptop - longhand    -11.5 5.28 97  -2.184  0.0314"},{"path":"case-study-laptop.html","id":"conceptual-memory-scores-from-mueller-and-oppenheimer-2014-experiment-1","chapter":"6 Case Study: More Results from Note Taking Studies","heading":"6.4 Conceptual Memory Scores from Mueller and Oppenheimer (2014) Experiment 1","text":"discussed , two data sets looked far direct replications Mueller Oppenheimer (2014)’s Experiment 1, data set yet discussed. Consequently, now re-analysing data set, Mueller Oppenheimer (2014)’s Experiment 1. data also available OSF.following code snippet prepares data using strategy Experiment 2 shown . party preparation, code excludes one participant also excluded Mueller Oppenheimer (2014). code ends number participants reported Mueller Oppenheimer (2014) (see ).can run analysis data sets:results show significant effect note taking condition, \\(F(1, 63) = 2.46\\), \\(p = .122\\). somewhat surprising result, Mueller Oppenheimer (2014) report significant effect \\(p = .046\\). reason difference used different statistical analysis ANOVA reported , mixed-effects model (introduction, see Singmann Kellen 2019).Whether analysis appropriate ANOVA analysis performed , something can discuss detail. Mixed models advanced statistical procedure appropriate specification always straight forward. case, fact sensible analysis shows clearly non-significant difference suggests results Experiment 1 clear cut. words, situation two seemingly sensible analysis strategies lead different qualitative results. indicates evidence difference population strong.plot data using afex_plot() visual impression line statistical one. Even though observed difference direction Experiment 1, difference appears smaller. can also see three participants longhand conditions overall lowest conceptual memory scores, even though mean longhand condition larger mean laptop condition.\nFigure 6.4: afex_plot() figure conceptual memory scores Mueller Oppenheimer (2014, Experiment 1). ANOVA shows significant difference two note taking conditions.\ncan also use combination emmeans() pairs() introduced get estimated difference. shows memory benefit longhand note taking experiment bit less 9 points, rounded 9.0 points.Overall, results Mueller Oppenheimer (2014)’s Experiment 1 look similar also different results Experiment 2. results similar observed conceptual memory benefit longhand condition 9.0 points Experiment 1 11.5 points Experiment 2. results different effect significant Experiment 2, Experiment 1.One possible reason difference inferential outcome (.e., significant versus significant) difference sample size. condition Experiment 1 consisted around 30 participants, whereas condition Experiment 2 consisted around 50 participants. One general rule NHST fixed observed effect, study larger sample size lower \\(p\\)-value. reason behaviour NHST something makes intuitive sense, larger sample size representative observed effect becomes effect population. limit observed participants population (.e., sample = population), effect sample identical effect population.","code":"\nmo2014_e1 <- read_csv(\"data/Study 1 abbreviated data.csv\")\n#> Rows: 66 Columns: 22\n#> -- Column specification ------------------------------------\n#> Delimiter: \",\"\n#> dbl (22): participant, LapLong, whichtalk, threeGR, Wcou...\n#> \n#> i Use `spec()` to retrieve the full column specification for this data.\n#> i Specify the column types or set `show_col_types = FALSE` to quiet this message.\nmo2014_e1 <- mo2014_e1 %>% \n  filter(participant != 63) %>% \n  mutate(\n    pid = factor(participant),\n    condition = factor(LapLong, \n                       levels = c(0, 1),\n                       labels = c(\"laptop\", \"longhand\"))\n  ) %>% \n    mutate(\n    factual = factualindex / perfectfactindexscore * 100,\n    conceptual = conceptualindex / perfectconceptindexscore * 100\n  ) %>% \n  select(pid, condition, factual, conceptual)\nglimpse(mo2014_e1)\n#> Rows: 65\n#> Columns: 4\n#> $ pid        <fct> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, ~\n#> $ condition  <fct> longhand, longhand, longhand, laptop, l~\n#> $ factual    <dbl> 50.00000, 52.38095, 92.85714, 73.80952,~\n#> $ conceptual <dbl> 50.00000, 50.00000, 66.66667, 83.33333,~\nmo2014_e1 %>% \n  count(condition)\n#> # A tibble: 2 x 2\n#>   condition     n\n#>   <fct>     <int>\n#> 1 laptop       31\n#> 2 longhand     34\nmc_mo1 <- aov_car(conceptual ~ condition + Error(pid), mo2014_e1)\n#> Contrasts set to contr.sum for the following variables: condition\nmc_mo1\n#> Anova Table (Type 3 tests)\n#> \n#> Response: conceptual\n#>      Effect    df    MSE    F  ges p.value\n#> 1 condition 1, 63 530.96 2.46 .038    .122\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\nafex_plot(mc_mo1, \"condition\")\nemmeans(mc_mo1, \"condition\") %>% \n  pairs()\n#>  contrast          estimate   SE df t.ratio p.value\n#>  laptop - longhand    -8.97 5.72 63  -1.567  0.1220"},{"path":"case-study-laptop.html","id":"overview-of-experiments-investigating-longhand-versus-laptop-notetaking-during-lectures","chapter":"6 Case Study: More Results from Note Taking Studies","heading":"6.5 Overview of Experiments Investigating Longhand versus Laptop Notetaking During Lectures","text":"Let’s try sum learned different types note taking memory performance. previous chapter (Chapter 5, seen overall memory performance study Urry et al. (2021) slightly non-significantly higher taking notes laptop compared longhand format.chapter, concerned conceptual memory scores , one part overall memory scores. reason focussing conceptual memory scores based Mueller Oppenheimer (2014). argued “[l]aptop use facilitates verbatim transcription lecture content students can type significantly faster can write” (p. 1160). Furthermore, “[p]revious studies shown detriments due verbatim note taking prominent conceptual factual items” (p. 1160). Consequently, expected note taking laptop lead reduced conceptual memory compared note taking longhand format.looking conceptual memory scores considering experimental design introduced , data three basically identical studies: Experiments 1 2 Mueller Oppenheimer (2014) experiment Urry et al. (2021).shown , data Urry et al. (2021) showed slight non-significant memory advantage laptop compared longhand condition (difference 3.5 points). Experiment 1 2 Mueller Oppenheimer (2014) show opposite results pattern, benefit longhand compared laptop note taking around 10 points (Experiment 1 = 9.0 points, Experiment 2 = 11.5 points). reanalysis observed effects quite similar across two experiments Mueller Oppenheimer (2014), significant Experiment 2 Experiment 1.consider results three experiments together, limited evidence overall effect type note taking conceptual memory. Whereas Mueller Oppenheimer (2014) clearly suggest note taking longhand format better, Urry et al. (2021) descriptively observe opposite pattern roughly number participants (total N = 142) Experiments 1 2 Mueller Oppenheimer (2014) combined (total N = 164).mean research question type note taking matters conceptual memory? three different situations responsible inconsistent results. different situations different consequence research.first possibility effect type note taking conceptual memory population. situation, study Urry et al. (2021) produced false negative result; non-significant effect sample reality effect population – , reality null hypothesis false. NHST results always probabilistic, false negative result can always happen due random chance (.e., researchers can just get unlucky). possible reasons possibility () error data collection data handling (e.g., condition codes incorrectly assigned participants) (b) sort experimenter demand effect. experimenter demand effect can happen person performing data collection knows hypothesis condition participant influences participant subtly specific direction. influence can happen unknown experimenter participant. example, students performing data collection believe hypothesis type note taking effect acted way made task difficult participants longhand condition.second possibility effect type note taking conceptual memory population. situation, study Mueller Oppenheimer (2014) produced false positive results; significant effect sample reality effect population – , reality null hypothesis true. , outcome can occur purely due chance. However, studies row show significant effect, less likely chance outcome least theory (reanalysis results Experiment 1 changed significant non-significant, strong argument ). possible reasons possibility , error studies experimenter demand effects. example, quite likely researchers performing data collection Mueller Oppenheimer (2014) believed hypothesis note taking laptop leads worse memory note taking longhand format. Therefore, might acted way made task difficult participants laptop condition.third possibility neither results false positive false negative, correctly reflect underlying population. situation, two different populations two different papers sampled participants. example, data collection Mueller Oppenheimer (2014) must taken place 2013 earlier. conceivable participants time still used taking lecture notes longhand format thus might performed better format used . contrast, data collection Urry et al. (2021) took place spring 2017 students time might used note taking laptop thus removing potential benefit note taking longhand format. words, whereas might difference population earlier difference might disappeared now people use laptops everyday now.58In absence additional data studies, difficult decide three possibilities. Many researchers seem put weight first study published study using specific experimental task following replication studies. mindset, still believe effect Mueller Oppenheimer (2014) “real” whereas results Urry et al. (2021) must fluke. However, difficult see logical argument position. extreme position held researchers failed replication study, means must something wrong original study. , position seem logically justifiable. Whereas failed replication clearly weakens claims made original study, mean anything necessarily wrong .deciding believe, let’s consider detail consequences first possibility described actually true. reminder, possibility Urry et al. (2021)’s results false negative result reality difference population two note taking conditions conceptual memory scores. outcome mean research question? , need circle back two epistemic gaps discussed Section 1.3. knew sure effect note taking population second epistemic gap apply . However, first epistemic gap still apply.knew benefit note taking longhand compared laptop format experimental task developed Mueller Oppenheimer (2014), mean benefit always occurs. example, experiment used rather short video lectures around 15 minutes length. benefit also occur longer lectures? Likewise, effect appear relatively well educated participants original study really apply whole population students? Another potential issue long memory benefit holds. experiments considered , memory task followed around thirty minutes lecture. Mueller Oppenheimer (2014) addressed question Experiment 3, testing took place roughly one week virtual still found longhand memory benefit. However, result yet replicated.list open questions shows even best case scenario knew failed replication problem, implications statistical result research question still limited. discussed detail Section 1.3.1, largest problem strong inference statistical result underdetermination theory data. find specific pattern one task, pattern generalises instances hope research question applies. important – perhaps difficult – task us researchers, fall trap interpret significant \\(p\\)-value means research question answered. one piece evidence research process.sum, results suggests benefit particular type note taking conceptual memory, easy detect. available data suggest everyone ditch laptop paper pencil. results also used justify prohibiting students using laptops lectures. researchers feel important research question real-life issue, need provide stronger evidence.","code":""},{"path":"case-study-laptop.html","id":"summary-5","chapter":"6 Case Study: More Results from Note Taking Studies","heading":"6.6 Summary","text":"chapter, discussed three things. Firstly, used tidyverse prepared data two Experiments Mueller Oppenheimer (2014) analysis. seen data preparation often requires necessary tidyverse skills, also knowledge data set. part data preparation, discussed importance performing coherence checks ensure data matches assumptions. assured data okay go ahead statistical analysis.Secondly, used afex::aov_car() re-analysed three data sets using NHST. three data sets shared structure, one continuous dependent variable one categorical independent variable two levels, therefore use ANOVA analysis. part analysis also created results figures using afex_plot() performed follow-analyses using emmeans(). part analysis seen data set correct format, estimating statistical model obtaining \\(p\\)-value relatively easy straight forward.Thirdly, discussed learn three different experiments research question different types note taking lectures affect conceptual memory lecture content. seen despite results Mueller Oppenheimer (2014) suggests note taking longhand format provides memory benefit compared note taking laptop, conclusion clear. One problem replication Urry et al. (2021) questions whether original pattern genuine effect holds population. problem even results Mueller Oppenheimer (2014) reliable replicate, provide conclusive answer general research question. find something one particular task, mean holds underlying research question. reality research, epistemic problems discussed Chapter 1 ignored.overarching goal chapter paint realistic picture can learn significant result. One reality research practice significant results generally researchers looking . results significant happy – experiment “worked” can publish . result significant unhappy, generally problems publishing results. Whereas might reality research practice, significant \\(p\\)-value actually end goal. “mistake think statistical inference scientific inference.” (Rothman, Gallacher, Hatch 2013, 1013). words, even find significant \\(p\\)-value can learn particular study always limited. Likewise, use established task study established phenomenon, finding non-significant result can also major interest.focus whether \\(p\\)-value significant created situation many domains published literature paints picture easy obtain particular result. However, reality often case, simply researcher publish failed attempts. sum, please confuse \\(p\\)-value answer research question.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
