[{"path":"index.html","id":"overview","chapter":"Overview","heading":"Overview","text":"point time, book written chapters already available.Chapter 1 provides introduction role statistics research process. also answers important question: need statistics?Chapter 2 provides overview important concepts correct terminology need describe research designs (e.g., variable? difference dependent independent variables?). also introduces distinction experimental observational studies.Chapter 5 introduces basic statistical approach.","code":""},{"path":"index.html","id":"acknowledgments","chapter":"Overview","heading":"0.1 Acknowledgments","text":"project exist without help feedback provided others: David Kellen, Lukasz Walasek, Stuart Rosen, Anna Krason","code":""},{"path":"index.html","id":"license-and-attribution","chapter":"Overview","heading":"0.2 License and Attribution","text":"book licensed Attribution-NonCommercial 4.0 International (CC -NC 4.0) license. license allows share adapt work long give appropriate attribution use materials commercial purposes.Parts book uses materials released compatible CC license. , source clearly indicated. Please ensure attribute original source case re-use materials.","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"role-of-statistics-in-the-research-process","chapter":"1 Role of Statistics in the Research Process","heading":"1 Role of Statistics in the Research Process","text":"book concerned experimental psychology, particular statistical analysis experiments psychology related disciplines language science, behavioural science, cognitive science, neuroscience. order expressed previous sentence – science first statistical analysis second – one overarching principles think statistics. Whereas goal introduce concepts techniques required perform statistical analysis (mostly experimental) data, perspective taken statistical analysis can performed understood within scientific context takes place .One consequence perspective start point statistical analysis needs specific clear research question. case data collection analysis guided research question, statistical analysis generally indispensable part research process. describe detail coming chapters, statistics tool allows us draw inferences go beyond data observed. ability generalise allows us connect experimental results research questions underlying theories. sum, statistical techniques introduced book can provide meaningful scientifically helpful answers data collected analysed clear research question mind.","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"the-research-process","chapter":"1 Role of Statistics in the Research Process","heading":"1.1 The Research Process","text":"One way describe research process psychology related sciences terms four interrelated steps: research question, operationalisation research question data collection, statistical analysis data, finally communication results. Let us explain steps detail.research begin research question. disciplines considered often theory hypothesis human behaviour human mind. example, widely accepted idea decision making behavioural science people exhibit loss aversion – displeasure resulting losing £10 stronger pleasure derived winning £10. look example research question, whether evidence idea loss aversion, detail chapter. example already shows something common research questions – contains general statement involves directly observable quantities (pleasure displeasure).research questions general question whether loss aversion. fact, many research questions lot specific applied. example, many applied domains important question whether specific intervention – new therapeutical procedure workplace training – better intervention already existing one.Sometimes, researcher just wants find answer specific issue come previous study. concrete example, consider Hinze Wiley (2011) addresses specific question concerning testing effect. testing effect describes now well established phenomenon testing newly learned materials (e.g., self-tests quizzes) leads better memory additional studying (e.g., re-reading) (Roediger Karpicke 2006). Hinze Wiley (2011) noticed time research testing effect used limited ways implementing testing, multiple choice tests open-ended questions. see whether phenomenon general, research question whether testing effect also occurred another type testing, fill--blank tests (.e., sentences learned materials shown words missing needed filled ). results showed whereas fill--blank tests effective mere re-reading (.e., showed testing effect), effective testing types require involved processing materials (e.g., open ended questions).next step research process transformation research question empirical statistical hypothesis. call step operationalisation. , instead talking abstract ideas research questions, need concretely decide study run. need find tasks measures (e.g., questionnaires) allow us collect data addresses research question. important part operationalisation specification relevant variables. can understand variables dimensions, features, characteristics individuals situations can differ. example, relevant variables loss aversion magnitude potential loss gain intensity resulting pleasure displeasure (e.g., measured questionnaire). testing effect, relevant variables type additional learning (e.g. re-reading versus multiple choice testing) final memory performance.can see example testing effect, sometimes appears difficult separate research question operationalisation. case, research question tied specific aspect operationalisation (.e., testing implemented). However, even concrete research question always aspects study require operationalisation (e.g., measure learning?). example loss aversion shows, research questions general. Consequently, multitude possible studies can performed investigate one research question. However, specific study researcher needs decide one specific study design. exactly tasks measures using investigate question interested ?traditional view research process – known hypothetico-deductive method – operationalisation step, researchers derive empirical prediction tests theory. words, theory coupled operationalisation predicts specific outcome (empirical prediction), needs occur theory true. example, discuss detail , loss aversion predicts people unwilling gamble money chance losing specific amount money equal chance winning amount money (losing amount hurts winning amount). According traditional view, predicted outcome occur, learn theory false. Furthermore, predicted outcome occur, entail theory necessarily true, provide support theory. However, see (Section 1.3.1), reality generally learn less prescribed idealised form hypothetico-deductive method.Research practice often diverges idealised view research process (e.g., Haig 2014). cases one clear prediction specific theory tested. Researchers might compare multiple theories diverging predictions, vague hypothesis instead fully fledged theory single prediction follows, might simply curious happens specific situation. example, Hinze Wiley (2011), research question fill--blank test show testing effect testing procedures. None possible results, even complete absence testing effect type testing, provided evidence testing effects general. goal research confirm disconfirm testing effect. Instead, goal test generality testing effect. Consequently, seem appropriate say operationalisation always involves making specific empirical predictions. However, even absence specific empirical prediction, operationalisation must result empirical hypothesis relating two variables part research design research question.1 final step operationalisation data collection.research question operationalised corresponding data collected, time statistical analysis. Generally, statistical analysis answers one specific question: data provide evidence empirical hypothesis derived research question? remainder book show detail perform statistical analyses common study designs interpret results light research question operationalisation.data sufficiently analysed, reached final step research process, need communicate results (step also known dissemination). different forms dissemination depending goal audience (e.g., scientific journal article, dissertation report, conference presentation, press release). Whereas different forms differ amount detail background provided, need provide truthful comprehensive account whole research process: research question? investigated (.e., describe operationalisation)? results? mean research question? Often, difficult problem solve step provide comprehensive truthful account succinct manner. One important tool graphical means – pictures results. Consequently, book discuss present statistical results text create appropriate graphs.Whereas abstract overview leaves important things also part research – research questions come – shows three important things.primacy research question. research question determines operationalisation thus data collected. research question also determines statistical analysis, indirectly; research question determines empirical hypothesis tested analysis.primacy research question. research question determines operationalisation thus data collected. research question also determines statistical analysis, indirectly; research question determines empirical hypothesis tested analysis.statistical analysis directly connected research question. statistical analysis performed operationalisation research question, research question . means statistical analysis directly inform us research question. words, statistically test research question. Instead, statistics can tell us something specific operationalisation. Whether allows strong inferences research question depends operationalisation. shown following example, important part scientific discourse argue whether certain operationalisations allow one address specific research questions. However, generally statistical question.statistical analysis directly connected research question. statistical analysis performed operationalisation research question, research question . means statistical analysis directly inform us research question. words, statistically test research question. Instead, statistics can tell us something specific operationalisation. Whether allows strong inferences research question depends operationalisation. shown following example, important part scientific discourse argue whether certain operationalisations allow one address specific research questions. However, generally statistical question.Statistics end goal research. Instead, end goal usually written communication research. cases, statistical analysis indispensable part communication can provide evidence specific empirical hypothesis. However, understand full meaning implications particular statistical result, important know context – research question operationalisation. task research communicate context communicating research. Without context, impact meaning statistical result severely limited.Statistics end goal research. Instead, end goal usually written communication research. cases, statistical analysis indispensable part communication can provide evidence specific empirical hypothesis. However, understand full meaning implications particular statistical result, important know context – research question operationalisation. task research communicate context communicating research. Without context, impact meaning statistical result severely limited.","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"example-the-psychology-of-loss-aversion","chapter":"1 Role of Statistics in the Research Process","heading":"1.2 Example: The Psychology of Loss Aversion","text":"get better understanding research process problems can arise , let us consider detail concrete example research question can investigated empirically. Specifically, let us return example loss aversion. Loss aversion one assumptions underlying prospect theory (Kahneman Tversky 1979), mathematically formalised theory combining cognitive psychology economic theory.2 concise description loss aversion “losses loom larger gains” (Kahneman Tversky 1979, 279). described , loss aversion means negative feeling associated loss certain amount money larger positive feeling associated gain amount. example, loss aversion predicts displeasure pain losing £10 larger pleasure joy winning £10.can see loss aversion theoretical statement involving latent - , unobservable - quantities negative positive feelings (.e., displeasure versus pleasure). can ask people feel, easily observe feelings without asking. psychology, generally call unobservable theoretical concepts constructs. can test whether people indeed show loss aversion directly observe constructs form core ?One possibility testing hypothesis individuals show loss aversion hinted . either give people certain amount, say £10, take away , ask feel. procedure runs least two problems. First, clearly ethically unacceptable perform experiment consists taking £10 away participants. Second, even overcome ethical problems (e.g., first giving participants endowment taking money away endowment) still problem measure feelings associated two events.One way avoid ethical problem taking away money participants ask imagine feel lost gained certain amount money. Even though certain whether imagined feeling corresponds actual feeling participants actually losing gaining amount, procedure commonly used. example, McGraw et al. (2010) asked participants imagine play single game 50% chance losing $200 50% chance winning $200 (e.g., flipping coin comes heads win $200; otherwise lose $200). asked participants imagine feel either two outcomes. identified two different ways ask question, shown Figure 1.1 (McGraw et al. 2010). first possibility, bipolar scale, shown upper part. response scale participants loss condition respond left side scale participants gain condition respond right side scale. compare ratings, measured participants’ responses absolute distance rating neutral point “Effect” (.e., “Small Positive Effect” treated intensity “Small Negative Effect”). second possibility shown lower part Figure 1.1 shows unipolar intensity scale. scale, participants conditions provide response scale rate intensity displeasure pleasure.\nFigure 1.1: Example two different scales measuring feelings potential gain loss. upper part shows bipolar scale losses receive rating left side (“Effect”) gains receive rating right side. lower part shows unipolar scale intensity losses gains given scale. Image adapted McGraw et al. (2010).\nreading , take moment ask believe two scales shown Figure 1.1 make difference whether participants show loss aversion. put differently, can think reason matters ask participants’ feelings loss gain? difference, scale expect likely loss aversion occurs?investigate question whether loss aversion scales, McGraw et al. (2010) asked half participants use bipolar scale half use unipolar scale rate feelings imagined loss gain $200. data hand, compared feeling ratings imagined losses wins two conditions. results showed indeed matters scale used. bipolar scale, evidence loss aversion. feeling ratings gains losses approximately equal around 3.4 (1 = Effect 5 = Large Effect). However, using unipolar scale, found evidence loss aversion. loss condition, participants reported stronger feeling average (around 3.6) compared gain condition (around 3.1).McGraw et al. (2010) explain results terms relative versus absolute feeling judgements. bipolar scale, people first judge valence feeling – , whether good bad – determine side provide response. done, judge intensity feeling. However, intensity judgement follows valence judgement, make intensity judgement comparing feelings valence. specifically, McGraw et al. (2010) assume loss condition, loss compared negative events. Similarly, gain condition, gain compared positive events. words, bipolar scale, participants make relative judgement intensity compared loss gains. Consequently, argue results bipolar scale helpful answering question whether loss aversion.3For unipolar scale, McGraw et al. (2010) argue judgement feeling intensity preceded valence judgement. Therefore, people use absolute judgement feeling, comparing negative positive feelings. Consequently, data, shows pattern line loss aversion, helpful question whether loss aversion. Overall conclude study provides evidence loss aversion, appears using unipolar scale.results McGraw et al. (2010) show seemingly minor differences operationalisation research question can tremendous effect results. line , perhaps surprising operationalisation investigating loss aversion – asking participants feelings – common. discuss alternative operationalisations.whereas McGraw et al. (2010) provide explanation results, explanation might difficult come seen data. inability able predict effect response scale results , potential consequences reaching. take results seriously generalise domains, might conclude whenever interested participants’ feelings matters whether use bipolar unipolar scales. One even go another step say whenever use subjective rating scales, type scale unintended effect results. , often good idea try find operationalisation research questions involve subjective rating scales, types responses choices, response times, complex behaviour.","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"evidence-for-loss-aversion-lotteries","chapter":"1 Role of Statistics in the Research Process","heading":"1.2.1 Evidence for Loss Aversion: Lotteries","text":"different operationalisation testing loss aversion compare choices across different risky choices, lotteries, gambles (terms can understood interchangeably ), common experimental paradigm decision making behavioural economics. lottery context consists different options, associated one multiple outcomes, participant choose one.simplest type lotteries ones consisting one option. case, participants can decide whether accept reject lottery. example, evidence loss aversion can found lotteries used Battalio, Kagel, Jiranyakul (1990). One lotteries (Question 15) :play following gamble:\n50% chance losing $20 50% chance winning $20.Participants decide whether accept reject lottery.4 43% participants accepted lottery.result provide evidence loss aversion? evidence comes fact participants likely reject lottery accept (.e., acceptance rate 50%). important lottery symmetric 50-50 lottery. , magnitude potential loss equal magnitude potential gain possible outcomes appear equal probability 50%.5 Remember loss aversion means disliking loss liking gain magnitude. symmetric lottery exactly situation. fact participants likely decline participate symmetric lottery therefore much line loss aversion.One problem results Battalio, Kagel, Jiranyakul (1990) collected data 35 participants provide statistically compelling evidence loss aversion. Whereas data shows descriptive pattern line loss aversion (.e., 50% acceptance symmetric lotteries) supported statistical analysis. specifically, statistical analysis provide support empirical prediction observed acceptance rate 50%.[] descriptively look like , evidence data enough surpass statistical criterion use judging evidence. fuller description set statistical criterion come later.compelling evidence loss aversion comes study Brooks Zank (2005). task, participants asked make decision complex lotteries participants decide two options. example, one lotteries following:option prefer?: 25% chance +£11, 50% chance £0, 25% chance -£11.B: 25% chance +£10, 50% chance £0, 25% chance -£10.see two options, two symmetric outcomes magnitude. options B, 25% chance loss 25% chance gain magnitude (50% probability participants neither lose gain anything). difference B , magnitude potential loss gain larger £1.notion loss aversion makes interesting prediction case. Participants increasingly dislike symmetric lottery larger potential outcomes . lottery means dislike losing £10 liking gaining £10, difference dislike liking larger potential outcomes £11. terms empirical prediction means participants willing choose option B option lottery .study Brooks Zank (2005), 49 participants worked around 50 lotteries similar structure lottery . , two options loss gain magnitude probability (third potential outcome always smaller magnitude loss/gain). example lottery , difference options always loss/gain magnitude one option £1 larger option. line empirical prediction loss aversion, participants chose option smaller magnitude loss/gain 63% cases. given larger sample size study, statistical analysis also supported prediction 63% larger 50%.show thing loss aversion? Results Brooks Zank (2005) certainly appear support theoretical idea (see also Camerer 2005). also makes intuitive sense. people (included) feel symmetric lotteries really attractive become increasingly unattractive increasing magnitude (really thinks flipping coin chance losing winning say £100,000 sounds like good idea?). However, previous example, evidence loss aversion discussed hinges particular operationalisation theoretical idea. results, really learned magnitude loss gain psychologically relevant factor minds people. thing learned people dislike certain lotteries options lotteries. Can find alternative data pattern involve loss aversion?","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"alternative-explanation-loss-aversion-or-loss-seeking","chapter":"1 Role of Statistics in the Research Process","heading":"1.2.2 Alternative Explanation: Loss Aversion or Loss Seeking?","text":"One clever alternative explanation looks like loss aversion provided Walasek Stewart (2015). study, participants also presented 50-50 lotteries. example one lotteries shown Figure 1.2 . shown figure, lottery involved mixed outcomes, gains losses (.e., -$18 +$20 example). Furthermore, lottery consisted one option, participants choose whether accept play lottery . Finally, possible outcomes always 50% probability occurring. make logic clearer participants, told accepting lottery shown Figure 1.2 equal flipping coin -$18 one side +$20 side. Depending outcome came top, money change accordingly. participants rejected lottery, neither lost won money.\nFigure 1.2: Screenshot lottery task used investigate loss aversion. screenshot Walasek Stewart (2015, Figure 1).\ndescribing study results detail, let first lay gist argument Walasek Stewart (2015). Among lotteries participants saw study symmetric lotteries potential loss equal potential gain. example, -$12/+$12 lottery. participants accepted lotteries average neither win lose money.clever manipulation Walasek Stewart (2015) across different conditions (.e., groups participants) manipulated attractive symmetric lotteries , relative lotteries participant saw. one condition, many lotteries potential loss smaller potential gain, -$12/+$20 lottery. , participant condition accepted lotteries, likely win money. condition symmetric lotteries therefore relatively unattractive. another condition, many lotteries potential loss larger potential gain, -$20/+$12 lottery. , participant condition accepted lotteries, likely lose money. condition symmetric lotteries therefore relatively attractive.According original idea loss aversion, thing matters lottery absolute magnitude potential gains losses manipulation relative attractiveness symmetric lotteries effect whatsoever. However, results Walasek Stewart (2015) showed manipulation mattered. condition symmetric lotteries relatively unattractive, participants accepted 21% cases, condition lotteries relatively attractive, participants accepted 71% cases. statistical analysis supported empirical prediction acceptance rates symmetric lotteries differed across conditions. results pattern contradiction original idea loss aversion.next section present full details study Walasek Stewart (2015), coming back means loss aversion fits within general theme chapter. decision making research behavioural economics main interest, details may super relevant , okay understand perfectly. However, use study example next chapters great least give cursory read.","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"full-details-of-walasek-and-stewart-2015","chapter":"1 Role of Statistics in the Research Process","heading":"1.2.2.1 Full Details of Walasek and Stewart (2015)","text":"many experiments fall within cognitive domain, study Walasek Stewart (2015) consisted series similar trials participants task (.e., accept reject shown lottery). differed across trials values two possible outcomes. example, one conditions experiment losses ranged -$6 -$20 increments -$2 (resulting 8 different possible losses) gains ranged $12 $40 increments $4 (resulting 8 different possible gains). Across trials one participant condition, possible losses combined possible gains total participants decide \\(8 \\times 8 = 64\\) lotteries whether accepted rejected .Within 64 trials subset trials allowed researchers directly address question whether evidence loss aversion.6 Whereas lotteries shown participants asymmetric – , potential loss differed numerically potential gain (example Figure 1.2) – small subset lotteries symmetric. lotteries, amounts potential loss equal amount potential gain. specifically, symmetric lotteries -$12/+$12, -$16/+$16, -$20/+$20.condition described losses ranged -$20 gains ranged +$40, 191 participants accepted symmetric lotteries 21% time. 21% descriptively 50% indicating participants indeed disliked lotteries liked (.e., overall likely reject accept symmetric lotteries). Furthermore, statistical analysis supported empirical hypothesis (.e., provided evidence pattern results generalises beyond current data). described , result makes sense light loss aversion. losing certain amount money worse winning amount money, one reject symmetric lottery one equally likely lose win certain amount money.clever manipulation Walasek Stewart (2015) included three conditions changed range possible outcomes. addition -$20/+$40 condition discussed , 202 participants $-20/+$20 condition saw lotteries losses ranging -$20 gains also ranging +$20 . Another group 190 participants, -$40/+$40 condition, saw lotteries losses ranging -$40 gains also ranging +$40. Finally, Walasek Stewart (2015) also included -$40/+$20 condition 198 participants losses ranged -$40, gains +$20 (.e., complement -$20/+$40 condition). Importantly, conditions number possible outcomes losses gains 8 (step size either \\(\\pm\\)$2 \\(\\pm\\)$4). Table 1.1 shows possible outcome conditionTable 1.1:  Possible outcomes lotteries Experiment 1 Walasek Stewart (2015). condition, participant saw 64 lotteries resulting combining possible gains possible losses condition.consequence design, changed across conditions whether symmetric lotteries relatively good relatively bad. understand , need look remaining asymmetric lotteries. -$20/+$40 condition discussed far, lotteries possible gain larger possible loss (e.g., -$18/+$20 lottery) lotteries possible loss larger gain (e.g., -$20/+$18 lottery). Consequently, symmetric lotteries relatively bad (.e., compared many lotteries possible gain larger possible loss). -$20/+$20 -$40/+$40 conditions, asymmetric lotteries balanced. half asymmetric lotteries possible gain larger possible loss, whereas half possible loss larger possible gain. Consequently, symmetric lotteries neither relatively good relatively bad. Finally, -$40/+$20 condition pattern flipped respect -$20/+$40 condition. lotteries possible gain larger possible loss compared many lotteries possible loss larger gain. Consequently, symmetric lotteries relatively good.matter whether symmetric lotteries relatively good ? Indeed . reminder, people unlikely accept symmetric lotteries -$20/+$40 condition symmetric lotteries relatively bad. Participants condition accepted 21% symmetric lotteries. -$20/+$20 condition symmetric lotteries neither relatively good relatively bad, participants accepted 50% symmetric lotteries. Similarly, -$40/+$40 condition participants accepted 45% symmetric lotteries. Finally, -$40/+$20 condition symmetric lotteries relatively good, participants accepted 71% symmetric lotteries. can see 71% descriptively 50%, indicating participants liked symmetric lotteries disliked (.e., overall likely accept reject lotteries). Furthermore, statistical analysis supported empirical hypothesis (.e., provided evidence results pattern generalises beyond current data).","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"what-do-the-results-mean-for-loss-aversion","chapter":"1 Role of Statistics in the Research Process","heading":"1.2.2.2 What do the Results Mean for Loss Aversion?","text":"results Walasek Stewart (2015) show, choice pattern lotteries always line idea loss aversion. context symmetric lottery relatively bad see evidence line idea loss aversion. context symmetric lottery relatively good, see opposite pattern one term loss seeking. mean loss aversion? original idea Kahneman Tversky (1979) matters magnitude loss gain surely line results Walasek Stewart (2015). Instead, Walasek Stewart (2015) argue relevant determine psychological impact gain loss relative magnitude rank possible outcome: Compared gains losses regularly experience, large gain large loss?moving linking example psychology loss aversion general goal book, let us answer one last question. matters rank (suggested Walasek Stewart (2015)) magnitude outcome (proposed Kahneman Tversky (1979)), see evidence effect magnitude studies investigating loss aversion manipulate context gains losses (e.g., Brooks Zank 2005)? answer question provided Stewart, Chater, Brown (2006). argue (also provide empirical evidence) daily lives experience small losses (e.g., buying something bakery) larger gains (e.g., monthly salary). consequence, gain loss magnitude, relative rank gain compared gains lower corresponding relative rank loss compared losses. difference, generally observe pattern consistent loss aversion different theoretical reason proposed Kahneman Tversky (1979).","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"epistemic-gaps-the-difference-between-what-we-want-to-know-and-what-we-can-know","chapter":"1 Role of Statistics in the Research Process","heading":"1.3 Epistemic Gaps: The Difference Between What we Want to Know and What we can Know","text":"goal chapter provide introduction role statistics research process. need statistics can tell us research questions interested ? However, far talked much statistics. Instead introduced abstract concept research process exemplified example loss aversion. example tried show even though can find statistical evidence supports empirical hypothesis, mean found answer research question. question whether loss aversion statistical question. answer research question depends whether can rule possible alternative explanations, example, rank potential loss gain important, rather magnitude. nevertheless need statistics help us determine whether data provides support particular empirical hypothesis operationalises research question.Let<U+0092>s put bluntly. yet know statistics detail, application statistical methods can appear like magical machine provides us answer research question . throw data , turn handle statistics machine, get answer research question . Sadly, image statistics (mention needing handle!) false. reason least two epistemic gaps research process prevent us getting straight answer research question. section introduce gaps ensure can get realistic image role statistics research process.explaining epistemic gap , need take step back think science general. least two different domains constitute scientific discipline. Firstly, substantive content, science . example, psychology concerned human mind behaviour. one domain psychology consists theories mind behaviour (e.g., people exhibit loss aversion). , just theories enough. problem many intuitively plausible ultimately untrue theories (e.g., idea distinct “learning styles”; see Pashler et al. 2008). second domain constituting science research; systematic investigations research questions provide us evidence. evidence allows us decide theories believe theories discard.perspective science allows us draw conclusions means us scientists. Firstly, important adopt theories easily early. scientists need natural sceptics. Instead believing theory sounds compelling, need ask evidence first. need evaluate evidence use basis degree belief. example, independent studies support certain theoretical position (case loss aversion), seems appropriate consider certain theoretical position possibility also consider alternative accounts (example, rank based account Walasek Stewart 2015). evidence weaker, say studies proponents theory, even cautious degree belief assign theory. scientific evidence overwhelming willing treat theory approximately true. psychology, many theories majority researchers agree latter criterion reached (besides maybe operant classical conditioning). consequence, many cases, best thing honest admit evidence sufficient hold strong theoretical position. us scientists, perspective also means need able evaluate evidence research area interest. requires substantive knowledge research domain, also statistical knowledge introduce book. Finally, means need open revising beliefs light new evidence (case loss aversion results Walasek Stewart 2015).last two paragraphs show, science ultimately evidence. evidence support theories field? scientists hope statistics tool helps us answer question evidence. degree , much hope. need aware epistemic gaps. Let us explain now means.Epistemology branch philosophy concerned knowledge (e.g., knowledge, know know) justification (e.g., reason believe something) (philosophical overview see: Steup Neta 2020). “Epistemic” corresponding adjective. Epistemology therefore, field deals philosophical questions core science, example, theories believe based available evidence. epistemic gap describes difference want know can actually know.7 Ideally, want know whether theory hypothesis true. already seen example , many cases even carefully designed experiments unambiguously answer question (e.g., Walasek Stewart (2015) available evidence appeared support <U+0093>loss aversion<U+0094>, now sure ). following, take abstract look question discuss two general problems complicate issue. can know often quite different like know. competent application statistics requires one aware problem avoids -interpreting results one’s research.","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"epistemic-gap-1-underdetermination-of-theory-by-data","chapter":"1 Role of Statistics in the Research Process","heading":"1.3.1 Epistemic Gap 1: Underdetermination of Theory by Data","text":"provided example theoretical (basic) research question (evidence loss aversion?). seen answering research question requires careful thinking operationalisation; exactly set study test question? decided one operationalisation, seen can find alternative explanations can explain results without making assumptions original research question. words, even though operationalisation carefully chosen, unambiguously answer research question.fact compellingly answer research questions despite employing carefully chosen operationalisation problem unique example. contrast, important insight philosophy science problem empirical study. issue also known underdetermination theory data Duhem-Quine thesis always occurs difference research question corresponding operationalisation. seen examples, essentially always .different aspects underdetermination. first specification research question operationalisation. Research questions usually involve unobservable constructs – emotions (e.g., fear), memory, attention, comprehension, learning – vague phrases, “works better” “improves.” empirical investigation questions however requires precise specification operationalisation. going research question concrete operationalisation, guarantee operationalisation captures intended meaning research question.example, consider test new therapeutic intervention compared standard one. Imagine found new intervention decreased self-reported discomfort symptoms patient questionnaire strongly old treatment, reduce number sick days due disorder. interpret mean new treatment “works better” old one? sense , another . problem nuances result operationalising research question concretely always align broad way like think research questions.point might think yet sound like big problem. just need define research questions precisely enough able learn something research question. Sadly, easier said done. first problem often impossible precisely define research question, yet found way precisely define constructs involved (known problem coordination, (Kellen et al. 2021)). cases, precisely defining abstract constructs possible anyway.example, hypothesis specific emotion, say fear, related behavioural pattern, say aggression, run problem generally agreed upon definition either constructs. probably exist questionnaires measuring fearfulness aggressive tendencies, questionnaires represent corresponding constructs definition . ask sample participants fill questionnaires found scores participants two questionnaires related, allow conclude fearfulness aggression related. conclusion allowed fearfulness measured one questionnaire related aggression measured another questionnaire. course, scientists like make general conclusion constructs related, inference logically follow.general problem run Duhem-Quine thesis: empirical hypothesis tested study two parts: theoretical prediction well set auxiliary assumptions links theoretical prediction data. stay example, theoretical prediction fear aggression related. auxiliary assumptions additional assumptions needed test question empirically decided part operationalisation: questionnaire valid measure constructs (big assumption), data collection took place without unforeseen problems, tested enough participants find effect, use appropriate statistical procedures, etc. can seen, list auxiliary assumptions somewhat limitless difficult enumerate fully. also contains quite mundane assumptions assume research actually took place just made researcher (exception, see case Diederik Stapel).core Duhem-Quine thesis empirical result pertain solely theoretical prediction interest, union (conjunction) theoretical prediction interest auxiliary assumptions. results line empirical hypothesis, supports theoretical prediction auxiliary assumptions true. Likewise, results line empirical hypothesis, provides evidence theoretical prediction auxiliary assumptions true. However, testing whether auxiliary assumptions true done study tests empirical hypothesis set test (can always come auxiliary assumptions specifically tested). Consequently, individual result provide conclusive evidence particular theoretical prediction. can always alternative explanation differs theory hypothesis one .8 meant underdetermination theory data.Whereas issue might seem like purely philosophical discussion, far . actual scientific discussions literature auxiliary assumptions part operationalisation research question. example, argument loss aversion proposed Kahneman Tversky (1979) hinges auxiliary assumption participants interpret possible outcomes lotteries terms magnitude absolute value. shown Walasek Stewart (2015), auxiliary assumption appear hold least cases participants instead interpret relative value possible outcomes lotteries. easy find similar examples research area interested .sum , problem underdetermination first epistemic gap particular result never uniquely supports challenges one theoretical position hypothesis. result appears support theory another theory makes prediction auxiliary hypothesis false thus require different theory. Likewise, result seems disagree theory, theory can always protected claiming one auxiliary assumptions incorrect. also exactly happens real scientific discourse. example, John Bargh, prominent social psychologist Yale, confronted results disagreed one prominent findings (Doyen et al. 2012) attacked (now deleted blog post still can found ) “incompetent ill-informed researchers” claimed study “many important differences procedure, worked eliminate effect.” section shown, questioning methods (.e., auxiliary assumptions) legitimate defence protects one’s theory. course, one can question auxiliary assumptions original results appeared support theory way. case Bargh, appears exactly happened. psychologists stopped believing original finding (e.g., Harris, Rohrer, Pashler 2021).","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"epistemic-gap-2-signal-versus-noise","chapter":"1 Role of Statistics in the Research Process","heading":"1.3.2 Epistemic Gap 2: Signal versus Noise","text":"first epistemic gap strong logical link theories underlying research questions operationalisation research question. Thus, terms steps research process concerns relationship step 1, research question, step 2, operationalisation data collection. next epistemic gap concerns relationship steps 2 step 3, statistical analysis.described (1.1), important task operationalisation transform research questions empirical hypothesis. Ideally, hypothesis comes form empirical prediction, describing possible outcome support theoretical hypothesis (.e., outcome predicted theory). part also clearly designate possible outcome , occur, speak theoretical hypothesis. decided , collect data run statistical analysis. goal statistical analysis provides us evidence respect empirical hypothesis. data support empirical hypothesis ?providing overview done, let us go back example , study Walasek Stewart (2015). contrast original formulation loss aversion (Kahneman Tversky 1979), based magnitude absolute value gain loss, theoretical prediction Walasek Stewart (2015) drives people’s behaviour relative value gain loss. test , presented participants lotteries different conditions range gains losses differed. one condition small losses large gains another condition large losses small gains (ignore two conditions ). hypothesis follows design relative attractiveness symmetric lotteries (magnitude possible loss = magnitude possible gain) differs conditions. small losses/large gains condition symmetric lottery relatively unattractive large losses/small gains condition relatively attractive. resulting empirical prediction participants less willing accept symmetric lotteries small losses/large gains condition large losses/small gains condition. line prediction, participants small losses/large gains condition accepted 21% symmetric lotteries whereas participants large losses/small gains condition accepted 72% symmetric lotteries.just looking bare numbers, results appear support empirical prediction. Participants roughly 50 percentage points less likely accept symmetric lotteries small loss/large gains condition large loss/small gains condition. However, can sure particular difference chance occurrence? Maybe just got unlucky participants small loss/large gains condition reason generally less likely accept lotteries participants large loss/small gains condition. Maybe former participants horrible night sleep really bad mood time testing therefore reject gambles whereas case participants latter condition. case, observed difference actually tell us anything research questions.problem described previous paragraph heart statistical approach described book. core problem responses get participants experiments inherently noisy. Human participants can things number reasons. reasons related research question operationalisation others . example, participants read lotteries carefully think provide answer, likely values possible outcome play role answer. case, responses relevant research question. participants distracted message phone read problem fully? intend accept lottery accidentally reject (.e., press wrong button)? can also imagine matters participants relatively rich relatively poor. someone million bank, might really matter lose win $16 might inherently likely gamble lottery someone hourly wage. cases, values lotteries minor effect thus responses less irrelevant research question.question whether results support empirical hypothesis, therefore like distinguish responses relevant research question – can call signal – responses generated less randomly irrelevant research question – can call noise. procedure distinguish signal noise simply see whether signal supports empirical prediction. , data provide support hypothesis , data support empirical hypothesis. Sadly, procedure exist (know people , reason research first place).absence procedure can definitely separate contribution signal noise, statistical approach introduced compares estimate signal estimate noise. Let us assume moment estimated signal supports empirical prediction example (.e., predict participants less likely accept lottery small loss/large gains condition data shows). compare estimated signal estimated noise. estimated signal large given estimated level noise, assume data supports empirical prediction. estimated signal large given estimated noise, assume data support empirical prediction.can estimate signal noise? Estimating signal straight forward. just use observed difference conditions estimate signal. example Walasek Stewart (2015), observed difference accepting symmetric lotteries two conditions (roughly 50 percentage points). Estimating noise bit complicated described detail later chapters. now enough understand affected two components: (1) variability responses within condition (2) overall sample size (.e., number participants). variability within condition becomes smaller (.e., measurement becomes precise) sample size stays , levels noise decrease. Likewise, sample size increases constant level variability, level noise decreases.Another important question counts “large” comparing estimated signal estimated noise. following chapters introduce decision threshold signal noise ratio make judgement.9 signal noise ratio threshold, act signal change beliefs. , make decision. decision threshold chosen across many decisions control rate making false positive decisions (false positive occurs say signal none). particular, decision threshold chosen across many situations signal, incorrectly assume signal 5% decisions.Taken together, statistical procedures use attempt answer question whether signal supports empirical hypothesis given human data inherently random noisy. problem estimated signal – observed difference conditions – also affected noise. never know observed difference due signal interested just based noise. overcome problem compare observed signal observed level noise. observed level large relative observed noise, decide data supports idea genuine signal present . words, never really know current data genuinely supports prediction ; just act . always chance effect due noise. one data set analysing, 100% certain estimate signal noise fully accurate. However, describe detail later, across decisions use statistical decision procedure, controls rate making false positive decisions.case first epistemic gap, second epistemic gap also shows unambiguously learn wanted know – whether data supports empirical hypothesis . signal large relative noise, evidence , evidence never fully conclusive. evidence might strong, later see can identify , always remaining doubt back head. Maybe just got unlucky participants study responded way made look like signal, wasn’t. just one data set hand, ruled .","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"summary","chapter":"1 Role of Statistics in the Research Process","heading":"1.4 Summary","text":"chapter provided conceptual overview research process psychology related disciplines. concept, research always begins research question. want know? Often research question stems particular theory want test, can also purely applied hypothesis.next important step operationalisation question followed data collection. means need find appropriate tasks measures develop study design can test empirical hypothesis following research question. seen discussion first epistemic gap, consequence separation research question operationalisation , strictly speaking, study lets us learn tasks measures using. problem underdetermination theory data, even apparently positive result allow us infer supports theoretical hypothesis. core problem empirical hypothesis combination theoretical hypothesis auxiliary assumptions rule one auxiliary assumptions false.collected data hand, next step perform statistical analysis. , hope find evidence informs us empirical hypothesis. procedure use book attempts distinguish signal data, part data relevant empirical hypothesis, noise, randomness inherent using human participants.10 However, second epistemic gap entails even statistical procedure, find fully conclusive evidence. problem estimate true amount noise data. Research participants myriad potential reasons show certain behaviour reasons need related research question. precise measures participants, can control level noise degree, ultimately sure whether just get unlucky see due noise hypothesis.final step research process communication results. step essentially combines previous steps. need communicate research question, operationalisation, data collection process, results statistical analysis. Whereas communication results ultimately goal research project, also step mindful limits research. biggest danger forget epistemic gaps inherent empirical research oversell results. course want research allows us answer (potentially big broad) research questions, honest audience stick reality primarily learning something operationalisation. get statistical result appears support empirical prediction, want treat true, clear always chance result might fluke.","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"some-further-examples-and-passing-thoughts","chapter":"1 Role of Statistics in the Research Process","heading":"1.4.1 Some Further Examples and Passing Thoughts","text":"Let us end chapter another concrete example literature highlights issues discussed ties together thoughts communicate research. motivate one’s research question, good idea start big picture. real world issues theoretical problems want address? Whereas good idea, mistake operationalisation research question big picture. Surely, use particular task – symmetric lotteries investigating loss aversion – learn something underlying research question theory? surely , question difficult answer exactly learn.final example illustrate problem, let us consider research risk preferences decision making. idea risk preferences people might willing take risks (e.g., gambling choosing investment) others. exist number different tasks investigate risk preferences experimentally, balloon analogue risk task [BART; Lejuez et al. (2002)] Columbia card task (Figner et al. 2009), well number different questionnaires. large study around 1500 participants performed eight different tasks filled twelve different questionnaires designed measure risk preferences (Pedroni et al. 2017; Frey et al. 2017) show participants’ behaviour across tasks questionnaires surprisingly unrelated. Whereas participants scored high one questionnaire also scored high questionnaires (.e., different questionnaires shared common risk trait), scores questionnaires largely unrelated behaviours different tasks. Furthermore, behaviours across different risk tasks unrelated (.e., participant specifically risky one task particularly risky another task). words, even though tasks questionnaires appear measure risk preferences, failure find consistent pattern across participants suggests fail coherent manner. One might wonder fact questionnaires related represents sort silver lining. share interpretation instead attribute common-method variance. important result questionnaires also unrelated behaviour tasks. tells point time, really understand risk preferences , measure , exist way conceptualised.sum , important keep mind thing learn something research primarily operationalisation. want make case also learn something underlying research question, make good case spell alternative explanations rule auxiliary assumptions can take granted. usually requires considering results study. short, confuse task measures theory research question.communicating statistical results, also need avoid overselling results. general principle, report results humble manner. end, avoid language suggests level confidence provide. means, statistical results never “prove” “confirm” empirical hypothesis. Instead, may “support” “suggest” certain interpretations.","code":""},{"path":"role-of-statistics-in-the-research-process.html","id":"conclusion","chapter":"1 Role of Statistics in the Research Process","heading":"1.5 Conclusion","text":"scientists, aim answer interesting important questions domain studying. problem research address research questions directly. Instead, research addresses operationalisation research question. Drawing inferences research question requires accepting number usually untested auxiliary assumptions. even data appears support hypothesis willing accept auxiliary assumptions, always risk just got unlucky interpret noise signal. consequence, individual study can provide little evidence, especially large general research questions. Even worse, sometimes learn research chosen operationalisation unable answer research question. sum, definitive answers research questions need one study.Whereas paints less optimistic picture can learn research one hope, important stay realistic humble. Many ideas, especially , appear intuitive compelling feel must right. scientists need stay sceptical avoid urge believe theories overwhelming evidence conclusively rules possible alternative explanations (even haven’t yet thought ). one thing distinguishes science non-scientific belief systems science principle based solid evidence. overall strength evidence provided research depends whole research process statistics one part.","code":""},{"path":"chapter-1-quiz.html","id":"chapter-1-quiz","chapter":"Chapter 1: Quiz","heading":"Chapter 1: Quiz","text":"Note: pull-menu selecting answer turns green selecting correct answer.Exercise 1.1  start point statistical analysis ?data set many variables allows testing many different hypothesesA clearly specified research questionThe believe hypothesis probably trueAnswer: 123Exercise 1.2  following one steps research process?Research QuestionOperationalisation data collectionStatistical analysisResponding emailsCommunication resultsAnswer: 12345Exercise 1.3  part operationalisation?Hypothetico-deductive methodSpecifying tasks questionnaires usedTransformation research question empirical statistical hypothesisDeciding variables data collectedAnswer: 1234Exercise 1.4  main reason need statistics?prove/confirm theoryTo test whether data collected provide evidence empirical hypothesisTo publish papers high-impact journalsAnswer: 123Exercise 1.5  epistemic gap?difference want know can knowA specific outcome predicted theoryA mathematically formalized cognitive theoryAnswer: 123Exercise 1.6  Underdetermination theory data suggests :Every specific hypothesis theory false eventually shown falseWe need hypotheses answer research questionsNo single data sets unambiguously supports specific hypothesis theoryAnswer: 123Exercise 1.7  can researcher assume data support empirical prediction?data descriptively supports empirical predictionWhen epistemic gapsWhen estimated signal large given estimated level noiseWhen estimated level noise smaller pre-specified thresholdAnswer: 1234Exercise 1.8  can conclude statistical analysis provides strong support empirical hypothesis?research question true probably trueOur empirical hypothesis true probably trueIt unlikely alternative explanation explain resultsNone aboveAnswer: 1234","code":""},{"path":"research-designs.html","id":"research-designs","chapter":"2 Data and Research Designs","heading":"2 Data and Research Designs","text":"previous chapter (Chapter 1) provided overview research process psychology related disciplines. goal overview show statistics just one part full research endeavour usually end goal. also highlighted answer research questions requires results statistical analysis, context result generated. specifically, argued important part research process usually operationalisation research question: measures? task participants ? study design? goal chapter provide us necessary conceptual knowledge right terminology answer questions research.","code":""},{"path":"research-designs.html","id":"evidence-data","chapter":"2 Data and Research Designs","heading":"2.1 Empirical Evidence and Data","text":"book concerned empirical research. means generally interested research questions answer proof can found purely thinking hard, mathematics philosophy. Instead, interested research question evidence comes form observations experiences, empirical evidence short. discussed previous chapter, mean theories include unobservable quantities mental states (e.g., fear, enjoyment, attention). However, theories include unobservable quantities, must causally responsible something observable (e.g., behaviour). way, can still test theories (e.g., theory predicts fear lead aggression, can induce fear without leading aggression, learn theory must wrong).fact interested empirical research means ultimate arbiter whether believe theory empirical evidence. matter elegant intuitive theory . observed behaviour people disagrees theory, wrong. also means theories vague possible empirical evidence disprove part empirical sciences (.e., empirical theories). criterion also known falsifiability introduced philosopher Karl Popper 1930s. example, never ending discussion whether Freudian psychoanalysis principle falsifiable . Whereas Karl Popper strong belief (render psychoanalysis non-scientific), proponents Freudian psychoanalysis naturally see rather differently.11Empirical evidence comes least two different forms, either anecdotes data. Whereas anecdote typically refers single person, data usually contains information multiple persons. However, anecdotes data differ dimensions just number observations, summarised following aphorism: plural anecdote data.Anecdotes unsystematic observation, typically form stories (e.g., “friend friend”), somehow address research questions. problem anecdotes generally difficult verify investigate . makes impossible rule possible alternative explanations relationship anecdote research question. seen previous chapter, one main criteria deciding whether observation provides evidence theoretical claim whether can rule plausible alternative explanations. sum, anecdotes surely matter coming good hypotheses ideas study, mature sciences anecdotes play minor evidentiary role deciding claims believe.Data systematic observations collected specific purpose, answering research question bookkeeping. Data generally consists observations multiple variables. previous chapter defined variables dimensions, features, characteristics individuals situations can differ. technical definition variable corresponds specific set possible outcomes (states affair/events), possible outcome corresponds one value variable. Furthermore, can define observation smallest unit data. technically, one observation results collecting least one value one variable values different variables one unit observation (psychology, unit observation usually participant).example data, consider study Walasek Stewart (2015) discussed previous chapter (1). task participants accept reject 50-50 lotteries (example, see Figure 1.2) participant 64 trials. Table 2.1 shows six observations two different participants study. way data shown exactly format Lukasz Walasek used analyse data (.e., variables added removed). Observations shown rows variables shown columns. tabular representation data observations rows variables columns common used throughout book.Table 2.1: Selected observations data Walasek & Stewart (2015, Exp. 1a). “[…]” indicates part whole data set observations shown.total can see six different variables data set. Let us discuss turn. first variable, subno (generally use monospace font refer variable names appear data set), participant identifier “subject number” (actual individuals take part research passive subjects, term “participant” now preferred “subject”). variable part data set uniquely identify participant (generally, unit observation) specific observation belongs. , see takes numbers. uncommon use numbers participant identifier variable, can also combination numbers letters, (ideally anonymous) name.second third variables, loss gain, specify possible outcomes lotteries trial. example, second observation/row shows lottery potential loss $6 potential gain $8. Based two columns, can see observations ordered combination loss gain. means order observations data reflect actual order trials participants saw (order random). values two variables numbers lowest possible loss/gain 6 largest possible loss/gain 40, depending condition participant .Column four shows response participant lottery, either accept reject. sixth variable, resp, numeric version response. , accept decision represented 1 reject decision 0. two variables carry information, different formats different benefits. response variable makes easy understand participants’ response (.e., clear value corresponds possible response, accept reject, study). resp variable makes easy perform calculation results given uses numerical code represent information. example, reject mapped onto 0 accept mapped onto 1, take mean observations get overall acceptance rate across gambles (mean whole data 0.38 corresponds acceptance rate 38%).12 However, resp variable, additionally need information actual response 0 1 correspond .Finally, variable five informs us condition participant . Remember previous chapter experiment varied range gains losses across participants resulting four conditions total: condition loss gains ranging -$20/+$40, -$20/+$20 condition, -$40/+$40 condition, -$40/+$20. information provided form decimal number values decimal point referring range gains values decimal range losses without trailing 0 (.e., opposite order referred conditions far). Thus, participant subno 8 -$20/+$20 condition participant 369 -$20/+$40 condition (showing participant -$40 loss condition, lowest possible loss outcome 12 6 two participants shown ).example Walasek Stewart (2015) hopefully clarifies abstract definition variable provided . variable, set possible outcomes defined research design. example, participant identifier subno, set encompasses possible participants can take part study. loss gain, set contains potential losses potential gains occur lotteries. response two possible outcomes, accept reject lottery. define values possible outcome. case subno assign different number every participant collect data. loss gain, values correspond magnitude potential loss gain US dollars (currency used experiment). response, use numeric code use word represent two outcomes.things note example data shown Table 2.1.Every observation data complete; observation values variable missing data. Whereas common experimental research, always case types research. Whereas missing data something discuss detail book, important aware can happen think case (sadly, general solution). example, type experimental research used example , missing data can happen computer used data collection crashes. usually happens rarely unsystematic manner, generally simply discard incomplete data collect data another participants. different issue exist data missing systematically. can happen research sensitive issues. example, easy imagine study sexual health participants sexual transmittable disease (STD) unwilling report fact therefore just answer questions STDs. Discarding cases missing data problematic bias results sense resulting lower rates STD actually present.Every observation data complete; observation values variable missing data. Whereas common experimental research, always case types research. Whereas missing data something discuss detail book, important aware can happen think case (sadly, general solution). example, type experimental research used example , missing data can happen computer used data collection crashes. usually happens rarely unsystematic manner, generally simply discard incomplete data collect data another participants. different issue exist data missing systematically. can happen research sensitive issues. example, easy imagine study sexual health participants sexual transmittable disease (STD) unwilling report fact therefore just answer questions STDs. Discarding cases missing data problematic bias results sense resulting lower rates STD actually present.multiple observations per participant, 64 precise. 64 observations given different rows. call data format – data participant potentially spans multiple rows, one row per observation (.e., 64 rows per participant present case) – long format. long format contrasts wide format also commonly found social sciences. wide format, data one participant spans single row. case participant multiple observations, given different columns. procedures introduced book, generally want data long format. (participant provides one observation, difference long wide format.)multiple observations per participant, 64 precise. 64 observations given different rows. call data format – data participant potentially spans multiple rows, one row per observation (.e., 64 rows per participant present case) – long format. long format contrasts wide format also commonly found social sciences. wide format, data one participant spans single row. case participant multiple observations, given different columns. procedures introduced book, generally want data long format. (participant provides one observation, difference long wide format.)variables differ whether contain numbers (variables response) numbers (response).variables differ whether contain numbers (variables response) numbers (response).","code":""},{"path":"research-designs.html","id":"data-types","chapter":"2 Data and Research Designs","heading":"2.2 Data Types","text":"Let us discuss last point detail. common intuition think numbers thinking data. seen example data, necessary. Data numbers. response variable shows can use values, words phrases, represent values variables. However, conception data primarily numbers also completely false. example, statistical analyses introduced book need represent variables terms numbers. Fortunately us, tools using generally convert data using numbers numeric data necessary. means use type data representation makes easiest understand data stands .","code":""},{"path":"research-designs.html","id":"numerical-versus-categorical-variables","chapter":"2 Data and Research Designs","heading":"2.2.1 Numerical Versus Categorical Variables","text":"important issue arises thinking data numbers numbers can mean different things. One possibility numbers represent numerical information; , represent measurement, magnitude, count something. However, can also use number broader sense serve label (e.g., numbers football jerseys telephone numbers). case, numbers represent categorical information; observation falls one set mutually exclusive categories. meaning numbers important consequences use . numbers represent numerical information mathematical operations make sense. example, make much sense calculate average two telephone numbers. Let us exemplify variables example data.Let us begin loss/gain variable pair (can consider together, type information , difference whether number refers potential loss potential gain). variables, meaning numbers corresponds common understanding numbers magnitude something. particular, magnitude potential loss potential gain. understand variables measuring magnitude potential loss potential gain lottery. larger number larger potential loss gain. fact, number exactly represents potential loss potential gain (.e., measurement potential loss gain perfectly accurate). can treat variables numeric variables statistical analysis, () numbers variables represent numeric information, (b) performing mathematical operations, addition calculating average numbers, meaningful variable. example, calculate average potential loss/gain participant useful information (.e., interpret average meaningful way, example comparing average loss/gain different condition).second example, let us consider subno variable. , numbers really measure magnitude something. Participant number 16 twice participant 8. just looking variable, also know means. described , one just needs assign numbers somehow participant. example, one assign number 1 first participant participates experiment, number 2 second one participates, forth. Alternatively, one also assign number 1 first participant invited, number 2 second one invited, forth. Another possibility specify maximum number participants one can collect, say 500 participants, just assign unique random number 1 500 every participant participates (e.g., drawing pool numbers without replacement). Importantly, need know procedures used. reason subno variables know participant particular observation belongs . numbers subno serve purpose label identifying participant. Instead numbers, also use non-numeric labels, random strings letters, participant variable. Consequently, make much sense perform mathematical operation subno variable. example, average participant number provide useful information.purposes book, distinction two data types central: treat variable numerical variable categorical variable? statistical methods introduced following chapters can deal two types variables (categorical variables can generally also serve role explanatory variable outcome variable). can identify whether variable numerical categorical?Usually, easy identify categorical variables among variables numbers. Whenever numbers represent label, variable usually categorical variable. example, addition subno variable, numbers condition variable serve label identify condition. interpret numbers condition variable shown Table 2.1 actually representing numerical value either 20.2 40.2. Instead, four possible values variable, 20.2, 20.4, 40.2 40.4, refers one four conditions experiment, loss gains ranging either -$20/+$20, -$40/+$20, -$20/+$40, -$40/+$40 (note value decimal point range potential loss value decimal point range potential gains). non-numeric variables values labels, response variable, also clearly categorical variables.difficult decision resp variable. Clearly, two possible values response variable, accept reject decision, response categories labels. However, transforming variable numbers 1 0, can perform meaningful mathematical operations . discussed , mean variable can interpreted average accept proportion. generally, binary categorical variable (.e., categorical variable two categories) can seen special case treating numerical variable can certain situations meaningful. However, whether meaningful depends situation. general best explicitly treat variable categorical unless one sure treating numerical meaningful.sum , statistical purposes book distinguish numerical variables categorical variables. Numerical variables hold numerical information magnitudes something degree something holds. categorical variables values variable serve labels designating membership one number mutually exclusive categories. categorical variables part experimental design, later also call factors.","code":""},{"path":"research-designs.html","id":"assumptions-of-numerical-variables","chapter":"2 Data and Research Designs","heading":"2.2.2 Assumptions of Numerical Variables","text":"case resp variable (.e., numerical representation binary categorical variable) shows decision whether something numerical categorical variable can depend situation. help decision, helpful know exactly entailed treating variable numerical. statistical methods used , treat variable numerical assume represents continuous numerical information. means assume :certain difference interval meaning anywhere scale. example, difference 1 unit variable means whether add 10 20. can see holds loss/gain variable pair later discuss examples questionable assumption. corollary assumption calculating mean variable must meaningful . interpret mean, variable treated numeric.certain difference interval meaning anywhere scale. example, difference 1 unit variable means whether add 10 20. can see holds loss/gain variable pair later discuss examples questionable assumption. corollary assumption calculating mean variable must meaningful . interpret mean, variable treated numeric.variable can principle take real-valued (.e., decimal) number. , even though might used discrete values variables,13 loss/gain variable pair subset whole numbers 6 40 (see Table 1.1), statistical method assumes -values possible principle meaningful.variable can principle take real-valued (.e., decimal) number. , even though might used discrete values variables,13 loss/gain variable pair subset whole numbers 6 40 (see Table 1.1), statistical method assumes -values possible principle meaningful.can see, loss/gain variable pair, two assumptions fully satisfied. However, loss/gain variable pair actually outcome measured experiment. Therefore, statistical analysis play role variable important assumptions fulfilled. Instead, variable pair part design experiment. Consequently, let us consider example variables see well fulfil two assumptions numerical variable.example, numerical outcome variable data set resp. Clearly, resp fulfil assumptions two discrete outcomes, values 0 1. However, can calculate interpret mean (average proportion accepted). also assume specific difference, say 0.1 (10%) difference, means whether happens acceptance rate 50% acceptance rate 85%.14 whereas assumptions violated also partially fulfilled. entails whether can interpret results analysis depends exact context circumstances. example, statistical analysis lead results predictions beyond probability range 0 1, clearly problematic results meaningful. words, learned little meaningful data statistical analysis.popular variable psychology related sciences subjective rating scales (also known “Likert scales”). example, discussed study McGraw et al. (2010) participants one condition asked rate intensity emotional reaction potential loss potential gain response scale ranging 1 = “Effect” 5 = “Large Effect” (see Figure 1.1, unipolar intensity scale). variable represent numerical variable? Clearly, value 5 represents emotional reaction larger value 1. means variable represent magnitude, fulfil assumptions spelled ? can also take average scale interpret meaningful way. Specifically, average emotional intensity participants loss condition, 3.6, larger average emotional intensity participants gain condition, 3.1. However, questionable whether difference 1 means everywhere across scale therefore assumption 2 satisified. specifically, difference “Effect” “Small Effect” (.e., difference 1 2) difference “Moderate Effect” “Substantial Effect” (.e., difference 3 4)? Numerically , whether also holds psychologically question difficult answer. Like researchers McGraw et al. (2010) treated variable numerical variable made assumption (also implicit process calculating average). validity conclusions rests degree whether believe making assumption makes sense.Let us generalise conclusion previous paragraph answer question means two points represent assumptions treating variable numerical variable. Can treat variable numeric variable statistical model perfectly meets assumptions? ideal statistical world answer yes, reality data analysis always differs ideal. Many variables regularly encounter research (e.g., rating scales) violate two assumptions degree still need include numerical variables model (treating categorical help us answering research questions). Whenever assumptions degree violated can interpreted another instance epistemic gap (instance first epistemic gap, Section 1.3.1). fact assumptions violated opens possibility alternative explanation results differs hypothesis. words, assumptions perfectly met evidence provided statistical analysis stronger assumptions partially met.problem numbers treat numerical variable, computer treats numbers way (.e., assuming continuous numerical variable), “numbers don’t remember came ” (Lord 1953)15. – researchers – know numbers came need take account interpreting statistics. can also interpret insight terms concepts introduced previous chapter. numbers part operationalisation; establish procedure maps real world entities (called possible outcomes states affairs) onto values variables (many cases numbers). numbers emerge procedure related research question, identical research question. inference statistical results based numbers requires many auxiliary assumptions, one assume numerical variables continuous. can never sure auxiliary assumptions true, careful humble conclusions draw research.","code":""},{"path":"research-designs.html","id":"measurement","chapter":"2 Data and Research Designs","heading":"2.3 Measurement","text":"far categorized different variables appear data sets can integrate statistical analysis. , take step back consider principled manner variables created. question considering measurement process assign values events allow us infer variable.","code":""},{"path":"research-designs.html","id":"measurement-scales","chapter":"2 Data and Research Designs","heading":"2.3.1 Measurement Scales","text":"discussion variables consisting numbers meaning numbers. One way interpret discussion terms first epistemic gap introduced previous chapter, difference research question operationalisation research question (Section 1.3.1). apply distinction issue section, meaning variables, can understand distinction magnitude latent construct, called attribute context, variable supposed represent (e.g., strength emotional intensity personal risk preference) measurement attribute operationalisation (.e., application procedure assigns number attribute observation). important theoretical contribution distinction within context psychology comes Stevens (1946). assumed can distinguish four different types measurement operationalisation, called measurement scales, respect type relationship attributes reveal. four different measurement scales , nominal, ordinal, interval, ratio scale.scale constructed assigning labels different attribute values, called nominal scale can understood equivalent termed categorical variable. nominal scale, values attributes exhibit quantitative relationship among . addition examples discussed , many demographic variables can understood nominal scale gender (e.g., male, female, non-binary, ) handedness (right-handed, left-handed, ambidextrous).scale constructed rank orderings attributes called ordinal scale. consequence, can order attributes along dimension make quantitative distinctions. common example ordinal scale final result sports competition first place, second place, . important aspect ordinal scale differences values ordinal scale need correspond equivalent differences attribute measured ordinal scale. stay within sports competition example, difference first second place terms performance need difference second third place. example, 2020 Olympics 100 m women sprints final difference first place (Elaine Thompson-Herah) second place (Shelly-Ann Fraser-Pryce) 0.13 seconds, whereas difference second third place (Shericka Jackson) 0.02 seconds. case difference ordinal rank scale (.e., one rank difference), correspond difference underlying attribute (.e., performance, time needed sprinting 100 m). demographic characteristics can also understood ordinal scale, education levels (e.g., primary secondary school education, compulsory education age 16, college, higher education professional & vocational equivalents).interval scale results operationalisation also maintains differences, intervals, values attributes scale need meaning across scale. Thus, interval scale can also understood fulfilling requirements numerical variable.16 typical example interval scale temperature measured either degrees Celsius (°C) Fahrenheit (°F). Within temperature scale, 1 degree difference meaning independent current temperature. Furthermore, scales can transformed . interval scales also makes sense calculate mean (e.g., mean temperature), calculating ratios make sense. example, saying 40 °C double temperature 20 °C really meaningful statement (e.g., 20 °C = 68 °F 40 °C = 104 °F \\(2 \\times 68 \\neq 104\\)).final scale type, ratio scale, results operationalisation addition maintaining meaning differences across scale also contains true zero point attribute. stay within typical example temperature scale, whereas zero point degrees Celsius Fahrenheit arbitrary represent interval scales, zero point Kelvin scale, 0 K, lowest possible temperature making Kelvin scale ratio scale. Many physical scales ratio scale length time. example, makes sense say sprinter took 20 seconds 100 m sprint took twice time sprinter took 10 seconds 0 seconds true zero point time.Whereas Stevens’ four measurement scales widely popular psychology related disciplines thanks prominence introductions textbooks, actual scientific contribution needs considered critically (following Michell 1997, 2002, 1999). Stevens proposes attempts bridge epistemic gap attribute measurement. According position, established interval ratio scale, learned underlying attribute exhibits interval ratio structure. Unfortunately, problem underdetermination discussed , learning actual structure theoretical construct attribute easy. numbers look like numerical variable, mean underlying attribute behaves like numerical variable. Therefore, recommend using four different measurement scales discuss psychological measures. example, book use theoretically neutral terms categorical numerical variables. discusses previous chapter, way description research remains level performed, level operationalisation statistical analysis.Discussing problem measurement scales detail beyond scope present chapter, crux matter issue already highlighted previous chapter. operationalisation used measure certain latent construct represent latent construct. Stevens defines measurement application procedure assigns number representing attribute observation. However, definition measurement allows one infer structure attribute operationalisation. Instead, proper quantitative measurement analogous measurement physical quantities (e.g., length) needs fulfils number assumptions, expressed theory conjoint measurement, psychological behavioural measures shown fulfil. Put bluntly, Stevens’ attempt bridge epistemic gap logically incorrect, establish wants establish need stronger theories define theoretical constructs rigorously (Michell 1999).Even though Stevens’ idea can measure latent constructs interval even ratio scales incorrect, distinction nevertheless helpful allows us understand limits can learn data. can take away present discussion common measurement approach psychology related fields generally able establish ordinal relationships. example, subjective ratings scales, also choices among lotteries, represent ordinal relationships terms underlying latent attributes. Nevertheless, generally treat data variables numerical variables statistical analyses. reinforces point made interpret results statistical analyses directly answering research questions. statistical analysis generally makes assumptions nature underlying attribute construct verify. However, mean hope lost. Stevens (1946) already mentioned introducing ordinal scales, treating numeric always pointless (p. 679): “numerous instances leads fruitful results.” just mindful measurement psychology generally measurement physics interpreting results. primarily learn something operationalisations directly theoretical constructs use formulation research questions.","code":""},{"path":"research-designs.html","id":"reliability-and-validity","chapter":"2 Data and Research Designs","heading":"2.3.2 Reliability and Validity","text":"main message previous section measurement mind behaviour straightforward measuring physical attributes length. Nevertheless, aim use measures (.e., operationalisations provide measurements) high quality. Two concepts important judging quality measurement reliability validity, introduce now.Reliability refers consistency measure. One intuitive way understand reliability consistency measure across different measurement occasions conditions. Reliability also inversely related noise measurement process. measure high reliability repeated applications conditions lead similar outcomes (.e., level noise measurement process low). measure low reliability repeated applications conditions lead widely different outcomes (.e., level noise measurement process high). example, consider regular bathroom scale. expect scale high reliability; get pretty much exact results step several times row, long change weight (e.g., drinking something).Validity refers strongly measure measures supposed measure. Within concepts introduced within book, validity thus refers ability measure bridge epistemic gap operationalisation construct actual (.e., “true”) value construct. Thus, validity can seen one way conceptualise question strongly measurement operationalisation corresponds measurement physical quantity. Given difficulties defining even establishing constructs researchers interested , establishing whether measure high low validity generally difficult.One way visualise reliability validity given Figure 2.1 . , panel represents one operationalisation measure shot target represents one measurement measure. can see reliable measures low levels noise (.e., low level dispersion shots) around one mean value. figure, validity visualised bias respect centre target. Valid measures centred target whereas invalid measures centred around -target value. figure highlights reliability validity principle distinct qualities. measure reliable, mean valid. likewise, valid measure reliable.\nFigure 2.1: Visualisation reliability validity shots target. Figure taken “Statistical Thinking 21st Century” Russell . Poldrack (Figure 2.1).\nWhereas visualisation Figure 2.1 provide good first intuition reliability validity, conceptualisation validity bias way think . stay within metaphor provided figure, invalid measurement also one aims floor instead target. even something completely missing task shooting darts goal shoot bullets. problem validity sometimes don’t know better define measure measuring determining validity always easy.previous paragraph already points important distinction reliability validity. Usually way quantify reliability, quantifying validity possible specific interpretations validity.common ways quantify reliability measure :Split-half reliability internal consistency refer reliability estimate results splitting measure sub-measures comparing scores across sub-measures (e.g., calculating score odd items comparing score even items). Generally, split-half reliability internal consistency can improved making measure longer (.e., adding additional items). Therefore, longer measures often reliable shorter measures (also intuitively makes sense, shorter measure likely noise plays important role).Split-half reliability internal consistency refer reliability estimate results splitting measure sub-measures comparing scores across sub-measures (e.g., calculating score odd items comparing score even items). Generally, split-half reliability internal consistency can improved making measure longer (.e., adding additional items). Therefore, longer measures often reliable shorter measures (also intuitively makes sense, shorter measure likely noise plays important role).Test-retest reliability refers consistency applying measure different time points. using measure questionnaire, difficulty calculating test-retest reliability possibility memory consistency effects temporal instability. Memory consistency effects refer observation participants often prefer self-consistent previous answers remember thus potentially artificially increasing reliability. Temporal instability hand can result artificially lower reliabilities measured construct temporally unstable (mood).Test-retest reliability refers consistency applying measure different time points. using measure questionnaire, difficulty calculating test-retest reliability possibility memory consistency effects temporal instability. Memory consistency effects refer observation participants often prefer self-consistent previous answers remember thus potentially artificially increasing reliability. Temporal instability hand can result artificially lower reliabilities measured construct temporally unstable (mood).Inter-rater reliability refers agreement different raters event situation. Inter-rater reliability can usually calculated measures self-assessments subjective scores. Prominent examples situations one can calculate inter-rater reliabilities medical diagnoses (.e., comparing diagnosis across multiple doctors) essay marks (marked multiple independent markers).Inter-rater reliability refers agreement different raters event situation. Inter-rater reliability can usually calculated measures self-assessments subjective scores. Prominent examples situations one can calculate inter-rater reliabilities medical diagnoses (.e., comparing diagnosis across multiple doctors) essay marks (marked multiple independent markers).quantify validity need external criterion also measures construct interest. criterion, can compare value criterion value measure gives us estimate criterion validity measure. example, previous chapter discussed exist different tasks questionnaires measuring individuals’ risk preferences. Consider also access persons’ financial history showing degree invest money relatively high risk (e.g., stock options), medium risk (e.g., individual stocks), lower risk (e.g., index funds) financial instruments. risk preference measure high criterion validity one participants high score measure invest actual money high-risk investments. discussed previous chapter, unclear measure risk preferences exist.types validity non quantifiable. Construct validity usually considered important type validity refers empirical theoretical support provides evidence measure measures supposed measure. construct validity essentially asks well measure bridges first epistemic gap, abstract concept one talks measures actual criterion used describe measures (.e., aware psychological measure construct validity).Another interesting type validity face validity. refers degree test appears measure supposed measure. Whereas face validity objectively important validity test (.e., matters test looks like measures supposed measure, ), can important commitment participants. example, participants intrinsically interested contributing time effort research question measure high face validity (.e., matches interest) shoulod reduce participant drop compared measure lower face validity.Considerations reliability validity incorporated overall assessment results judging empirical evidence provided given study. example, dependent variable study consists single item response, generally implies low internal consistency compared dependent variable based items responses. case, level noise relatively high, influence strongly weigh evidence study.","code":""},{"path":"research-designs.html","id":"independent-and-dependent-variables","chapter":"2 Data and Research Designs","heading":"2.4 Independent and Dependent Variables","text":"addition distinguishing type information variables can contain, can also distinguish different roles variables can play research process. Remember, discussing operationalisation step research process specified need identify relevant variables hope can address research question well specify empirical hypothesis involving least two variables. see , can assign two different roles least two different variables.Usually exactly one variable variable interested results, psychology call variable dependent variable. Synonyms “dependent variable” common statistical literature response variable, outcome variable, criterion can see meaning points direction “dependent variable.” dependent variable main outcome study, variable primarily interested measuring.variable(s) called independent variable(s) psychology, believe values dependent variable depend values independent variable(s). popular synonym “independent variables” statistical literature covariates, dependent variable assumed covary independent variables.17 also used term explanatory variable describe independent variables. Loosely speaking can describe distinction study effect independent variable dependent variable.18Let us show distinction loss aversion study Walasek Stewart (2015) data shown Table 2.1 . research question matters people’s preference symmetric 50-50 lotteries absolute value potential loss gain relative rank lotteries compared lotteries. operationalisation research question involved manipulation range lotteries one variable, condition, measuring participants’ responses symmetric lotteries response\\resp variable pair. distinction variable types relatively straight forward. interested effect condition response: impact different ranges lotteries (.e., manipulated across conditions) participants responses. makes condition independent variable response variable dependent variable.general, distinction independent dependent variable easy understand experiment, study Walasek Stewart (2015). experiment, independent variables manipulated, case condition. “Manipulated” means assign participants different conditions (compared measuring condition participant ).studies can experiments independent variable manipulated. example, common research question effect demographic variable outcome. However, demographic variables really manipulated assigned participants. example, might interested studying effect parental wealth children’s educational attainment. Whereas manipulating parental wealth principle possible, common approach measure variable well children’s educational attainment. Nevertheless, can still make distinction independent variable, parental wealth, dependent variable, educational attainment.variables experiment neatly fall within distinction independent dependent variables. example, let us go back six variables shown Table 2.1 make full data collected study Walasek Stewart (2015). discussed , response\\resp variable pair dependent variable condition independent variable. leaves us three variables need classified. go try classify three variables continue reading paragraph.Let us begin loss/gain variable pair determines possible outcomes lottery shown participants. Clearly also manipulated. specifically, condition determines exactly lotteries therefore values loss/gain variable pair participant works . Thus, loss/gain variable pair also independent variables jointly determine independent variable condition (.e., condition variable data set determine loss/gain variable pair). means determining independent variables study depends perspective one takes. focus main research question symmetric lotteries condition independent variable. However, also look lotteries individually, condition loss/gain variable pair independent variables.Table 2.1 contains one variable, subno, participant identifier variable. might seem bit surprising think variable terms independent dependent variable, able classify variable somehow. Clearly, subno dependent variable. interested values results subno variable. However, also seems clearly independent variable. specific expectations ideas different subjects affect response variable. help us classification let us consider subno variable data first place; collect data multiple participants need identify participant observation belongs. follow-question collect data multiple participants? discussed previous chapter (Section 1.3.2), participants source noise experiment different participants can multitude reasons. data one participant, distinguish idiosyncratic noise participant signal interested . collecting data multiple participants, try control noise averaging hope remains signal. overarching idea noise unsystematic effect results; participants may likely show particular behaviour whereas participants may less likely show behaviour, average noise cancels . sum, collect data multiple participants control noise inevitable dealing real people. Thus, say subno control variable. even specific say control variable independent variable, use control noise level design. situations might measure variable control purposes case call control variable dependent variable.sum section : Jointly, dependent independent variables key components operationalisation research question. central concepts make study design link practical reality research (.e., research actually takes place measured) research question. wrong say study defined primarily dependent independent variables. designing one’s study, making clear dependent independent variables maybe important decision decided research question. Likewise, reading scientific article describing study, understanding clearly independent dependent variables central understanding study. Therefore, whenever thinking talking research, make sure clear dependent independent variables . experimental research often boils asking: task participants? variables part study usually serve control purpose can denoted control variables.","code":""},{"path":"research-designs.html","id":"experimental-versus-observational-variables","chapter":"2 Data and Research Designs","heading":"2.5 Experimental versus Observational Variables","text":"seen discussing distinction independent dependent variables, can distinguish different types independent variables research designs, namely experimental non-experimental independent variables. adopt common terminology use observational variable describe non-experimental independent variables. study solely consists experimental variables (drop “independent” part now experimental observational variables always independent variables), can call experimental study experiment short. study solely consists observational variables, can call observational study. study contains experimental observational variables, agreed upon name depending variable relevant research question, researchers tend use either experimental observational study. However, experimental variables provide number evidential benefits discussed , tendency call study experiment even also contains observational variables. Depending actual situation inferences drawn can seen stretch.experimental variable one control can manipulated researcher. means values variables can assigned participants researcher.example, study Walasek Stewart (2015) researchers assigned participant one four conditions corresponding different range potential losses gains. Likewise, previous chapter briefly introduced study Hinze Wiley (2011) generality testing effect. initial reading piece text, participants assigned one three experimental conditions: control condition re-read materials, testing condition using open-ended questions, testing condition using fill--blank text. cases, researchers decided condition participant part .important part experiment variable participants can assigned different conditions, assigned. specifically, experimental variable assignment needs performed randomly; say participants randomised available conditions. One way understand random assignment experiment takes place, probability experimental conditions needs every participant.19For example, random assignment means every participant study Walasek Stewart (2015), probability four conditions 0.25 (.e., 1/4). every participant study Hinze Wiley (2011), probability three conditions approximately 0.33 (.e., 1/3).can imagine randomisation actual physical process produces random outcome, toss coin throw dice. example, study Hinze Wiley (2011) imagine every participant takes part experiment, researcher (research assistant) throws regular six-sided dice. dice lands 1 2 participant assigned re-reading condition, dice lands 3 4 participant assigned open-ended question condition, dice lands 5 6 participant assigned fill---blank condition. Alternatively, pre-specify sample size want ensure every group approximately size, use different approach. prepare many sheets paper number participants want collect. sheet write one condition among sheets, condition appears equally often. , shuffle sheets bowl randomise order. performing experiment, take one sheet bowl (without putting back) every participant assign participant condition written sheet. Nowadays randomisation mostly done computer using -called random number generators.observational variable variable control researcher randomly assign participants condition. words, whenever randomisation impossible independent variable observational variable.talked demographic characteristics, particular parental wealth, independent variable. already described , demographic variables generally randomly assigned mostly immutable part person. Consequently, demographic characteristics (e.g., age, gender) generally observational variables. true many psychological characteristics person personality traits (e.g., extraversion) abilities (e.g., intelligence quotient). vast majority cases variables observational variables.20At point might wonder experiment necessarily entails randomisation independent variable. benefit experimental observational variable? reason randomisation allows drawing causal inferences study. experimental – randomised – independent variable can say independent variable cause dependent variable. Without randomisation (.e., dealing observational independent variable) inference permitted.Remember said one way think distinction dependent independent variables study want learn effect independent variable dependent variable. specify mean “effect,” said way think dependent independent variable holds loosely. “effect” meant causal relationship, independent variable cause dependent variable. absence cause-effect relationship, seems wrong speak “effect independent variable.” can now see reason said holds loosely, holds independent variable experimental variable, observational variable.","code":""},{"path":"research-designs.html","id":"epistemic-gap-3-causal-inference-and-confounding-variables","chapter":"2 Data and Research Designs","heading":"2.5.1 Epistemic Gap 3: Causal Inference and Confounding Variables","text":"reason experimental variable allows causal inference due another epistemic gap, possible influence confounding variables dealing observational data. causal inference means learn independent variable nothing else responsible effect observed dependent variable. causal inference possible plausible alternative explanations observed effect dependent variable involve independent variable can ruled . context causal inference, alternative explanation known confounding variable confounder short. randomly assign participants conditions can theoretically rule confounders alternative explanation. However, case observational variable ; may reasons, confounders, related observational variable responsible observed effect. Similar first epistemic gap, inference independent variable cause effect dependent variable underdetermined observational variable, underdetermined experimental variable.21Let us exemplify problem new example. Imagine want investigate effectiveness novel drug control treatment (.e., old drug) treatment viral infection hospital setting. independent variable treatment (control treatment versus novel drug) dependent variable viral load (.e., whether virus can still detected system). Let us imagine results show new drug effective control treatment. , participants show lower viral load (.e., less sick) novel treatment control treatment. question trying answer now whether matters inference can draw study assignment treatment condition random .Let us begin considering non-random assignment independent variable. example, one way implement non-random random assignment use novel drug one hospital control treatment another hospital. run study, allow us conclude difference dependent variable due differences treatment? inference allowed two hospitals identical. systematic differences hospitals, say patients hospital control treatment average older patients hospital new treatment (e.g., different areas different population characteristics), difference responsible difference dependent variable.systematic difference age hospitals plays role confounder. Age responsible choice hospital patients go , patients near hospital control treatment administered average older patients near hospital new treatment administered. Age also responsible difference dependent variable, older patients less likely recover thus higher viral load end study young patients. situation confounder present, infer treatment cause observed effect. logical structure indicative underdetermination: two conditions differ terms two characteristics, treatment status age, either () can responsible differences dependent variable. sure one two possible causes .Let us now consider situation participants randomly assigned two treatment conditions. , data two hospitals participants hospital randomly assigned treatment conditions. example, new patient shows relevant symptoms doctor administering treatment takes pre-randomised envelope contains either old drug novel drug.22 assignments conditions random expect confounders age balanced across two condition. Every participant comes two hospitals chance either get control treatment novel drug, independent age characteristics. Consequently, long randomness introduce accidental confounding (see Section 1.3.2) can attribute effect dependent variable independent variable.summary, difference experimental observational variable degree can rule possible alternative explanations. experimental variable participants randomly assigned conditions know theory confounding variables balanced. long randomisation proceed planned, one can certain alternative explanation effect independent variable dependent variable random chance. always might get unlucky confounding variable just happens unbalanced data set. However, larger sample sizes larger effects chance alternative explanation becomes increasingly unlikely. experimental variables epistemic gap wanting judge whether independent variable responsible effect independent variable dependent variable effect random chance noise (see Section 1.3.2).example, main study lead approval Biontech Covid-19 vaccine (Polack et al. 2020), 40,000 participants randomly assigned (.e., 20,000 participants per condition) either receive real vaccine placebo (.e., saline injection without active ingredients). Among participants received vaccine 8 participants developed Covid-19 among participants received placebo 162 participants developed Covid-19. Whereas results definitely rule confounding variable explains difference contracting Covid-19 seems extremely unlikely. Participants trial recruited six different countries (e.g., USA, Turkey, Brasil) diverse demographic characteristics (e.g., sex, ethnicity, age, weight), characteristics extreme similar conditions (Polack et al. 2020, Table 1).observational variable participants randomly assigned condition know whether potential confounding variable. One way address problem measure known confounding variables show responsible difference dependent variable. even able control measure large number possible confounding variables, can never certain another unobserved confounding variable responsible effect. observational variables always deal two epistemic gaps wanting judge whether independent variable responsible effect independent variable dependent variable, problem possible confounders plus random chance noise.end section, let us come back example trial testing new drug hospital setting. lengthy discussion observational versus experimental variables can hopefully see idea administering new drug one hospital control treatment another hospital bad idea. Without proper randomisation participants treatments inference drug responsible effect viral load seems weak thanks possible influence confounders. might even go far wonder ever run study without proper randomisation believe corresponding results.Sadly, study pretty much exactly sketched – administering novel drug one hospital control treatment another hospital, patients systematically differing hospitals – played unfortunate role Covid-19 pandemic. particular, first study suggest Hydroxychloroquine effective Covid-19, study Gautret et al. (2020), exactly problem.23 Whereas critics quick point problems study (Bik 2020; Rosendaal 2020; Sayare 2020), damage done. current US president Donald Trump praised Hydroxychloroquine wonder cure Covid-19. required much scientific effort follow-studies, using resources potentially used productively elsewhere, show (full timeline events see Sattui et al. 2020). problem case whereas medical statistical experts immediately see problems study, general public . false claim appears scientific (.e., media reported along lines “researchers shown …”) established public discourse, often difficult combat . general seems discussing empirical evidence provided particular scientific study either beyond expertise available mass media unwilling invest time commitment .","code":""},{"path":"research-designs.html","id":"is-causal-inference-from-observational-data-possible-at-all","chapter":"2 Data and Research Designs","heading":"2.5.2 Is Causal Inference from Observational Data Possible at All?","text":"previous section argues causal inference generally possible experimental independent variables. observational variables can always confounder responsible effect instead independent variable. However, many interesting research questions investigated experiments observational variables. discussed , demographic variables immutable features individuals, personality traits, observational variables definition. Likewise, many variables relating lifestyle choices, dietary exercise habits, might principle amenable experimental manipulations, reality seems difficult impossible completely unethical run corresponding experiments. mean draw causal inferences research questions? believe honest realistic answer vast majority cases . eyes fair assessment situation , causal inference observational data literally difficult problem empirical sciences.Importantly, causal inference observational data primarily statistical problem. introduced problem confounders pose epistemic gap. epistemic gaps, overcoming epistemic gap requires diverse conceptually strong evidence. statistical methods can assist providing evidence, provide type compelling evidence needed. problem even observational data strongly suggests something, always possibility confounder missed adequately taken account.example problem, let us consider case vitamin supplements, specifically vitamin C E supplements (Lawlor et al. 2004; Woodside et al. 2005; Mozaffarian, Rosenberg, Uauy 2018). Early evidence large observational studies 90s tens thousands participants suggested taking vitamin C E supplements reduces chance getting cancer cardiovascular diseases considerable degree. Based positive results, large scale experiments (.e., also tens thousands participants) followed participants randomly assigned either take vitamin supplements placebo (.e., sugar pill without vitamins) monitored several years. large, experiments replicate positive effects found observational studies. Unless individual susceptible vitamin deficiency, vitamin supplements appear measurable health benefit. probable reason difference observational studies experiments likely due insufficient adjustment socioeconomic status confounder. often found, participants came better socioeconomic background healthy (.e., less likely develop cancer cardiovascular diseases) also likely take vitamin pills (believed helpful). Whereas observational studies measured tried account differences socioeconomic status participants already take vitamin supplements , partially [e.g., account differences socioeconomic status parents led developmental differences also affected probability developing cancer cardiovascular diseases well probability taking vitamin pills; Lawlor et al. (2004)].example shows even situation confounder principle known (.e., socioeconomic status) observational data sets large (> 10K participants), causal inference observational data possible. Even attempting account confounding, observational data suggested relationship turned spurious. apparent problem accurately measuring influence confounder possible, knowing observed relationship fact spurious. experiment able reveal effect vitamin supplements. example suggests many research questions data sets common psychology related disciplines causal inference observational data equally difficult even impossible. Especially data sets often considerably smaller less known causal relationship existing domain (.e., variables act confounders).consequence problem observational data, current book primarily focuses experimental data sets , non-experimental variables considered, limitations discussed. Whereas focussing experimental data restricts type research questions can investigated, least eliminates one three epistemic gaps introduced . also means applying methods introduced observational data sets require additional care trying draw justified conclusions recommended. repeat said , question whether effect found observational data reflects causal relationship statistical question. statistical tools introduced provide answer question whether relationship observational data causal.researchers interested analysing observational data, good introductory literature attempt approach problem confounders principled manner Rohrer (2018), Gelman, Hill, Vehtari (2021), McElreath (2020), Hernán Robins (2021), Shadish, Cook, Campbell (2002). Note , given additional epistemic gap needs bridged, methods advanced methods introduced (.e., require technical mathematical knowledge going beyond required ). , drawing causal inferences observational data literally difficult problem empirical sciences.","code":""},{"path":"research-designs.html","id":"internal-versus-external-validity","chapter":"2 Data and Research Designs","heading":"2.5.3 Internal versus External Validity","text":"talked validity context measurement. context, question validity question measure measures supposed measure (e.g., risk attitudes questionnaire really measures risk attitudes high validity). However, term “validity” also used context experimental versus observational studies. context, two relevant types validity internal validity external validity refer specific measure, used describe complete studies research designs. provide brief introduction two terms , see Shadish, Cook, Campbell (2002).24Internal validity refers internal structure study reflects degree study provides evidence causal relationship independent dependent variable. means generally speaking, internal validity high study experiment (.e., independent variable randomised) internal validity low study observational study.25 Within terminology epistemic gaps introduced book, internal validity related third epistemic gap. can sure possible confounders internal validity high, can sure case experimentally manipulated independent variable.External validity refers degree results study generalise different settings, different situations, people, stimuli, times. Within terminology epistemic gaps introduced book, external validity related first epistemic gap, underdetermination theory data. degree can sure results really address research question confined specifics operationalisation can sure results generalise situations. words, learn causal link holds specific circumstances tested within study, actually hold general terms research question formalised, external validity low. example, study Hinze Wiley (2011) introduced Chapter 1 directly addressed external validity seeing whether testing effects also holds different operationalisation “testing.”","code":""},{"path":"research-designs.html","id":"summary-1","chapter":"2 Data and Research Designs","heading":"2.6 Summary","text":"chapter introduced number important concepts allow us describe studies research designs. begun highlighting empirical scientists ultimate arbiter whether believe theoretical position hypothesis empirical evidence. evidence come systematically collected data sets anecdotes.Data sets can used address research questions consist independent variable(s) usually one dependent variable. distinction assume dependent variable depends independent variable. independent variable experimental variable participants randomised onto conditions can even infer independent variable causally responsible effect dependent variable. independent variable solely observational variable, generally make causal judgement.reason observational variables allow causal inferences lies third epistemic gap introduced . observational variable, can always different confounding variable responsible effect independent variable dependent variable.Together three epistemic gaps put clear limits can learn empirical data psychology related disciplines. first epistemic gap, underdetermination theory data, difference research question operationalisation research question. Whereas operationalisation attempts address research question usually . second epistemic gap, signal versus noise, concerns relationship operationalisation statistical analysis. Even statistical analysis appears provide support empirical hypothesis, 100% sure . always chance observed outcome just occurred chance – noise – represent genuine signal data. Finally, third epistemic gap, confounding variables, always present dealing observational independent variables. just summarised, absence randomisation can never really sure independent variable confounding variable reason observed effect. Thus, can reiterate message ended previous chapter. interpret statistical results, need careful humble conclusions draw.also introduced different data types can deal statistical analysis. Independent variables can numerical categorical variables. independent variable categorical, generally call experimental factor, just factor. Dependent variables can generally numerical variables, unless binary numerical variable treating numerical.also argued genuine psychological variables collect, responses rating scales, ordinal scale satisfy assumptions numerical variable. However, analyses nevertheless treat numerical. violation statistical assumption places limits inferences permitted study. line , argued measurement psychology generally difficult problem simply assuming measures provide information actually provide another inferential problem deal .","code":""},{"path":"chapter-2-quiz.html","id":"chapter-2-quiz","chapter":"Chapter 2: Quiz","heading":"Chapter 2: Quiz","text":"Note: pull-menu selecting answer turns green selecting correct answer.Exercise 2.1  mean statistical assumptions perfectly met?results statistical analysis true.results statistical analysis likely true.evidence provided statistical analysis stronger assumptions met.Answer: 123Exercise 2.2  one following good way checking quality measurement?ReliabilityValidityOperationalisationAnswer: 123Exercise 2.3  mean measure low reliability?means reliability analysis conductedIt means repeated applications conditions lead widely different outcomesIt means level noise measurement lowAnswer: 123Exercise 2.4  one following measure reliability?Test-retest reliabilityInter-rater reliabilitySplit-half reliabilityCriterion reliabilityAnswer: 1234Exercise 2.5  one synonym dependent variable?Result variableOutcome variableResponse variableCriterionAnswer: 1234Exercise 2.6  randomisation important experimental studies?allows researchers theoretically rule confounders cause effect observed dependent variableIt allows researchers run statistical analysisIt allows researchers correctly choose dependent independent variablesAnswer: 123Exercise 2.7  true numbers?\n1. Numbers can represent categorical information\n2. Numbers can represent numerical information\n3. apply mathematical operation numbers, can interpret results independent meaning numbersAnswer: 123Exercise 2.8  one type variable research design?\n1. Dependent variable\n2. Epistemic variable\n3. Control variableAnswer: 123Exercise 2.9  mean data presented wide format?\n1. data one participant spans single row\n2. data one participant spans multiple rows\n3. one row per observationAnswer: 123Exercise 2.10  mean variable represents ‘continuous numerical information?’\n1. variable can principle take real-valued (.e., decimal) number\n2. certain difference interval meaning anywhere scale\n3. answers correctAnswer: 123","code":""},{"path":"short-introduction-to-r-and-the-tidyverse.html","id":"short-introduction-to-r-and-the-tidyverse","chapter":"3 Short Introduction to R and the tidyverse","heading":"3 Short Introduction to R and the tidyverse","text":"empirical scientists believes based empirical evidence comes form data. Consequently, central skill researchers ability process analyse data. using statistical programming language R.","code":""},{"path":"short-introduction-to-r-and-the-tidyverse.html","id":"what-is-r","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.1 What is R?","text":"R comprehensive tool enables skilled user perform steps tasks data analysis . example, R can:Prepare data analysis: read data comes pretty much format, data manipulation, data wranglingExplore data using summary statistics graphical summaries: exploratory data analysis, descriptive statistics, data visualisationPerform statistical analysis data: inferential statisticsCommunicate results: publication-ready results graphics, research reports combine narrative text statistical resultsAnd much : data simulations advanced statistical methods, machine learning, interactive data visualisations, websites, books (present one)top incredible list things can done R, R free software (sometimes also known open source software). means, R completely free download, install, use, also free inspect source code make changes (long make version R un-free).Given flexibility things can R, completely surprising requires effort start R. Especially R first real experience learning programming language.important thing know new R user beginning hard almost everyone (including present author). important message hang keep trying. likelihood, least rather frustrating situations first weeks interacting R, get better. taught R many users great variety backgrounds experiences many struggled way beginning, anyone kept hopes continued put time effort struggles vain. can learn R just give believe . assured believe . Learning R incredibly powerful skill surely positive effect whatever comes later life, career Academia (.e., university) “real world” (academics like call everything university job).","code":""},{"path":"short-introduction-to-r-and-the-tidyverse.html","id":"getting-started-with-r","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.2 Getting Started with R","text":"somewhat scary introduction way, next important question probably get going R? Providing comprehensive answer question beyond scope present work. Instead, shortly provide pointers freely accessible resources provide introductions different levels prior experiences. , feel important highlight basic R software performs calculations can installed CRAN, Comprehensive R Archive Network, particularly bare bone. Therefore, addition R also recommend install RStudio. RStudio popular IDE – , integrated development environment – R makes using quite bit comfortable. Also note R RStudio need updated independently . Especially R updated least per year CRAN. already R installed computer remember last time updated, now probably good time . personally update R usually within weeks new version appearing.following provides overview resources get started R brief explanations offers:R Psychological Science Danielle Navarro. series websites provide introduction basic concepts R recommended introductory resource. time writing yet complete (.e., still progress), excellent. guide steps getting know R learn basics. introductory resource recommend check :\nPart 1, “core toolkit” covers R installation, variables data types, scripts, packages, basic programming concepts, loops, branches, functions. recommend go sections Part 1 first thing getting started R.\nPart 2, “working data” covers complex data types, importantly R’s central data type, data.frame, already provides introduction tidyverse, also used . recommend least go prelude data types (pay special attention dataframes). discussed many issue also relevant present work.\nR Psychological Science Danielle Navarro. series websites provide introduction basic concepts R recommended introductory resource. time writing yet complete (.e., still progress), excellent. guide steps getting know R learn basics. introductory resource recommend check :Part 1, “core toolkit” covers R installation, variables data types, scripts, packages, basic programming concepts, loops, branches, functions. recommend go sections Part 1 first thing getting started R.Part 2, “working data” covers complex data types, importantly R’s central data type, data.frame, already provides introduction tidyverse, also used . recommend least go prelude data types (pay special attention dataframes). discussed many issue also relevant present work.case prefer video introductions instead reading text, Danielle Navarro also published series awesome video lectures introductory topics. getting started R recommend:\nInstalling R: Downloading installing R specific videos operating system\nProject structure: provides great overview introduction topic fundamental working computers beyond R: working file system. covers naming files, file paths, folders, related technical stuff important programming, often taught explicitly.\ncase prefer video introductions instead reading text, Danielle Navarro also published series awesome video lectures introductory topics. getting started R recommend:Installing R: Downloading installing R specific videos operating systemProject structure: provides great overview introduction topic fundamental working computers beyond R: working file system. covers naming files, file paths, folders, related technical stuff important programming, often taught explicitly.MSc Conversion: R Research Methods book Emily Nordmann book accompanying video lectures. book aimed similar audience present book. However, books focuses concepts use (e.g., RMarkdown, accessing R runs server). book part psychTeachR book plus video series developed University Glasgow contains interesting books Data Skills: psyTeachR Books introductory.MSc Conversion: R Research Methods book Emily Nordmann book accompanying video lectures. book aimed similar audience present book. However, books focuses concepts use (e.g., RMarkdown, accessing R runs server). book part psychTeachR book plus video series developed University Glasgow contains interesting books Data Skills: psyTeachR Books introductory.Learning Statistics R also Danielle Navarro (can see, big fan Danielle’s work). completely free introductory book statistics using R, can downloaded website (currently available version 0.6). Part II (.e., Chapters 3 4) provides gentle comprehensive introduction R newcomers. installing R RStudio, navigating console RStudio windows, basic data types, reading data file system, important data types, data.frames, roughly 70 pages (.e., pp. 35 - 109) covered. look two chapters, Chapter 8 great next step introduces R scripts. completely new statistics, Chapter 5 also provides great introduction important concepts. One downside resource comes form PDF website, read comfortably devices (great printing). Also note R Psychological Science updates version book, probably start first. Finally, note present book somewhat different conceptual focus introducing statistical tests methods (.e., especially compared Part IV).Learning Statistics R also Danielle Navarro (can see, big fan Danielle’s work). completely free introductory book statistics using R, can downloaded website (currently available version 0.6). Part II (.e., Chapters 3 4) provides gentle comprehensive introduction R newcomers. installing R RStudio, navigating console RStudio windows, basic data types, reading data file system, important data types, data.frames, roughly 70 pages (.e., pp. 35 - 109) covered. look two chapters, Chapter 8 great next step introduces R scripts. completely new statistics, Chapter 5 also provides great introduction important concepts. One downside resource comes form PDF website, read comfortably devices (great printing). Also note R Psychological Science updates version book, probably start first. Finally, note present book somewhat different conceptual focus introducing statistical tests methods (.e., especially compared Part IV).prefer introduction stronger programming focus, recommend free book Hands-Programming R Garrett Grolemund. Chapters 1 5 (.e., pp. 1 - 99) also provide introduction starts installing R gentle manner introduces necessary basic concepts including different data types data.frames.prefer introduction stronger programming focus, recommend free book Hands-Programming R Garrett Grolemund. Chapters 1 5 (.e., pp. 1 - 99) also provide introduction starts installing R gentle manner introduces necessary basic concepts including different data types data.frames.sum long list , minimum get book recommended go sections Part 1 (especially chapters 1, 2, 3, 4, 5, 6, 11) least data.frames chapter (Chapter 13) Part 2 Danielle Navarro’s R Psychological Science. yet super comfortable file system computer, also recommend check Danielle Navarro’s video lecture project structure.","code":""},{"path":"short-introduction-to-r-and-the-tidyverse.html","id":"a-base-r-example-session","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.3 A Base R Example Session","text":"discussed previous chapter, important format data representation tabular format column representing single variable typically one row per observation. data represented base R data.frame, important data format needs. use term “base R” refer using R without additional packages. Let us quickly recap data.frame base R looks like let us basic operations also sets stage using R scripts.","code":""},{"path":"short-introduction-to-r-and-the-tidyverse.html","id":"files","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.3.1 Data Files, Scripts, and Working Directory","text":"chapter, mainly working data Walasek Stewart (2015) introduced detail Section 1.2.2. data consists two data files representing separate experiment discussed together far exact replications another. Walasek Stewart (2015) reported Experiments 1a 1b. original data files sent Lukasz Walasek Excel files (can found : Experiment 1a Experiment 1b).base R allow read Excel files, opened original data files Walasek Stewart (2015) Excel saved .csv files (.e., comma separated files) can downloaded : Experiment 1a Experiment 1b. recommend download files save folder can access within R.follow along upcoming R code, save downloaded data files folder named data current R working directory. also recommend create new empty R script also save working directory. example, say already already created folder book/class, let’s assume folder called stats_r_intro-stuff easy find location (e.g., depending operating system Documents folder home folder ~). Let’s assume want folder working directory. , create new folder folder, called data (.e., stats_r_intro-stuff/data), download two data files (Experiment 1a Experiment 1b) folder. Next create empty R script folder chapter, say tidyverse-intro.R (.e., now stats_r_intro-stuff/tidyverse-intro.R). can now easily set current R session folder working directory opening tidyverse-intro.R RStudio (yet opened) using menu: Session - Set Working Directory - Source File Location. sets working directory folder (.e., location) thr currently active R script located, stats_r_intro-stuff folder. Note, using current R session already time, now good time restart session (necessarily RStudio) using menu : Session - Restart R.workflow laid previous paragraph represents general recommendation working R stage. folder designated project folder. folder R scripts well sub folder data. can open script RStudio set folder working directory menu (.e., Session - Set Working Directory - Source File Location). can access data files simply accessing data/-data.file.csv. Importantly, don’t forget restart R session starting something new menu: Session - Restart R.","code":""},{"path":"short-introduction-to-r-and-the-tidyverse.html","id":"read-data-and-inspecting-it","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.3.2 Read Data and Inspecting it","text":"set-steps way, can now load data Experiment 1a using base R’s read.csv() function:read data file without warnings problems.26 reason fail download file set correct working directory, can also try read directly internet following code. please tried downloading dealing actual file. Handling data files working directories important R skill need acquire.first thing usually want data.frame inspect structure using str() function. lists variables, data types, number observations variables.present case 20 thousand observations six variables. variables int means integers; , numeric variables consisting whole numbers (.e., discrete values). also one num – , numeric – variable (.e., numeric variable decimal values), condition. general, int num variables treated way, numeric variables, hardly ever reason transform one . Finally, condition chr character variable.","code":"\ndf1a <- read.csv(\"data/ws2015_exp1a.csv\")\ndf1a <- read.csv(\"https://github.com/singmann/stats_for_experiments/raw/master/data/ws2015_exp1a.csv\")\nstr(df1a)\n#> 'data.frame':    22912 obs. of  6 variables:\n#>  $ subno    : int  8 8 8 8 8 8 8 8 8 8 ...\n#>  $ loss     : int  6 6 6 6 6 6 6 6 8 8 ...\n#>  $ gain     : int  6 8 10 12 14 16 18 20 6 8 ...\n#>  $ response : chr  \"accept\" \"accept\" \"accept\" \"accept\" ...\n#>  $ condition: num  20.2 20.2 20.2 20.2 20.2 20.2 20.2 20.2 20.2 20.2 ...\n#>  $ resp     : int  1 1 1 1 1 1 1 1 0 1 ..."},{"path":"short-introduction-to-r-and-the-tidyverse.html","id":"transforming-categorical-variables-into-factors","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.3.3 Transforming Categorical Variables into Factors","text":"discussed Section 2.2, numeric variables well condition actually categorical variables, Factors R parlance. can transform variables Factors using apptly named factor() function. general use factor() instead methods (e.g., function .factor()) allows us specify ordering factor levels potentially labels factor levels. Let us three variables now, subno, response, condition. use $ operator access variables data.frame. transforming variables take another look structure data.frame using str().see running commands, expected three Factors data. Let us take look calls factor() understand call different manner case.call subno passes variable (.e., df1a$subno vector) arguments factor(). consequence, factor levels ordered alpha-numerical increasing manner.call subno passes variable (.e., df1a$subno vector) arguments factor(). consequence, factor levels ordered alpha-numerical increasing manner.call response specifies ordering factor levels using levels argument passed vector levels, c(\"reject\", \"accept\") (.e., remember, c() function creating vectors kind, character vectors ). reason otherwise factor levels alphabetically ordered accept first level reject second level (“” comes “r” alphabet). inconsistent resp, 0 = reject 1 = accept.call response specifies ordering factor levels using levels argument passed vector levels, c(\"reject\", \"accept\") (.e., remember, c() function creating vectors kind, character vectors ). reason otherwise factor levels alphabetically ordered accept first level reject second level (“” comes “r” alphabet). inconsistent resp, 0 = reject 1 = accept.call condition specifies levels levels well new names factor levels using labels. labels use ordering (.e., potential loss first, potential gain second) used throughout book differs ordering original condition (.e., switches ordering format used now). arguments vector passed, elements mapped position (.e., new label first level, 40.2, first label, -$20/+$40). specify ordering factor levels levels maintain ordering used throughout condition typically shows loss aversion, -$20/+$40 first condition. done , first level -$20/+$20 condition (20.2 smallest number original vector).call condition specifies levels levels well new names factor levels using labels. labels use ordering (.e., potential loss first, potential gain second) used throughout book differs ordering original condition (.e., switches ordering format used now). arguments vector passed, elements mapped position (.e., new label first level, 40.2, first label, -$20/+$40). specify ordering factor levels levels maintain ordering used throughout condition typically shows loss aversion, -$20/+$40 first condition. done , first level -$20/+$20 condition (20.2 smallest number original vector).interesting part three calls factor() function run , already ran , break condition variable. try , see values condition variable change NA run call second time. reason values passed levels argument need present variable. However, replaced original values new labels, none original levels (.e., c(40.2, 20.2, 40.4, 20.4)) part variable . Consequently, values replaced NA (.e., “available” means missing data). get values back need reload data using read.csv() command can run factor() call state data ran first time.shows assume running piece code twice gives output cases. problem code changes data (.e., values condition). However, code also assumes certain values condition. assumption holds first time run code, second time, second calls breaks results somewhat unexpected ways. means randomly re-running code can lead unexpected results (called “bugs” programming language parlance). Therefore, instead re-running individually pieces code can often help re-start top script re-running everything order ensure data state think (ideally restarting R session Session - Restart R).One tip transforming variables factors. often bit annoying type factor levels hand, especially say two. case, handy trick know can get R produce c() call factor levels. just need make sure using correct ordering. heart trick dput() function creates text representation R output can copied console script. use factors basic structure call dput(unique(df$variable)) returns c() call unique elements variable. want elements ordered can use dput(sort(unique(df$variable))). example, create levels argument condition variable initially executed dput(sort(unique(df1a$condition))) console returns c(20.2, 20.4, 40.2, 40.4) original df1a data (.e., turning everything Factors). just copy pasted bit vector get ordering right (admittedly, typing might faster copying pasting, whatever).Another thing often want reading data getting overview actually looks like. One way recommend data.frames just typing name variable console (calling just name script). means R object printed , large data.frames, leads hundreds thousands rows printed printing limit reached. alternative just look first rows using head() function (prints first 6 rows per default):Alternatively, can click object RStudio Environment pane opens data viewer pane. equivalent R call using View(df1a) opens viewer. However, View() used interactively console R script requires user interaction beyond script console (.e., opens viewer). words, nice development get overview data, final analysis script.","code":"\ndf1a$subno <- factor(df1a$subno)\ndf1a$response <- factor(df1a$response, levels = c(\"reject\", \"accept\"))\ndf1a$condition <- factor(df1a$condition, \n                         levels = c(40.2, 20.2, 40.4, 20.4), \n                         labels = c(\"-$20/+$40\", \"-$20/+$20\", \"-$40/+$40\", \"-$40/+$20\"))\nstr(df1a)\n#> 'data.frame':    22912 obs. of  6 variables:\n#>  $ subno    : Factor w/ 358 levels \"5\",\"8\",\"13\",\"24\",..: 2 2 2 2 2 2 2 2 2 2 ...\n#>  $ loss     : int  6 6 6 6 6 6 6 6 8 8 ...\n#>  $ gain     : int  6 8 10 12 14 16 18 20 6 8 ...\n#>  $ response : Factor w/ 2 levels \"reject\",\"accept\": 2 2 2 2 2 2 2 2 1 2 ...\n#>  $ condition: Factor w/ 4 levels \"-$20/+$40\",\"-$20/+$20\",..: 2 2 2 2 2 2 2 2 2 2 ...\n#>  $ resp     : int  1 1 1 1 1 1 1 1 0 1 ...\nhead(df1a)\n#>   subno loss gain response condition resp\n#> 1     8    6    6   accept -$20/+$20    1\n#> 2     8    6    8   accept -$20/+$20    1\n#> 3     8    6   10   accept -$20/+$20    1\n#> 4     8    6   12   accept -$20/+$20    1\n#> 5     8    6   14   accept -$20/+$20    1\n#> 6     8    6   16   accept -$20/+$20    1"},{"path":"short-introduction-to-r-and-the-tidyverse.html","id":"the-tidyverse","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.4 The tidyverse","text":"popular alternative base R tidyverse (Wickham et al. 2019), selection packages curated large parts developed RStudio company. mastermind behind tidyverse Hadley Wickham, RStudio chief scientist maybe one person can considered R superstar.27 core tidyverse packages introduced , dplyr ggplot2, developments (even though many contributed packages well).easiest way access tidyverse first installing tidyverse package CRAN using install.packages(). Note good idea run install.packages() console put R script. reason want execute install.packages() per R installation updating R every time run script., can load tidyverse packages . something top pretty much scripts creating.loading packages common produces status messages console (.e., packages , don’t). example, tidyverse lists package versions loaded (“attached”) packages lists function conflicts; , cases tidyverse function masks previous loaded function name. include messages show normal nothing worry , generally later book (get repetitive). However, note exact version numbers packages shown may differ version numbers see (e.g., packages might update meanwhile).core tidyverse packages (descriptions taken adapted official websites):tibble: modern version data.frametibble: modern version data.framereadr: Reading data , RStudio way.readr: Reading data , RStudio way.Data wrangling magrittr, tidyr, dplyr: Coherent set functions tidying, transforming, working rectangular data. Supersedes many base R functions makes common problems easy.Data wrangling magrittr, tidyr, dplyr: Coherent set functions tidying, transforming, working rectangular data. Supersedes many base R functions makes common problems easy.ggplot2: System data visualization. disucssed next chapter.ggplot2: System data visualization. disucssed next chapter.purr broom: Advanced modelling tidyverse tidymodels discussed .purr broom: Advanced modelling tidyverse tidymodels discussed .following provide short introduction core components tidyverse needed book. comprehensive introduction provided Wickham Grolemund book “R Data Science” available freely http://r4ds..co.nz. get good grip tidyverse, highly recommend working chapters 1 21, better 1 25 (chapters “R Data Science” lot shorter chapters present book).","code":"\ninstall.packages(\"tidyverse\")\nlibrary(\"tidyverse\")\n#> -- Attaching packages ------------------- tidyverse 1.3.1 --\n#> v ggplot2 3.3.5     v purrr   0.3.4\n#> v tibble  3.1.4     v dplyr   1.0.7\n#> v tidyr   1.1.3     v stringr 1.4.0\n#> v readr   2.0.1     v forcats 0.5.1\n#> -- Conflicts ---------------------- tidyverse_conflicts() --\n#> x dplyr::filter() masks stats::filter()\n#> x dplyr::lag()    masks stats::lag()"},{"path":"short-introduction-to-r-and-the-tidyverse.html","id":"tibble-and-readr","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.4.1 tibble and readr","text":"back bone tidyverse tibble, tbl_df (’s class name R), “modern reimagining data.frame, keeping time proven effective, throwing ” (quote tibble documentation). main difference data.frame tibble daily use tibbles try print rows columns thus overwhelm console just entering name data console. feature alone enough prefer tibbles data.frames. working analysis one often wants quick look data see happened easiest way often just enter name data console. data.frames regularly clutters console tibbles .convert data.frame tibble can use as_tibble() function. Let us existing data.frame Walasek Stewart (2015), df1a.output tibble can see two things. First, output shows first 6 rows. Compare happens typing df1a console print many rows (see column names). Second, data type column shown column names. another feature makes tibbles convenient everyday use. directly see example subno Factor (<fct>) .alternative way creating tibble instead standard data.frame loading data using tidyverse specific read functions readr package. Remember used base R read function read.csv(), one handful base R data read functions read.table() (reading data separated spaces) read.delim() (reading tab separated data). readr offers similar set functions main differences instead using . function name use _ instead data.frame return tibble. case csv file, use read_csv() instead read.csv(). Note following code chunks going override existing tbl1a object clutter work space.can see output read_csv() returns tibble, also message showing column specification (.e., data type column). message actually call cols() function copied modified change column specification reading data. example, wanted change subno directly :something similar also response condition, one issue function defining Factors within readr, col_factor(), allows level argument labels argument (see ?cols). get factor structure , need use factor() function similar way , show .Another feature readr compared corresponding base R functions bit restrictive cases. , case data expected particular read function, readr fails often base R. Whereas can appear annoying programming, benefit one learns problems data early compared late. words, readr function likely base R functions ensure data looks like expect look like. data likely look like expect look like, code less likely produce incorrect results.Therefore, addition using read_csv() instead read.csv(), often better use read_table() instead read.table() space separated data (read_table2() data format bit sloppy), read_tsv() instead read.delim() tab separated data, read_delim() want specify data delimiter.","code":"\ntbl1a <- as_tibble(df1a)\ntbl1a\n#> # A tibble: 22,912 x 6\n#>   subno  loss  gain response condition  resp\n#>   <fct> <int> <int> <fct>    <fct>     <int>\n#> 1 8         6     6 accept   -$20/+$20     1\n#> 2 8         6     8 accept   -$20/+$20     1\n#> 3 8         6    10 accept   -$20/+$20     1\n#> 4 8         6    12 accept   -$20/+$20     1\n#> 5 8         6    14 accept   -$20/+$20     1\n#> 6 8         6    16 accept   -$20/+$20     1\n#> # ... with 22,906 more rows\ntbl1a <- read_csv(\"data/ws2015_exp1a.csv\")\n#> Rows: 22912 Columns: 6\n#> -- Column specification ------------------------------------\n#> Delimiter: \",\"\n#> chr (1): response\n#> dbl (5): subno, loss, gain, condition, resp\n#> \n#> i Use `spec()` to retrieve the full column specification for this data.\n#> i Specify the column types or set `show_col_types = FALSE` to quiet this message.\ntbl1a\n#> # A tibble: 22,912 x 6\n#>   subno  loss  gain response condition  resp\n#>   <dbl> <dbl> <dbl> <chr>        <dbl> <dbl>\n#> 1     8     6     6 accept        20.2     1\n#> 2     8     6     8 accept        20.2     1\n#> 3     8     6    10 accept        20.2     1\n#> 4     8     6    12 accept        20.2     1\n#> 5     8     6    14 accept        20.2     1\n#> 6     8     6    16 accept        20.2     1\n#> # ... with 22,906 more rows\ntbl1a <- read_csv(\"data/ws2015_exp1a.csv\", \n                  col_types = cols(\n                    subno = col_factor(),\n                    loss = col_double(),\n                    gain = col_double(),\n                    response = col_character(),\n                    condition = col_double(),\n                    resp = col_double()\n                  ))\ntbl1a\n#> # A tibble: 22,912 x 6\n#>   subno  loss  gain response condition  resp\n#>   <fct> <dbl> <dbl> <chr>        <dbl> <dbl>\n#> 1 8         6     6 accept        20.2     1\n#> 2 8         6     8 accept        20.2     1\n#> 3 8         6    10 accept        20.2     1\n#> 4 8         6    12 accept        20.2     1\n#> 5 8         6    14 accept        20.2     1\n#> 6 8         6    16 accept        20.2     1\n#> # ... with 22,906 more rows"},{"path":"short-introduction-to-r-and-the-tidyverse.html","id":"the-pipe","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.4.2 The Pipe: %>%","text":"One coolest features time novel features tidyverse pipe, name %>% operator.28 understand great, makes sense begin taking step back thinking bit functions evaluation order R.One important features R can use result (output) one operation use input another operation. example, remember loss column data Walasek Stewart (2015) shows potential loss lotteries, losses range $6 $40. Now imagine reason want know midpoint minimum maximum loss ($23 present case). One way calculate taking mean minimum maximum. get R need two steps: get minimum maximum potential loss vector losses taking mean minimum maximum. can get minimum maximum vector using range() function mean obtained using mean() function. R vector potential losses, tbl1a$loss, :code two step calculation explicitly saves results first step temporary variable called tmp. also combine steps one passing results first step mean() function:first get minimum maximum (.e., range) pass mean() function. mean function last operation want outermost call bit. words, R code use pipe, code executed inside outside. can make difficult chain many operations without need create temporary variables first bit.contrast executing code innermost outermost, pipe allows us chain operations left right. can start innermost call pipe next function using %>%. example, using pipe following:shows typical pipe workflow. start data, present case tbl1a$loss vector. data piped first operation, range() function. output call piped next operation, mean() function. end, get exactly output get code easier read. One feature pipe makes code particularly readable can start new line step chain.pipe also works like regular R operation, can easily save result whole chain operation new object later use. example:sum , base R order operations innermost operation outermost operation. contrast, pipe can chain (pipe) operations left right. following image exemplifies using screenshot slide Andrew Heiss. top part see operations needs execute order finally leave house using base R (.e., starting innermost wakeup() function). coding style without pipe makes easy see happens generally difficult read. lower part shows operations using pipe can see much easier see logic waking , getting bed, forth.\nFigure 3.1: Comparison execution order without pipe (upper part) pipe (lower part). : https://evalsp21.classes.andrewheiss.com/projects/01_lab/slides/01_lab.html#116\nfollowing sections see powerful piping can . Especially pipe combined dplyr functions introduced next, can quite lot neat things relatively easily.Let us end section two points important. First, typing pipe (.e., typing characters %>%) annoying. RStudio need . Instead, keyboard short cut , Ctrl/Cmd + Shift + M (.e., Windows/Linux Ctrl + Shift + M Mac Cmd + Shift + M). highly recommend get used using short cut. one short cuts RStudio use regularly.29Second, base R now pipe, |>. works similar tidyverse pipe (.e., can many situations used instead tidyverse pipe) can used without loading packages. using reason adapted tidyverse pipe base R pipe seem many reason prefer one tiydverse already loaded. see |> instead %>% somewhere, assume pretty much exact thing.","code":"\n# step 1: get minimum and maximum and save in temporary variables\ntmp <- range(tbl1a$loss)\ntmp # print minimum and maximum, to check everything is okay\n#> [1]  6 40\n#step 2: calculate mean of minimum and maximum:\nmean(tmp)\n#> [1] 23\nmean(range(tbl1a$loss))\n#> [1] 23\ntbl1a$loss %>% \n  range() %>% \n  mean()\n#> [1] 23\nloss_midpoint <- tbl1a$loss %>%\n  range() %>% \n  mean()\nloss_midpoint\n#> [1] 23"},{"path":"short-introduction-to-r-and-the-tidyverse.html","id":"dplyr","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.4.3 dplyr","text":"Whereas back bone tidyverse tibble pipe, package significantly improves ones workflow dplyr. core dplyr five functions combine common data manipulation operations. quote official documentation: “dplyr grammar data manipulation, providing consistent set verbs help solve common data manipulation challenges.” five verbs/functions :mutate() adds new variables changes variables function existing variablesselect() picks variables based namesfilter() picks cases based valuessummarise() reduces multiple values summaryarrange() changes ordering rowsA great introduction overview functionality functions provided official getting started page, well, somewhat briefly.One important part dplyr functions developed work pipe can chained. Furthermore, dplyr functions work called non-standard evaluation. means can refer variable names data.frame/tibble working dplyr functions without enclosing quotes.","code":""},{"path":"short-introduction-to-r-and-the-tidyverse.html","id":"mutate","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.4.3.1 mutate()","text":"Let us show behaviour replicating call convert variables data factors. start reading data . pipe freshly read data mutate() function convert three variables factors done save object, tbl1a. get overview object.Compared code changed variables factors, changes:block code variables converted factors wrapped mutate() call piped data, tbl1a.call converts one variable factor, use = sign assignment operator <-. reason operations performed within mutate() call. generally use assignment operator <- outside function calls (try replace = <- see horrible consequences ).first two calls changing variable ended ,. reason inside one mutate() call tell mutate yet done, end one operation ,.Instead passing full vector factor() , example df1a$subno, can refer variable, subno, name (without quotes) directly (meant non-standard evaluation). reason working within context data piped mutate() call.","code":"\ntbl1a <- read_csv(\"data/ws2015_exp1a.csv\")\ntbl1a <- tbl1a %>% \n  mutate(\n    subno = factor(subno),\n    response = factor(response, levels = c(\"reject\", \"accept\")),\n    condition = factor(condition, \n                        levels = c(40.2, 20.2, 40.4, 20.4), \n                        labels = c(\"-$20/+$40\", \"-$20/+$20\", \"-$40/+$40\", \"-$40/+$20\"))\n  )\ntbl1a\n#> # A tibble: 22,912 x 6\n#>   subno  loss  gain response condition  resp\n#>   <fct> <dbl> <dbl> <fct>    <fct>     <dbl>\n#> 1 8         6     6 accept   -$20/+$20     1\n#> 2 8         6     8 accept   -$20/+$20     1\n#> 3 8         6    10 accept   -$20/+$20     1\n#> 4 8         6    12 accept   -$20/+$20     1\n#> 5 8         6    14 accept   -$20/+$20     1\n#> 6 8         6    16 accept   -$20/+$20     1\n#> # ... with 22,906 more rows"},{"path":"short-introduction-to-r-and-the-tidyverse.html","id":"summarise","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.4.3.2 summarise()","text":"second important function dplyr summarise() name suggests creates summaries. means usually reducing number rows data, often data reduced one row. example, use summarise() get average probability lotteries accepted across conditions lotteries. just need take overall mean resp variable.see summarise also returns tibble return values differs quite bit one base R (e.g., mean(tbl1a$resp)). can also see summarise() works structurally quite similarly mutate(). summarise() call can create new variables using name = operation syntax. can also create multiple summary statistics separating using ,. can also add comments code usual using #, long right side ,.","code":"\ntbl1a %>% \n  summarise(mean_acc = mean(resp))\n#> # A tibble: 1 x 1\n#>   mean_acc\n#>      <dbl>\n#> 1    0.381\ntbl1a %>% \n  summarise(\n    mean_acc = mean(resp),\n    sd_acc = sd(resp),  ## sd() returns the standard deviation\n    mean_pot_loss = mean(loss),\n    mean_pot_gain = mean(gain)\n  )\n#> # A tibble: 1 x 4\n#>   mean_acc sd_acc mean_pot_loss mean_pot_gain\n#>      <dbl>  <dbl>         <dbl>         <dbl>\n#> 1    0.381  0.486          19.5          19.2"},{"path":"short-introduction-to-r-and-the-tidyverse.html","id":"filter-select-and-arrange","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.4.3.3 filter(), select(), and arrange()","text":"discussed , one key ways using tidyverse chaining different operations together. example, first select \"-$20/+$40\" condition calculate summaries condition. pipe first filter() function pipe results summarise function:Note filter() requires use == operator = operator. reason want set something equal something, done using =, want check equality, done using ==. filter() also allows chain multiple comparisons together, either separating using , & (.e., logical ). one multiple conditions needs hold can use | (logical ).filter() also function removing observations tidyverse. done writing filter retains observations ones want remove. can also use logical operator, ! inverts logical vector, unequal operator, !=.example, discussed actually interested data Walasek Stewart (2015) responses symmetric lotteries potential loss equal potential gain. select filter():equivalently:, use .equal() function show two objects . function can compare arbitrary R objects returns TRUE two objects .One problem filter used symmetric lotteries occur conditions. following code, shows symmetric lotteries condition, demonstrates .particular, output shows asymmetric conditions, -$20/+$40 -$40/+$20, three symmetric lotteries, whereas two symmetric conditions quite lot . However, three symmetric lotteries appear asymmetric conditions, 12-12, 16-16, 20-20, appear conditions.focussing can build better filter, let us first see code exactly . first filter symmetric lotteries. data retain select() loss, gain, condition columns. use unique() function retain unique rows. specific ordering think conditions (expressed order factor levels) sort tibble along condition variable using arrange(). Finally, per default tibble prints rows use print(n = Inf) tibble prints rows (.e., row infinity). better understand code, encouraged run line line see output intermediate step (get output subset lines codes, select %>% end last line running ).can select symmetric lotteries appear conditions? clearly multiple possible filters . one easiest comes mind combine filter selects symmetric lotteries filter selects lotteries potential loss either 12, 16, 20. latter introduce new operator, %% operator. returns TRUE element vector matches element another vector. example case, want retain lotteries potential loss c(12, 16, 20). shown . also include code shows now set symmetric lotteries conditions.","code":"\ntbl1a %>%\n  filter(condition == \"-$20/+$40\") %>% \n  summarise(\n    mean_acc = mean(resp),\n    sd_acc = sd(resp),\n    mean_pot_loss = mean(loss),\n    mean_pot_gain = mean(gain)\n  )\n#> # A tibble: 1 x 4\n#>   mean_acc sd_acc mean_pot_loss mean_pot_gain\n#>      <dbl>  <dbl>         <dbl>         <dbl>\n#> 1    0.514  0.500            13            26\nsymm1 <- tbl1a %>%\n  filter(loss == gain)\nsymm1\n#> # A tibble: 1,989 x 6\n#>   subno  loss  gain response condition  resp\n#>   <fct> <dbl> <dbl> <fct>    <fct>     <dbl>\n#> 1 8         6     6 accept   -$20/+$20     1\n#> 2 8         8     8 accept   -$20/+$20     1\n#> 3 8        10    10 accept   -$20/+$20     1\n#> 4 8        12    12 accept   -$20/+$20     1\n#> 5 8        14    14 accept   -$20/+$20     1\n#> 6 8        16    16 accept   -$20/+$20     1\n#> # ... with 1,983 more rows\nsymm2 <- tbl1a %>%\n  filter(!(loss != gain))\nall.equal(symm1, symm2)\n#> [1] TRUE\ntbl1a %>%\n  filter(loss == gain) %>% \n  select(loss, gain, condition) %>% \n  unique() %>% \n  arrange(condition) %>% \n  print(n = Inf)\n#> # A tibble: 22 x 3\n#>     loss  gain condition\n#>    <dbl> <dbl> <fct>    \n#>  1    12    12 -$20/+$40\n#>  2    16    16 -$20/+$40\n#>  3    20    20 -$20/+$40\n#>  4     6     6 -$20/+$20\n#>  5     8     8 -$20/+$20\n#>  6    10    10 -$20/+$20\n#>  7    12    12 -$20/+$20\n#>  8    14    14 -$20/+$20\n#>  9    16    16 -$20/+$20\n#> 10    18    18 -$20/+$20\n#> 11    20    20 -$20/+$20\n#> 12    12    12 -$40/+$40\n#> 13    16    16 -$40/+$40\n#> 14    20    20 -$40/+$40\n#> 15    24    24 -$40/+$40\n#> 16    28    28 -$40/+$40\n#> 17    32    32 -$40/+$40\n#> 18    36    36 -$40/+$40\n#> 19    40    40 -$40/+$40\n#> 20    12    12 -$40/+$20\n#> 21    16    16 -$40/+$20\n#> 22    20    20 -$40/+$20\ntbl1a %>%\n  filter(loss == gain, loss %in% c(12, 16, 20)) %>% \n  select(loss, gain, condition) %>% \n  unique() %>% \n  arrange(condition) \n#> # A tibble: 12 x 3\n#>    loss  gain condition\n#>   <dbl> <dbl> <fct>    \n#> 1    12    12 -$20/+$40\n#> 2    16    16 -$20/+$40\n#> 3    20    20 -$20/+$40\n#> 4    12    12 -$20/+$20\n#> 5    16    16 -$20/+$20\n#> 6    20    20 -$20/+$20\n#> # ... with 6 more rows"},{"path":"short-introduction-to-r-and-the-tidyverse.html","id":"group-by","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.4.3.4 group_by()","text":"might remember main hypothesis Walasek Stewart (2015) absolute value lottery, relative rank, matters whether participants think lottery attractive. main evidence hypothesis proportion symmetric lotteries accepted differs across four conditions. calculate tiydverse?apply know far way split data four subsets using filter, apply summarise() subsets. Let us exemplary splitting data two subsets, one \"-\\$20/+\\$40\" condition shows loss aversion one “-$40/+$20” condition shows loss seeking. combine filter filter selecting symmetric lotteries. make logic easier, separate two steps separate filter() calls (also combine one).results show previously discussed pattern. Participants -$20/+$40 condition around 50% less likely accept symmetric lotteries participants -$40/+$20. Whereas great, code bit verbose clunky. example, first filter() call summarise() call identical parts. question : better way get results?course rhetorical question answer yes, better way get results. answer group_by() function, function gives dplyr full power. use group_by() need pass one multiple (categorical) grouping variables. group_by() creates grouped tibble consequence dplyr verbs applied grouped tibble applied group separately. Let’s show example see means practice:can see output now contains average proportion accept symmetric lotteries four conditions separately. instead first splitting data calculating summary statistics (: mean resp column) split hand, group_by() exactly internally.words, whenever categorical variable data can use group_by() perform operations conditionally across levels categorical variable. Given ubiquity categorical variables, least experiments experimental factor one, data sets also usually least one categorical variable, huge time effort saver.whereas shown summarise(), probably common use case, also works dplyr verbs results can depend rows, mutate() arrange().example , passed one variable group_by(), condition. can also pass multiple variables. perform operations conditional combination variables. example, selected exactly lotteries three symmetric lotteries across conditions. group_by() easy get average accept proportion separately symmetric lotteries per condition:Note pretty much result obtained passing one gain/loss variable pair group_by() . Feel free try .results straight forward interpret show consistent pattern across conditions. symmetric conditions difference acceptance rate seems small. However, asymmetric conditions see somewhat noticeable differences suggest 16-16 lottery may somewhat lower acceptance rates 12-12 20-20 lottery. However, descriptive results without additional inferential statistical evidence, much possible small differences within conditions reflects pure noise taken seriously.moving next function, one thing discuss. shown output , summarise() still returns grouped tibble case one calls group_by() initially one variable. can seen status message “summarise() grouped output …” output still shows # Groups: .... means operations result still performed grouped. remove grouping created group_by() use ungroup() function shown next.","code":"\n## \"loss aversion\" condition:\ntbl1a %>%\n  filter(loss == gain, loss %in% c(12, 16, 20)) %>% \n  filter(condition == \"-$20/+$40\") %>% \n  summarise(mean_acc = mean(resp))\n#> # A tibble: 1 x 1\n#>   mean_acc\n#>      <dbl>\n#> 1    0.167\n\n## \"loss seeking\" condition:\ntbl1a %>%\n  filter(loss == gain, loss %in% c(12, 16, 20)) %>% \n  filter(condition == \"-$40/+$20\") %>% \n  summarise(mean_acc = mean(resp))\n#> # A tibble: 1 x 1\n#>   mean_acc\n#>      <dbl>\n#> 1    0.670\ntbl1a %>%\n  filter(loss == gain, loss %in% c(12, 16, 20)) %>% \n  group_by(condition) %>% \n  summarise(mean_acc = mean(resp))\n#> # A tibble: 4 x 2\n#>   condition mean_acc\n#>   <fct>        <dbl>\n#> 1 -$20/+$40    0.167\n#> 2 -$20/+$20    0.462\n#> 3 -$40/+$40    0.437\n#> 4 -$40/+$20    0.670\ntbl1a %>%\n  filter(loss == gain, loss %in% c(12, 16, 20)) %>% \n  group_by(condition, loss, gain) %>% \n  summarise(mean_acc = mean(resp))\n#> `summarise()` has grouped output by 'condition', 'loss'. You can override using the `.groups` argument.\n#> # A tibble: 12 x 4\n#> # Groups:   condition, loss [12]\n#>   condition  loss  gain mean_acc\n#>   <fct>     <dbl> <dbl>    <dbl>\n#> 1 -$20/+$40    12    12    0.190\n#> 2 -$20/+$40    16    16    0.119\n#> 3 -$20/+$40    20    20    0.190\n#> 4 -$20/+$20    12    12    0.469\n#> 5 -$20/+$20    16    16    0.458\n#> 6 -$20/+$20    20    20    0.458\n#> # ... with 6 more rows\ntbl1a %>%\n  filter(loss == gain, loss %in% c(12, 16, 20)) %>% \n  group_by(condition, loss, gain) %>% \n  summarise(mean_acc = mean(resp)) %>% \n  ungroup()\n#> `summarise()` has grouped output by 'condition', 'loss'. You can override using the `.groups` argument.\n#> # A tibble: 12 x 4\n#>   condition  loss  gain mean_acc\n#>   <fct>     <dbl> <dbl>    <dbl>\n#> 1 -$20/+$40    12    12    0.190\n#> 2 -$20/+$40    16    16    0.119\n#> 3 -$20/+$40    20    20    0.190\n#> 4 -$20/+$20    12    12    0.469\n#> 5 -$20/+$20    16    16    0.458\n#> 6 -$20/+$20    20    20    0.458\n#> # ... with 6 more rows"},{"path":"short-introduction-to-r-and-the-tidyverse.html","id":"counting","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.4.3.5 Counting","text":"Another important functionality dplyr makes easy count observations using count() n() functions. Counting one operations seems somewhat unspectacular beginning, extremely important understanding data. example, important question might data many participants per condition many trials every participants worked .Let us begin looking number trials per participant (condition). two almost equivalent ways . Either using group_by() summarise(n = n()) using count():can see results pretty much , difference group_by() retains grouped tibble whereas count() .can also see number trials participants shown output 64. exactly number trials expect description study Section 1.2.2. However, far sure really holds participants. expect case looks like handful participants shown, really hold everyone?analysing data questions regularly come ones mind. One assumptions data looks like, one sure assumptions really hold. , example, data collection finish participants partial data . reality analysing actual data often assumptions one believes hold actually hold data. many things can go wrong go wrong data collection one important step analysis verify ones assumptions. Checking ones assumptions can provide assurances results biased mismatch assumptions reality.can check everyone exactly 64 trials? code can done per group extending another group_by() summarise() combination. also allows us count, step, number participants condition. Let show :exactly happening ? first three lines (.e., including first summarise()) create summarised tibble already shown previous code. tibble one row per participants n column given number trials participant. group tibble condition ensure following results shown separately conditions (really necessary, still grouped condition, makes code clearer). perform another summarise() summarises summarised tibble level condition. summarise() perform two operations:create new variable n_participants, contains number observations (.e., number participants) summarised tibble. done using n() function counting observations.create new logical variable all_64, TRUE observations/participants within one condition number trials 64. fully understand call creating variable, read inside outside. innermost call n == 64. creates logical variable observation summarised tibble TRUE number trials (.e., variable n) 64 FALSE otherwise. pass vector () function reduces logical vector single logical variable. () returns TRUE elements vector TRUE FALSE otherwise.results thus show two pieces information per condition. First, can see number participants per condition 84 96. Second, conditions participants exactly 64 trials (.e., all_64 vector TRUE conditions).","code":"\ntbl1a %>% \n  group_by(condition, subno) %>% \n  summarise(n = n())\n#> `summarise()` has grouped output by 'condition'. You can override using the `.groups` argument.\n#> # A tibble: 358 x 3\n#> # Groups:   condition [4]\n#>   condition subno     n\n#>   <fct>     <fct> <int>\n#> 1 -$20/+$40 5        64\n#> 2 -$20/+$40 13       64\n#> 3 -$20/+$40 53       64\n#> 4 -$20/+$40 61       64\n#> 5 -$20/+$40 73       64\n#> 6 -$20/+$40 85       64\n#> # ... with 352 more rows\n\ntbl1a %>% \n  count(condition, subno)\n#> # A tibble: 358 x 3\n#>   condition subno     n\n#>   <fct>     <fct> <int>\n#> 1 -$20/+$40 5        64\n#> 2 -$20/+$40 13       64\n#> 3 -$20/+$40 53       64\n#> 4 -$20/+$40 61       64\n#> 5 -$20/+$40 73       64\n#> 6 -$20/+$40 85       64\n#> # ... with 352 more rows\ntbl1a %>% \n  group_by(condition, subno) %>% \n  summarise(n = n()) %>% \n  group_by(condition) %>% \n  summarise(\n    n_participants = n(),\n    all_64 = all(n == 64)\n    )\n#> `summarise()` has grouped output by 'condition'. You can override using the `.groups` argument.\n#> # A tibble: 4 x 3\n#>   condition n_participants all_64\n#>   <fct>              <int> <lgl> \n#> 1 -$20/+$40             84 TRUE  \n#> 2 -$20/+$20             96 TRUE  \n#> 3 -$40/+$40             87 TRUE  \n#> 4 -$40/+$20             91 TRUE"},{"path":"short-introduction-to-r-and-the-tidyverse.html","id":"dplyr-summary","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.4.3.6 dplyr Summary","text":"seen dplyr powerful tool working data.frames tibbles. five verbs, mutate(), summarise(), filter(), select(), arrange(), relatively intuitive syntax solving one common data manipulation problem. commonly used verbs mutate() calculating new variables changing existing variables observations summarise() calculates new variables reduces number observations (usually one). One common use summarise() n() function allows counting number observations. Furthermore, filter() tool one needs selecting subsets observations.full power dplyr comes combining functions group_by(), performs operations conditional one several grouping variables. example, one important analysis steps calculating summary statistics level factor combination factor levels. can easily done dplyr using powerful group_by(factor1, factor2) %>% summarise(...) combination. addition getting informative summaries, also used regularly check whether data matches assumptions one ones data.","code":""},{"path":"short-introduction-to-r-and-the-tidyverse.html","id":"summary-2","chapter":"3 Short Introduction to R and the tidyverse","heading":"3.5 Summary","text":"chapter begun list resources provide general introduction R. , provided brief example read data changes variables factors base R. main functions introduced read.csv(), str(), factor(), head(). factor() function also shown can use different optional arguments, level labels.yet done, maybe now good time check internal R help system factor() see believe arguments matches actual description. either type ?factor prompt press F1 key cursor factor() function RStudio. brings help page (exists R function) can read description argument. able read understand R help pages one important R programming skills one usually develops time (.e., don’t demoralised understand little help page now).short recap base R, provided brief introduction tidyverse packages, tibble, readr, dplyr, introduced tidyverse pipe, %>%. shown back bone tidyverse tibble, modern variant base R data.frame. tibble also returned using readr function reading data, read_csv().pipe seen changes order can write commands. Whereas base R order inside outside (innermost function first), pipe can write pipe function left write. pipe makes easy chain different operations together, otherwise difficult base R (.e., lead unreadable code). often avoids need create intermediate variables generally leads readable code.Finally, introduced functionality dplyr. dplyr important functions three “verbs,” mutate(), summarise(), filter(), solve one specific data manipulation problem. mutate() creates/changes variables within current data, summarise() creates new variables summarising current data, filter() selects observations (.e., reduces number rows). powerful feature dplyr verbs can combined group_by(), performs operations conditional one multiple categorical variables.help get going R tidyverse particular, next page offers number examples exercises.","code":""},{"path":"data-visualisation-with-ggplot2.html","id":"data-visualisation-with-ggplot2","chapter":"4 Data Visualisation with ggplot2","heading":"4 Data Visualisation with ggplot2","text":"perhaps prominent member tidyverse, technically also pre-dates tidyverse several years, ggplot2, system declaratively creating graphics, based book “Grammar Graphics” (Wilkinson 1999).understand ggplot2 works, makes sense contrast base R graphics engine around plot() function. plot() creating plot done drawing individual graphical elements points() lines(). functions generally accept individual vectors data points.contrast, ggplot2 requires data passed either data.frame tibble. hand, “provide data, tell ggplot2 map variables aesthetics, graphical primitives use, takes care details.” (quote official documentation).","code":"\nlibrary(\"tidyverse\")\n#> -- Attaching packages ------------------- tidyverse 1.3.1 --\n#> v ggplot2 3.3.5     v purrr   0.3.4\n#> v tibble  3.1.4     v dplyr   1.0.7\n#> v tidyr   1.1.3     v stringr 1.4.0\n#> v readr   2.0.1     v forcats 0.5.1\n#> -- Conflicts ---------------------- tidyverse_conflicts() --\n#> x dplyr::filter() masks stats::filter()\n#> x dplyr::lag()    masks stats::lag()\ntbl1a <- read_csv(\"data/ws2015_exp1a.csv\")\n#> Rows: 22912 Columns: 6\n#> -- Column specification ------------------------------------\n#> Delimiter: \",\"\n#> chr (1): response\n#> dbl (5): subno, loss, gain, condition, resp\n#> \n#> i Use `spec()` to retrieve the full column specification for this data.\n#> i Specify the column types or set `show_col_types = FALSE` to quiet this message.\ntbl1a <- tbl1a %>% \n  mutate(\n    subno = factor(subno),\n    response = factor(response, levels = c(\"reject\", \"accept\")),\n    condition = factor(condition, \n                        levels = c(40.2, 20.2, 40.4, 20.4), \n                        labels = c(\"-$20/+$40\", \"-$20/+$20\", \"-$40/+$40\", \"-$40/+$20\"))\n  )\nglimpse(tbl1a)\n#> Rows: 22,912\n#> Columns: 6\n#> $ subno     <fct> 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8~\n#> $ loss      <dbl> 6, 6, 6, 6, 6, 6, 6, 6, 8, 8, 8, 8, 8, 8~\n#> $ gain      <dbl> 6, 8, 10, 12, 14, 16, 18, 20, 6, 8, 10, ~\n#> $ response  <fct> accept, accept, accept, accept, accept, ~\n#> $ condition <fct> -$20/+$20, -$20/+$20, -$20/+$20, -$20/+$~\n#> $ resp      <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1~"},{"path":"data-visualisation-with-ggplot2.html","id":"first-ggplot2-example","chapter":"4 Data Visualisation with ggplot2","heading":"4.1 First ggplot2 Example","text":"means practice first thing think variables data want show two axes? decided consider graphical elements, called geoms ggplot2 terminology, want use show data. Let us exemplify first example Walasek Stewart (2015) data.One intuition probability lottery accepted related size potential gain, average variables (.e., potential loss condition). One way investigate calculate average acceptance probability possible gain plot x-axis average accept probability y-axis. , first summarise data way.Looking returned tibble already shows intuition probably , let us plot data get better look. , pass ggplot() function shown next.plot shows clear relationship mean acceptance rate lottery potential gain. Let us now describe call detail:create plot ggplot2 generally want call ggplot() function first argument data want plot. data first argument, can also directly pipe ggplot() function shown .second argument aes() function used map variables data onto aesthetics. call consider two aesthetics, x y axes. two probably important aesthetics, also use aesthetics linetype, size, shape data points. general, aesthetic feature plot changes function values variable.passing data specifying aesthetics, close ggplot() call (.e., close opening parenthesis) add arguments, passed function calls, plot using +.important set arguments pass geoms (geometric objects). pass geom_point() indicate want plot points data.Another important thing ggplot2 use one geom, multiple. example, add line top connecting data points:One thing note usually geom_line() requires use groups aesthetic. data points connected, present case, group 1 (.e., geom_line(aes(group = 1))). following equivalent present case, idiom one aware .","code":"\nlot_sum <- tbl1a %>% \n  group_by(gain) %>% \n  summarise(mean_acceptance = mean(resp))\nlot_sum\n#> # A tibble: 13 x 2\n#>    gain mean_acceptance\n#>   <dbl>           <dbl>\n#> 1     6          0.0943\n#> 2     8          0.138 \n#> 3    10          0.214 \n#> 4    12          0.222 \n#> 5    14          0.340 \n#> 6    16          0.326 \n#> # ... with 7 more rows\nggplot(lot_sum, aes(x = gain, y = mean_acceptance)) +\n  geom_point()\nggplot(lot_sum, aes(x = gain, y = mean_acceptance)) +\n  geom_point() +\n  geom_line()\nggplot(lot_sum, aes(x = gain, y = mean_acceptance)) +\n  geom_point() +\n  geom_line(aes(group = 1))"},{"path":"data-visualisation-with-ggplot2.html","id":"two-continuous-variables","chapter":"4 Data Visualisation with ggplot2","heading":"4.2 Two Continuous Variables","text":"examples show simple case plotting two continuous variables, dependent variable shown y-axis independent variable (plot) x-axis. Let us now consider cases plotting two continuous variables, extending data looking .plots averaged possible potential losses looking effect gain average acceptance rates. now plot case consider individual lottery – , unique combination potential gain loss – one data point (.e., average potential losses one gain). Additionally, reduce influence condition manipulation consider two symmetric condition range losses equal range gains. Let us begin analysis preparing corresponding data.first plot, begin call change data passed ggplot() function.plot difficult interpret. see lotteries low acceptance rates 0 around 0.2 well another group acceptance rates around 0.3 0.85. plot look larger potential gain associated larger mean acceptance rates.One possibility differing visual impressions might overlap data points near 0; , evidence -plotting. means current plot differentiate one multiple data points share approximately x y coordinates.One way allow differentiation see whether potentially -plotting alpha blending (.e., choosing alpha < 1). Alpha blending computer graphics effect creates visual impression semi transparency one helpful techniques learning -plotting. means alpha blending overlapping points appear darker whereas non-overlapping points . example, can set alpha = 0.25 geom_point() shown . wondering alpha = 0.25 value? answer trial--error. just tried value (usually start value 0.5) tried saw plot looked good .can see plot, evidence -plotting. Especially mean acceptance rates near 0 (especially low values potential gains), see several points clearly darker rest data points. However, plot still clear. example, still difficult judge many points darker points.One possibility improve figure introducing random jitter plotted points using geom_jitter(). Given points discrete x-axis positions (.e., even whole numbers act potentially gains) makes sense add small amount jitter x-axis. done specifying width argument geom_jitter(), requires number specifying amount (horizontal) jitter units x-axis. Trial--error led conclude width = 0.4 produces appealing result.30 jitter x-axis points retain exact y-axis positions still shown around original x-axis position.important thing know using geom_jitter() due randomness used add jitter, plot look slightly different every time created (.e., code executed). Try times see mean. feel free try different values amount jitter happens remove width = 0.4 geom_jitter() call. can also try see happens add value height (e.g., height = 0.05).plot combines jitter alpha blending makes easier see see effect potential gain mean acceptance rates averaging losses. many data points low mean acceptance rates left side plot many data points medium high acceptance rates right side plot. imagine taking mean points can imagine monotonically increasing mean acceptance rate.One question plot leaves open question see qualitatively different pattern mean acceptance rates lotteries. low larger? elucidate question going create new factor, ev, separates expected value lottery (.e., loss plus gain) three bins: expected value = 0 (.e., symmetric lotteries), negative expected value (.e., potential loss larger potential gain), positive expected value (potential loss smaller potential gain).create ev variable, use another dplyr function, case_when(). case_when() vectorised variant multiple branching (.e., -else). allows create new variable based multiple logical conditions convenient way. argument case_when() consists logical statement, ~ operator (call tilde-operator), return value returned case logical condition true. three logical cases mapped onto one label describing sign expected value. also convert ev variable Factor using factor().can use new ev variable print points different colour depending value ev variable. just need map ev variable colour aesthetic aes() call get interesting plot.whereas plot already looks interesting, uses green red colours can difficult see individuals colour blindness (, even colour blindness mild). Therefore, can make plot nicer two regards: (1) using somewhat nicer theme default removes grey background (e.g., favourite theme_bw()) (2) can use colour blind friendly colour palette. use ggthemes::scale_colour_colorblind() (ggthemes:: just means use function ggthemes package without loading explicitly beforehand). gives us appealing interesting plot.plot shows expected value negative, participants highly unlikely accept lottery. expected value 0 (.e., symmetric lotteries) acceptance rates start around 0.5 small potentials losses gains decrease bit increasing loss/gain (just predicted loss aversion, see Section 1.2.1). Finally, lotteries positive expected value (.e., average win money) acceptance rate 0.5. Just one expect.done yet data? yet. one trick can use make plot even informative. can map loss variable onto size data points. , just add size = loss aes() call. allows us understand pattern.can now see something surprising. potential loss small, now indicated small data point, average acceptance rate high. higher potential gain one specific potential loss (.e., point size) larger mean acceptance rate. x-axis position data, can also nicely see size based ordering: small losses largest mean acceptance rates large losses smallest. sum, even though plot reveals pattern surprising, provides comprehensive overview data Walasek Stewart (2015) (least two symmetric conditions).","code":"\nlot_sum2 <- tbl1a %>% \n  filter(condition %in% c(\"-$20/+$20\", \"-$40/+$40\")) %>% \n  group_by(loss, gain) %>% \n  summarise(mean_acceptance = mean(resp))\n#> `summarise()` has grouped output by 'loss'. You can override using the `.groups` argument.\nlot_sum2\n#> # A tibble: 119 x 3\n#> # Groups:   loss [13]\n#>    loss  gain mean_acceptance\n#>   <dbl> <dbl>           <dbl>\n#> 1     6     6           0.510\n#> 2     6     8           0.615\n#> 3     6    10           0.729\n#> 4     6    12           0.844\n#> 5     6    14           0.833\n#> 6     6    16           0.906\n#> # ... with 113 more rows\nggplot(lot_sum2, aes(x = gain, y = mean_acceptance)) +\n  geom_point()\nggplot(lot_sum2, aes(x = gain, y = mean_acceptance)) +\n  geom_point(alpha = 0.25)\nggplot(lot_sum2, aes(x = gain, y = mean_acceptance)) +\n  geom_jitter(width = 0.4, alpha = 0.25)\nlot_sum2 <- lot_sum2 %>% \n  mutate(ev = factor(case_when(\n    gain == loss ~ \"EV = 0\",\n    gain < loss ~ \"EV negative\",\n    gain > loss ~ \"EV positive\"\n  )))\nlot_sum2\n#> # A tibble: 119 x 4\n#> # Groups:   loss [13]\n#>    loss  gain mean_acceptance ev         \n#>   <dbl> <dbl>           <dbl> <fct>      \n#> 1     6     6           0.510 EV = 0     \n#> 2     6     8           0.615 EV positive\n#> 3     6    10           0.729 EV positive\n#> 4     6    12           0.844 EV positive\n#> 5     6    14           0.833 EV positive\n#> 6     6    16           0.906 EV positive\n#> # ... with 113 more rows\nggplot(lot_sum2, aes(x = gain, y = mean_acceptance, colour = ev)) +\n  geom_jitter(width = 0.25, alpha = 0.5)\nggplot(lot_sum2, aes(x = gain, y = mean_acceptance, colour = ev)) +\n  geom_jitter(width = 0.25, alpha = 0.5) +\n  ggthemes::scale_colour_colorblind() +\n  theme_bw()\nggplot(lot_sum2, aes(x = gain, y = mean_acceptance, colour = ev, size = loss)) +\n  geom_jitter(width = 0.25, alpha = 0.5) +\n  ggthemes::scale_colour_colorblind() +\n  theme_bw()"},{"path":"data-visualisation-with-ggplot2.html","id":"changing-ggplot2-theme-for-r-session","chapter":"4 Data Visualisation with ggplot2","heading":"4.3 Changing ggplot2 Theme for R Session","text":"far looked two continuous variables seen mapping different variables different aesthetics changing arguments aesthetics, can create appealing figures. trying situation potentially common experimental data, situation one continuous dependent variable one categorical independent variable.producing data, going something continue throughout book. change default theme (.e., theme automatically used plots one R session unless different theme specifically requested) theme without grey background. call, also make changes larger axes labels remove unnecessary grid lines feel makes plots generally nicer.Note, global options affect plots created executing command within R session (.e., reset default theme restart R session).","code":"\ntheme_set(theme_bw(base_size = 15) + \n            theme(legend.position=\"bottom\", \n                  panel.grid.major.x = element_blank()))"},{"path":"data-visualisation-with-ggplot2.html","id":"one-continuous-and-one-categorical-variable","chapter":"4 Data Visualisation with ggplot2","heading":"4.4 One Continuous and one Categorical Variable","text":"creating plots one continuous dependent variable one categorical independent variable, need create corresponding data . plots aggregated data Walasek Stewart (2015) level individual lotteries (.e., unique combination gains losses). Whereas made sense plots , ignored different participants (.e., aggregated across participants), usually considered major source noise. words, plots focussed differences across lotteries ignored differences across participants. However, discussed , different people can number different reasons, know people differ quite bit; expect individual differences. means usually good idea show distribution responses participants, provides visual impression level noise data.Therefore, instead aggregating across participants create new data set first calculate one score per participant condition. , plot distribution across participants condition. begin creating data lotteries. Whereas might informative plot substantively (e.g., compared plotting shared symmetric lotteries), instructive present case main goal try different plotting options. later section produce plot shared lotteries interesting research question loss aversion.output shows individual mean acceptance rates already first participants one condition quite variable. also shows returned tibble still grouped (See message Groups ...):. perform operations , instead just plotting, might necessary add ungroup() call pipe remove grouping performing operations. However, just pass ggplot() function, necessary .","code":"\npart_sum <- tbl1a %>%\n  group_by(condition, subno) %>%   # aggregate for both, condition and subno\n  summarise(mean_acc = mean(resp)) \n#> `summarise()` has grouped output by 'condition'. You can override using the `.groups` argument.\npart_sum\n#> # A tibble: 358 x 3\n#> # Groups:   condition [4]\n#>   condition subno mean_acc\n#>   <fct>     <fct>    <dbl>\n#> 1 -$20/+$40 5        0.562\n#> 2 -$20/+$40 13       0.5  \n#> 3 -$20/+$40 53       0.344\n#> 4 -$20/+$40 61       0.484\n#> 5 -$20/+$40 73       0.266\n#> 6 -$20/+$40 85       0.375\n#> # ... with 352 more rows"},{"path":"data-visualisation-with-ggplot2.html","id":"displaying-all-data-points","chapter":"4 Data Visualisation with ggplot2","heading":"4.4.1 Displaying All Data Points","text":"plot general convention, plot dependent variable, average acceptance rate per participant, y-axis independent variable, experimental condition, x-axis. begin plotting individual data point points using point geom (.e., geom_point()).resulting plot bit difficult interpret. can see seems differences conditions (e.g., less large mean acceptance rates two right-conditions), difficult judge points top – , whether -plotting . , difficult adequately perceive distribution points.can begin add jitter points using geom_jitter() also use alpha blending (.e., alpha = 0.25). contrast , let us start without specifying amount horizontal (.e., width) vertical (.e., height) jitter. case, geom_jitter() automatically adds horizontal vertical jitter.resulting plot visually appealing. amount horizontal jitter large making difficult see x-axis position (.e., experimental condition) point belongs . improve visual impression better choose amount jitter hand. , need choose amount jitter x-axis units pass width argument. problem x-axis shows categorical variable easily identifiable unit. solution problem categorical variable x-axis position shown one whole number starting 1 difference 1 levels categorical variable. means, plot “actual” x-axis positions 1, 2, 3, 4, corresponding four factor levels. Thus, let us try improve plot specifying amount horizontal jitter, width = 0.2 (found trial--error) height (implies height = 0).plot provides better overview distribution. can example clearly see -$40/+$20 (.e., right-) condition majority data points 0.25.","code":"\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_point()\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_jitter(alpha = 0.25)\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_jitter(width = 0.2, alpha = 0.25)"},{"path":"data-visualisation-with-ggplot2.html","id":"bee-swarm-plot","chapter":"4 Data Visualisation with ggplot2","heading":"4.4.2 Bee Swarm Plot","text":"alternative geom_jitter() case avoids overplotting offered two geoms ggbeeswarm package, geom_beeswarm() geom_quasirandom(). choice geoms usually depends amount data points overlap points. structure provided geom_beeswarm() seems slightly nicer, please try geom_quasirandom() well (name suggests uses randomness well). use geoms, first need load ggbeeswarm package (yet installed, first install.packages()).geom_beeswarm() produce bee swarm plot; , plot -plotted points displaced shown adjacent next (case x-axis). allows interesting conclusions data.example, -$20/+$40 condition distribution wide data points along whole range almost part particular bump shape cluster data. contrast, condition less clearly identifiable centre cluster data points (around 0.5 two symmetric distributions near bottom -$40/+$20 condition).","code":"\nlibrary(\"ggbeeswarm\")\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_beeswarm()"},{"path":"data-visualisation-with-ggplot2.html","id":"box-plot","chapter":"4 Data Visualisation with ggplot2","heading":"4.4.3 Box Plot","text":"Alternatives visualising distribution data points geoms plot data point, summary distribution. popular box plot violin plot, geom_boxplot() geom_violin().Box plots, also known box whiskers plot, visualise distribution several summary statistics plus showing potential “outliers.” provides compact summary data can also used case many data points. Let us show looks data, explaining visual elements detail.can see box plot consists box, thicker line somewhere inside, well two whiskers. cases, see additional data points outside whiskers, “outliers.” thick line inside box measure central tendency data (.e., measure centre distribution). shows median, common choice box plots (sometimes box plots also show mean measure central tendency). median value separates cuts distribution points lower upper half. Technically, means median 50% quantile, data points 50% data points smaller 50% points larger.31The upper lower bound box, two hinges, show 25% 75% quantiles data. data point either 25% 75% data points smaller. consequence box encompasses 50% data points. allows one judge data . example, line earlier visual impression -$20/+$40 spans whole range scale, box encompassing 50% data also spans roughly 50% scale. conditions clearly defined centre, box noticeable smaller.two whiskers ends box (hinges) extend hinges largest value 1.5 times size box (.e., 75% quantile - 25% quantile, known interquartile range). idea whiskers represent, sense, typical range distribution data points outside typical range can considered un-typical. un-typical data points often called “outliers,” unclear sense terminology appropriate. example, present case quite data points per condition (80 90) seem unlikely observe cases look somewhat different cases (.e., un-typical).issue “outliers” identified box plot trivial. “outlier” genuine response participant simply removing seems appropriate. specifically, omitting data fit idea data can seen instance data fabrication. hand, single response extraordinary influence results (e.g., single data point far away others drives observed effect) also problematic. usually want results represent data overall just single observation. Thus, many situations analysis, deal “outliers” depends specific context situation. generally reasonable strategy see qualitative pattern results changes whether extreme data points included . , shows extraordinary influence results.","code":"\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_boxplot()"},{"path":"data-visualisation-with-ggplot2.html","id":"violin-plots","chapter":"4 Data Visualisation with ggplot2","heading":"4.4.4 Violin Plots","text":"Another possibility visualising distribution shapes summary statistics. One popular way violin plot. create violin plot ggplot2, simply need change geom geom_violin().clear plot, violin plot makes easy see whether distribution relatively flat, like -$20/+$40 condition, one multiple “bumps,” call “modi” statistical terminology (singular “modus”). distributions three conditions called unimodal. three conditions, modi located expect , given previous plots, around 0.5 symmetric conditions near bottom -$40/+$20 condition (course surprising case, violin plot just different visualisation compared bee swarm plot ). distribution two “bumps” call bimodal, appear justified distributions shown .One way increase amount information shown violin plot adding lines correspond different quantiles distribution. example, add 25%, 50% (.e., median), 75% quantile box plot follows:","code":"\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_violin()\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75))"},{"path":"data-visualisation-with-ggplot2.html","id":"plotting-the-mean","chapter":"4 Data Visualisation with ggplot2","heading":"4.4.5 Plotting The Mean","text":"already comfortable scientific literature might know plots one one continuous dependent variable one categorical independent variable quite common. Given structure basic experiment, probably common type data visualisation scientific literature. already know , might also noticed plots created far diverge quite considerably plots find literature. particular, many published plots show visualisation complete distribution data, done , focus one particular summary statistic data, condition means.example, one common plot type -called dynamite plot means shown terms bar graph. addition, error bar added mean measure uncertainty mean (provide thorough explanation error bars later chapters). example present data looks like following.can seen, bar graph together error bar looks like cartoon dynamite plunger, plot got name. Even though can created R, hiding code learn create .can maybe read lines, big fan type plot many people interested statistics (e.g., Vanderbilt biostatistics department). problem dynamite plots? Clearly issue plot shows means. remember discussion results particular study previous Chapter 1, discussion results always focussed conditions means (e.g., mean acceptance rate symmetric lotteries 21% -$20/+$40 condition, 71% -$40/+$20 condition). clearly means important. see coming chapters, statistical reason makes sense say mean generally important summary statistic data.problem dynamite plot shows mean, shows mean.32 One mean can come many different data distributions. best example problem shown “datasaurus plot” (Matejka Fitzmaurice 2017). Note plot shows two continuous variables continuous plus categorical variable, nicely exemplifies point made . datasaurus plot shows data dramatically changes shape, cycles 13 qualitatively different patterns one dinosaur, maintaining means plus summary statistics (two decimal points) x-axis y-axis.\nFigure 4.1: “datasaurus dozen.” Two variables maintain summary statistics dramatic changing shape data points. Justin Matejka George Fitzmaurice: https://www.autodesk.com/research/publications/-stats-different-graphs\ndatasaurus impressively shows one just focuses means summary statistics, high chance one can miss important features data. Consequently, better approach plotting ones data combining visualisation full distribution data mean. approach using throughout book.Another benefit showing visualisation full distribution data instead just means provides realistic picture noise data. Remember said one goal statistics help us distinguishing signal noise. focus means, even includes measure uncertainty means error bars, can forget actual level noise data. ignoring actual level noise, may inclined draw overly optimistic conclusions data. problem overly optimistic conclusions less likely correct true appropriate conclusions.Combining visualisation distribution data mean ggplot2 can achieved adding call stat_summary() function plot. allows us add summary geom plot, without additional arguments adds mean data plus error bar. Let us add call bee swarm plot see happens.can see just lead dramatically different plot. noticeable difference status message telling us default summary function, mean_se(), used want (can ignore message). problem plot add black summary point top black data points. One way improve plot plotting data points background semi-transparent manner using alpha = 0.2. alternative change geom_beeswarm() plot, stat_summary() plot (e.g., passing colour = \"red\"), left exercise reader.plot now makes easy see , full distribution data well mean (plus error bar). said , ignore technical meaning error bars later now accept represent uncertainty means.plot allows us interesting conclusion regards distribution mean. three conditions clearly visible mode (“bump”; .e., -$20/+$40 condition), mode differs noticeably mean. reason distributions asymmetric around mode long tail towards one end distribution. long tail pushes mean away mode towards tail can seen . means statistical analysis focuses mean (statistical analyses discussed book) provides sense imperfect picture data. Whereas mean course represents average values (defined way) might always represent typical value distribution (want understand typical value one near mode).message previous paragraph case rather asymmetric distribution focussing mean summary statistic necessarily wrong, wrong. message something else: focus mean show actual data, miss nuances real data usually . course interesting see happens mean, important summary statistic (absence additional information best prediction new observation data mean, even asymmetric distribution). However, mean typical distribution asymmetric shape, important result addition whatever happens mean. fully understand evidence provided data set, understand data fully, including ’s level noise, peculiarities, nuances. Therefore, always plot full distribution plotting mean.symmetric lotteries shared across conditions separately participant condition.","code":"#> Warning: `fun.y` is deprecated. Use `fun` instead.\n#> Warning: Ignoring unknown aesthetics: width\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_beeswarm() +\n  stat_summary()\n#> No summary function supplied, defaulting to `mean_se()`\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_beeswarm(alpha = 0.2) +\n  stat_summary()\n#> No summary function supplied, defaulting to `mean_se()`\npart_sum <- tbl1a %>%\n  filter(loss == gain, loss %in% c(12, 16, 20)) %>% \n  group_by(condition, subno) %>% \n  summarise(mean_acc = mean(resp)) %>% \n  ungroup()\n#> `summarise()` has grouped output by 'condition'. You can override using the `.groups` argument.\npart_sum\n#> # A tibble: 358 x 3\n#>   condition subno mean_acc\n#>   <fct>     <fct>    <dbl>\n#> 1 -$20/+$40 5        0    \n#> 2 -$20/+$40 13       0    \n#> 3 -$20/+$40 53       0    \n#> 4 -$20/+$40 61       0    \n#> 5 -$20/+$40 73       0.667\n#> 6 -$20/+$40 85       0.667\n#> # ... with 352 more rows\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_point()\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_jitter(width = 0.2, height = 0.1, alpha = 0.25)\nggplot(part_sum, aes(x = condition, y = mean_acc)) +\n  geom_jitter(width = 0.2, height = 0.1, alpha = 0.25)"},{"path":"data-visualisation-with-ggplot2.html","id":"one-continuous-variables-histograms","chapter":"4 Data Visualisation with ggplot2","heading":"4.4.6 One Continuous Variables: Histograms","text":"","code":""},{"path":"data-visualisation-with-ggplot2.html","id":"cheat-sheets","chapter":"4 Data Visualisation with ggplot2","heading":"4.5 Cheat Sheets","text":"","code":""},{"path":"standard1.html","id":"standard1","chapter":"5 The Standard Approach for One Independent Variable","heading":"5 The Standard Approach for One Independent Variable","text":"chapter introducing standard statistical approach analysing experimental data one independent variable (.e., one factor). simple case study comparing two experimental conditions one dependent variable. exemplify standard approach design using recent straightforward experiment.","code":""},{"path":"standard1.html","id":"ex:urry","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.1 Example Data: Note Taking Experiment","text":"Heather Urry 87 undergraduate graduate students (Urry et al. 2021) (yes, 87 students co-authors!) compared effectiveness taking notes laptop versus longhand (.e., pen paper) learning lectures. 142 participants (differed 88 authors) first viewed one several 15 minutes lectures (TED talks) asked take notes either laptop pen paper. proper experiment, participants randomly assigned either laptop (\\(N = 68\\)) longhand condition (\\(N = 74\\)). 30 minutes delay, participants quizzed content lecture. answers participant independently rated several raters (agreed strongly ) using standardised scoring key resulting one memory score per participant representing percentage information remembered ranging 0 (= memory) 100 (= perfect memory).33 Figure 5.1 shows memory scores across note taking conditions.\nFigure 5.1: Distribution memory scores Urry et al. (2021) across two note taking conditions.\nFigure 5.1, black point shows memory score one participant full distribution data visible. shape distribution also shown via violin plot (.e., black outline around points) added three lines representing three summary statistics data. top bottom lines 75% quantile, 50% quantile (.e., median), 25% quantile. red points show mean associated error bars show standard error mean34. see two means quite similar, although mean laptop condition slightly larger, 2.0 points (mean laptop = 68.2, mean longhand = 66.2).","code":""},{"path":"standard1.html","id":"the-logic-of-inferential-statistics","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.2 The Logic of Inferential Statistics","text":"previous paragraph provide us descriptive statistics describing results experiment Urry et al. (2021): memory difference 2.0 scale 0 100 laptop longhand condition sample 142 participants. However, researchers usually primarily interested happens sample. like know results generalises population sample drawn. case, like know whether memory difference note taking laptop longhand format students (roughly population sample drawn ).Going beyond present sample goal inferential statistics. different inferential statistical approaches, focussing popular one, null hypothesis significance testing (NHST). describe NHST thoroughly next chapter, already introduce main ideas . NHST, question generalising sample population two different possible true states world: either difference conditions means population (.e., mean difference sample due chance) difference condition means population. decide possibilities, set statistical model data. statistical model allows us assess difference population – call possible state world null hypothesis. specifically, statistical model allows us test compatible data null hypothesis difference. test null hypothesis proceeds follows: (1) assume state world difference population means true. (2) Based assumption calculate likely observe difference large one observed sample. (3) probability observing difference large one small, take evidence null hypothesis difference true – reject null hypothesis. (4) act difference population. case (.e., reject null hypothesis act difference), say experimental manipulation effect.clear description, logic underlying inferential statistics using NHST trivial. make clearer, let us apply logic example data. want know whether observed mean memory difference note taking laptop note taking long hand format sample generalises population students take note either formats. , set statistical model data. model allows us test compatible data null hypothesis mean memory difference population. Specifically, model allows us calculate probability obtaining memory difference large one observed assuming mean memory difference population. data incompatible null hypothesis (.e., unlikely obtain memory difference large one observed null hypothesis true), reject null hypothesis mean memory difference population. reject null hypothesis act mean memory difference population. words, act type note taking effect memory lectures population.Even though logic NHST necessarily intuitive, clearly helpful researchers. running experiment really like know difference observed experiment (.e., sample) meaningful sense generalises population sampled participants. almost every actual experiment mean difference condition (.e., extreme unlikely conditions exactly mean). Thus, pretty much always face question. NHST allows us test whether observed difference compatible world difference. unlikely, decide (.e., act ) difference.can see spelling logic detail quite inferential steps make get want. design experiments goal mind find difference different experimental conditions. However, test directly. Instead, test compatibility data converse actually interested – null hypothesis effect. test “fails” (.e., shows data likely incompatible null hypothesis) make two inferential steps. First reject null hypothesis act difference. inferential steps necessitated logically. means inferences based NHST alone never extremely strong.NHST de facto standard procedure inferential statistics across empirical sciences (.e., psychology related disciplines). Understanding logic NHST enable understand majority empirical papers also allow apply inferential statistics research. Nevertheless, exist long list popular criticisms NHST (e.g., Rozeboom 1960; Meehl 1978; Cohen 1994; Nickerson 2000; Wagenmakers 2007). discuss criticisms detail later chapters, now important realise NHST allow test, prove, whether mean difference population. thing NHST calculates probability compatible data null hypothesis. probability low necessarily mean difference. Likewise, probability high necessarily mean difference. inferences draw based NHST results probabilistic (.e., can false). important rule interpreting results NHST humble. NHST never “proves” “confirms” anything. Instead NHST results “suggest” “indicate” certain interpretations. -interpret results, stay instead stay humble interpretations, unlikely fall prey common (often justifiable) criticisms NHST framework.","code":""},{"path":"standard1.html","id":"the-basic-statistical-model","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.3 The Basic Statistical Model","text":"apply inferential statistics NHST framework data, begin setting statistical model data. statistical model attempts explain (predict) observed values dependent variable (DV) independent variable (IV).35 experimental context means predicting observed outcome, DV, experimental manipulation, IV.basic statistical model partitions observed DV three parts : overall mean, reasons become clear later called intercept, effect IV, part data explained model, residuals. summing three parts together, result observed value. mathematical form can express (used reading mathematical expressions, point end equation simply full stop ends sentence mathematical meaning.) someone without mathematics background , know equations text often intimidating immediately useful. Consequently, moving makes sense go equation detail. Furthermore, statistical analyses discussed book applications Equation (5.1). equation forms foundation statistical analysis experimental data thus understanding unlock analyses discussed book. Consequently, makes sense spend time .Let us consider variables Equation (5.1) detail. , also consider many different possible values variable can take .36 following Figure, variant Figure 5.1, shows elements graphically explain text just .\nFigure 5.2: Data Urry et al. (2021) showing overall mean (intercept, blue dotted line), condition specific effects (difference dashed red lines condition means blue line), residuals (grey lines condition means data points).\n\\(\\text{DV}\\): dependent variable, DV, observed values, one observation/participant. example data 142 black data points shown Figure 5.2. Thus, statistical model tries explain individually observed values.\\(\\text{DV}\\): dependent variable, DV, observed values, one observation/participant. example data 142 black data points shown Figure 5.2. Thus, statistical model tries explain individually observed values.\\(\\text{intercept}\\): intercept represents overall mean. Consequently, one intercept (.e., intercept observation). experimental designs define mean condition means. example data intercept (68.2 + 66.2) / 2 = 67.2 shown blue dotted line Figure 5.2.\\(\\text{intercept}\\): intercept represents overall mean. Consequently, one intercept (.e., intercept observation). experimental designs define mean condition means. example data intercept (68.2 + 66.2) / 2 = 67.2 shown blue dotted line Figure 5.2.\\(\\text{IV-effect}\\): IV-effect represents effect independent variable define difference condition means intercept (.e., deviation condition means intercept). Thus, always many different IV-effects conditions. example data two conditions, two different IV-effects, magnitude differ sign, 1.0 laptop condition -1.0 longhand condition. add values intercept, get condition means. discuss , relevant part answering statistical question interest. Figure 5.2, red dashed line (red points) show condition means, thus condition effects differences blue line red lines.\\(\\text{IV-effect}\\): IV-effect represents effect independent variable define difference condition means intercept (.e., deviation condition means intercept). Thus, always many different IV-effects conditions. example data two conditions, two different IV-effects, magnitude differ sign, 1.0 laptop condition -1.0 longhand condition. add values intercept, get condition means. discuss , relevant part answering statistical question interest. Figure 5.2, red dashed line (red points) show condition means, thus condition effects differences blue line red lines.\\(\\text{residual}\\): residuals idiosyncratic aspects data left unexplained statistical model. model predicts condition means (.e., intercepts plus independent variable), deviations individual observations condition means. Thus, DV, many residuals values DV. Figure 5.2, residuals shown grey lines condition means data point. information (variability data) model explain.\\(\\text{residual}\\): residuals idiosyncratic aspects data left unexplained statistical model. model predicts condition means (.e., intercepts plus independent variable), deviations individual observations condition means. Thus, DV, many residuals values DV. Figure 5.2, residuals shown grey lines condition means data point. information (variability data) model explain.","code":""},{"path":"standard1.html","id":"model-predictions","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.3.1 Model Predictions","text":"simplification Equation (5.1) makes clearer statistical model predicts obtained ignore residuals moment. reminder, residuals part data remains unexplained. words, represent idiosyncratic parts data independent manipulation (e.g., participants better memory others independent took notes). remains statistical model ignore idiosyncratic aspects predictions based IV. case experimental data, IV experimental condition. Thus, statistical model actually predicts means experimental conditions. can formalise asHere, hat symbol (\\(\\hat{}\\)) means predicted value. Thus contrast actual DV , predicted DV equation.performing statistical analyses sometimes help remind oneself standard statistical model predicts condition means. generally make predictions individual participants consider factors part model. predict, interested , condition means.","code":""},{"path":"standard1.html","id":"statistical-model-for-the-example-data","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.3.2 Statistical Model for the Example Data","text":"Let us take look first six participants values variables basic statistical model get better understanding Equation (5.1).first three columns show data. pid participant identifier (id) column. often case real data, ids missing (3 7) various reasons (e.g., potential participants interested study received id, finish start experiment) first 6 rows already go pid = 8. condition tells us note taking condition participant overall memory score scale 0 100 serves DV statistical model (.e., left-hand side Equation (5.1)).four right columns contain values variables right-hand side Equation (5.1), intercept, iv_effect, residual. addition, prediction column shows left-hand side Equation (5.2). described , every observation (.e., row) idiosyncratic DV residual. also see values share one intercept, IV-effect condition specific. consequence, prediction column (sum intercept iv-effect) also two values, one condition. Finally, can see sum three values right-hand side Equation (5.1) equals observed value DV. example, consider pid = 4. enter values Equation (5.1) \\[\n50.0 = 67.2 + (-1) + (-16.2).\n\\]example data can also understand better residuals mean, difference observed value predicted value, \\(\\text{residual} = \\text{DV} - \\hat{\\text{DV}}\\). Consider pid = 4. \\[\n-16.2 = 50- 66.2.\n\\]can also see residual captures idiosyncratic aspects data explained condition means. example, participants – pid = 5 pid = 8 – large positive residuals indicating good memory independent note taking condition. Likewise, pid = 4 large negative residual indicating comparatively worse memory (independent note taking condition).","code":""},{"path":"standard1.html","id":"understanding-the-statistical-model","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.3.3 Understanding the Statistical Model","text":"Now described parts statistical model almost ready fit model interpret output. makes sense look parts individually try understand set statistical model way . Remember, goal evaluate whether effect experimental manipulation (.e., difference two note taking conditions) population data sampled. , set model partitions observed data three parts, intercept representing overall mean, condition specific effect (IV-effect) representing difference condition means intercept, residuals representing idiosyncratic part explained model. reason allows us zoom matters statistical question, condition specific effect. answer question difference conditions population, can now focus part model. overall level performance captured intercept residuals can (now) ignored question. Consequently, statistical test reported statistical test condition effect. Thus, reason setting statistical model way make easy get answer question interests us: effect note taking manipulation/conditions memory? answer question need consider condition effect.","code":""},{"path":"standard1.html","id":"estimating-the-statistical-model-in-r","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.4 Estimating the Statistical Model in R","text":"","code":""},{"path":"standard1.html","id":"package-and-data-setup","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.4.1 Package and Data Setup","text":"statistical analyses reported book generally use afex package (R-afex?) combination emmeans package (Lenth 2021). afex stands “analysis factorial experiments” simplifies many things want (full disclaimer: main developer afex). analyses can also performed different functions, often easiest use afex functions developed particularly cognitive behavioural researchers working experimental data. specifically, afex functions provide expected results experimental data sets ---box without need change settings (true corresponding non-afex functions). emmeans stands “estimated marginal means” package use statistical model estimated investigate results. afex emmeans fully integrated allows test practically hypotheses interest combination two packages straight forward manner. already introduce interplay two packages , next chapters showcase full power combination.also regular use functions tidyverse package (e.g., plotting). tidyverse collection packages developed mainly RStudio head data scientist Hadley Wickham. full introduction tidyverse beyond scope present book, interested readers encouraged read introductory book, Wickham Grolemund (2017), also available free online.begin analysis loading three packages first (use install.packages(c(\"afex\", \"emmeans\", \"tidyverse\")) case yet installed). also change default ggplot2 theme using theme_set() nicer one.next step loading data. made easy data Urry et al. (2021) part afex, name laptop_urry. can load data() function. also get overview variables data set using str(), returns structure data.frame.str function shows six variables, three already mentioned :pid: participant identifier, factor 142 levels, one participant.pid: participant identifier, factor 142 levels, one participant.condition: factor identifying note taking condition participant belongs , two levels, laptop longhand.condition: factor identifying note taking condition participant belongs , two levels, laptop longhand.talk: factor identifying TED talk participant saw, 5 level.talk: factor identifying TED talk participant saw, 5 level.overall: Numeric variable participants’ overall memory performance scale 0 (= memory) 100 (= perfect memory). variable called overall average two separate memory performance scores given .overall: Numeric variable participants’ overall memory performance scale 0 (= memory) 100 (= perfect memory). variable called overall average two separate memory performance scores given .factual: Numeric variable participants’ memory score factual questions (ignored chapter).factual: Numeric variable participants’ memory score factual questions (ignored chapter).conceptual: Numeric variable participants’ memory score conceptual questions (analysed next chapter).conceptual: Numeric variable participants’ memory score conceptual questions (analysed next chapter).","code":"\nlibrary(\"afex\")\nlibrary(\"emmeans\")\nlibrary(\"tidyverse\")\ntheme_set(theme_bw(base_size = 15) + \n            theme(legend.position=\"bottom\", \n                  panel.grid.major.x = element_blank()))\ndata(\"laptop_urry\")\nstr(laptop_urry)\n#> 'data.frame':    142 obs. of  6 variables:\n#>  $ pid       : Factor w/ 142 levels \"1\",\"2\",\"4\",\"5\",..: 1 2 3 4 5 6 7 8 9 10 ...\n#>  $ condition : Factor w/ 2 levels \"laptop\",\"longhand\": 1 2 2 1 2 2 1 2 2 1 ...\n#>  $ talk      : Factor w/ 5 levels \"algorithms\",\"ideas\",..: 4 4 2 5 1 3 5 2 5 4 ...\n#>  $ overall   : num  65.8 75.8 50 89 75.6 ...\n#>  $ factual   : num  61.7 68.3 33.3 85.7 69.2 ...\n#>  $ conceptual: num  70 83.3 66.7 92.3 82.1 ..."},{"path":"standard1.html","id":"estimating-the-statistical-model","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.4.2 Estimating the Statistical Model","text":"estimating basic statistical model using afex can use aov_car() function. next code snippet show example data, saving output object res1 .first argument aov_car() formula specifying statistical model, overall ~ condition + Error(pid). second argument identifies data.frame containing data (.e., variables appearing formula), laptop_urry. can also see calling aov_car() produces status message informing us contrasts set contr.sum IVs model. message shown information purposes can safely ignored (want contr.sum contrasts variables, default R behaviour message shown).formula R defined presence tilde-operator ~ main way specifying statistical models. allows specifying statistical models similar way mathematical formulation, specifically prediction equation statistical model, Equation (5.2). Therefore, formula provides comparatively intuitive approach specifying statistical model. left hand side ~ dependent variable, overall. right hand side variables want use predict dependent variable.present case, right-hand side consists two parts concatenated +, independent variable condition Error() term participant identifier variable pid. Thus, two difference formula used prediction Equation (5.2), formula misses explicit intercept specified Error() term missing Equation (5.2). Let us address two difference turn. intercept actually missing equation, implicitly included. specifically, intercept specified using 1 formula. However, unless intercept explicitly suppressed – can done including 0 formula (done good statistical reason ; .e., makes rarely sense) – always assumed part models. Consequently, including explicitly produces equivalent results. following code shows comparing previous result without explicit intercept, res1 aov_car call explicit intercept using .equal() function. function can used compare arbitrary R objects returns TRUE equal.Error() term mandatory part model formula using aov_car() used specify participant identifier variable (.e., pid case). simple example present one seems unnecessary, later book see requirement Error() term useful.looking results, let us quickly explain function specifying models called aov_car(). regular statistical model ones considered solely includes factors (.e., categorical variables) independent variables also known analysis variance, usually shortened ANOVA.37 basic R function ANOVA models simply called aov(). However, aov() cases return expected results types ANOVA models considered book (.e., situations aov() can return results considered inappropriate, even used carfeully). alternative aov() Anova() function package car (Fox Weisberg 2019) (car stands book title, “Companion Applied Regression”). Anova() always returns expected appropriate ANOVA results used correctly. However, calling Anova() requires least two function calls can become tricky complicated models discussed later chapters. aov_car() combines simplicity model specification aov() function appropriate statistical results Anova() function car package (.e., aov_car() calls Anova() internally).","code":"\nres1 <- aov_car(overall ~ condition + Error(pid), laptop_urry)\n#> Contrasts set to contr.sum for the following variables: condition\nres1b <- aov_car(overall ~ 1 + condition + Error(pid), laptop_urry)\n#> Contrasts set to contr.sum for the following variables: condition\nall.equal(res1, res1b)\n#> [1] TRUE"},{"path":"standard1.html","id":"interpreting-the-results","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.4.3 Interpreting the Results","text":"can now look results statistical model. , simply call object contains results res1 (get output calling print(res1) nice(res1)).default aov_car() output “Anova Table” see throughout book. can also see results table contains “Type 3 tests,” ignore now. option, Type 2 tests, produces results example data. get back meaning “type test” later chapters makes difference ignore part .next line results table reference information. see response variable, also know DV, overall, just intended.get table effects, case one row, effect condition. row contains information null hypothesis significance test (NHST) condition effect. important column output last column, p.value, \\(p\\)-value. \\(p\\)-value column main results NHST allows us judge compatibility data null hypothesis. probability obtaining difference extreme observed assuming null hypothesis difference true. see case \\(p\\)-value small, .47. Thus, data incompatible null hypothesis suggest memory difference note taking laptop longhand format lectures.general, researchers adopted significance level .05. means \\(p\\)-value smaller .05 treat evidence data incompatible null hypothesis. case say result “significant.” However, case result smaller .05 result “significant” (avoid saying “insignificant” \\(p\\)-value larger .05, “significant” technical term ). Thus, present case reject null hypothesis. present data therefore provide evidence observed difference two modes note taking generalises sample population according NHST.two important columns whose results generally need reported, df, stands “degrees freedom” (df), F. Understanding columns detail beyond scope present chapter, introduce briefly. two degrees freedom reported , first value, 1, numerator degree freedom. always given number conditions minus 1. present case, two conditions, laptop longhand, numerator df 2 - 1 = 1. second value denominator df, generally given number participants minus numerator df minus 1. 142 participants therefore 142 - 1 - 1 = 140. general, larger denominator df (.e., participants ) better can detect incompatibility null hypothesis (.e., easier get small \\(p\\)-values). \\(F\\)-value value expressing observed incompatibility data null hypothesis. \\(F \\leq 1\\), data compatible null hypothesis. \\(F > 1\\) data degree incompatible null hypothesis, larger values indicating incompatibility. \\(p\\)-value calculated df \\(F\\)-value. Consequently, results usually reported following way: \\(F(1, 140) = 0.52\\), \\(p = .471\\).next column important ges stands generalised eta-squared, using mathematical notation Greek letters, \\(\\eta^2_G\\). \\(\\eta^2_G\\) standardised effect size tells us something absolute magnitude observed effect (Olejnik Algina 2003; Bakeman 2005). specifically, \\(\\eta^2_G\\) supposed measure proportion variance DV can accounted specific factor IV model. example, present case condition effect supposed explain 0.4% variance performance. general, avoid standardised effect sizes \\(\\eta^2_G\\) instead report simple effect sizes. simple effect size expressed units measured DV. example, throughout chapter mentioned observed difference memory performance note taking conditions 2.0 scale 0 100. , difference 2.0 simple effect size. say effect sizes later, journal editors publishing guidelines require standardised effect sizes (statistically reasonable recommendation eyes) default output contains .Finally, default output contains MSE column, stands “mean squared errors.” column mainly included historical reasons. Traditionally, ANOVA models relatively easily calculated hand calculator based different variance terms (hence name, analysis variance). One term mean squared error , combination residual squared error, \\(F\\)-value can calculated. undergrad studies still learned calculate ANOVA hand, seems rather unnecessary nowadays. Hence, simply ignore column. Interested reader can find detailed explanation meaning MSE example Howell (2013) Baguley (2012).One thing note results table contain information intercept. However, discussed , intercept included model. reason omitting intercept default output generally primary interest. experimental research usually main interest effect independent variables, effect experimental manipulation. statistical model separates intercept (.e., overall mean) condition effect allows zoom relevant part. line , default output aov_car . Later chapters show can also get information intercept.Estimating statistical model aov_car() provides us inferential statistical results, null hypothesis tests IV-effects shown . get , just need call object containing results R prompt (e.g., calling res1 present case). However, can use results object also others parts statistical analyses, data visualisation follow-analyses.","code":"\nres1\n#> Anova Table (Type 3 tests)\n#> \n#> Response: overall\n#>      Effect     df    MSE    F  ges p.value\n#> 1 condition 1, 140 269.66 0.52 .004    .471\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1"},{"path":"standard1.html","id":"data-visualisation","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.4.4 Data Visualisation","text":"data visualisation can use afex function afex_plot() built top ggplot2 package. afex_plot() requires estimated model object (e.g., returned aov_car()) specifying factors model want plot. present case, one factor, condition, can choose one. Importantly, factors passed afex_plot() need passed character strings (.e., enclosed \"...\").\nFigure 5.3: afex_plot() figure data Urry et al. (2021)\nsimple call afex_plot() produces already rather good looking results figure combining individual-level data points (background grey) condition means (black). Individual data points background similar values displaced x-axis lie top . achieved package ggbeeswarm (needs installed : install.packages(\"ggbeeswarm\")). plot also per default shows 95% confidence intervals means, explain detail later chapter.afex_plot() returns ggplot2 plot object, can manipulate plot make nicer.example, code snippet first save plot object p1 call number ggplot2 function plot object alter plot appearance (ggplot2 graphical elements added plot using +). Function labs() used change axis labels, coord_cartesian() changes extent y-axis (.e., plot now show full possible range memory performance score), geom_line(aes(group = 1) adds line connecting two means. figure now used results report manuscript .","code":"\nafex_plot(res1, \"condition\")\np1 <- afex_plot(res1, \"condition\")\np1 + \n  labs(x = \"note taking condition\", y = \"memory performance (0 - 100)\") +\n  coord_cartesian(ylim = c(0, 100)) +\n  geom_line(aes(group = 1))"},{"path":"standard1.html","id":"follow-up-analysis","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.4.5 Follow-Up Analysis","text":"Follow-analysis refers inspection predicted condition means relationships. case single independent variable two levels (e.g., laptop versus longhand) much investigate regard. can nevertheless show general procedure. follow-analyses generally begin function emmeans() package emmeans (Lenth 2021). Function emmeans() returns estimated marginal means, slightly complicated way saying condition means, plus additional statistical information.Similarly afex_plot(), emmeans() requires estimated model object well specification factor model want get condition means:now focus estimates means column emmean ignore additional inferential statistical information columns SE upper.CL. can see reported means match means given text beginning chapter, 5.1.power emmeans provide condition means, also allows us perform calculation condition means. example, case factor two levels can easily calculate difference condition means simple effect size. , can save object returned emmeans() call pairs() function object gives us pairwise comparisons conditions means one present case (get results combining calls one: pairs(emmeans(res1, \"condition\"))):output shows mean difference 1.99 slightly differs 2.0 reported , slightly concerning. However, results reported rounded one decimal . present results, also get estimated difference 2.0 (explain code detail ):","code":"\nemmeans(res1, \"condition\")\n#>  condition emmean   SE  df lower.CL upper.CL\n#>  laptop      68.2 1.99 140     64.3     72.1\n#>  longhand    66.2 1.91 140     62.4     70.0\n#> \n#> Confidence level used: 0.95\nem1 <- emmeans(res1, \"condition\")\npairs(em1)\n#>  contrast          estimate   SE  df t.ratio p.value\n#>  laptop - longhand     1.99 2.76 140   0.722  0.4715\nem1 %>% \n  pairs() %>% \n  as.data.frame() %>% \n  format(digits = 1, nsmall = 1)\n#>            contrast estimate  SE    df t.ratio p.value\n#> 1 laptop - longhand      2.0 2.8 140.0     0.7     0.5"},{"path":"standard1.html","id":"summary-3","chapter":"5 The Standard Approach for One Independent Variable","heading":"5.5 Summary","text":"goal chapter introduce standard statistical approach analysing experimental data one independent variable two levels – experiment two conditions. Practically every time run experiment, observe mean difference dependent variable two conditions. example data Urry et al. (2021) memory difference 2.0 points two note taking conditions (laptop versus longhand) response scale 0 100.important statistical question whether evidence suggesting observed difference sample generalises population. sample participants experiment population refers possible participants sampled. Urry et al. (2021) population loosely described students taking notes maybe precisely undergraduate students research intensive (R1) US universities. question like get statistical answer : believe generally memory difference note taking laptop versus longhand? answer question need inferential statistics.inferential statistical approach using called null hypothesis significance testing NHST. However, NHST directly address question whether evidence difference population. Instead, NHST tests compatibility data null hypothesis – assumption difference condition population. important result NHST \\(p\\)-value. \\(p\\)-value measure compatibility data null hypothesis; probability obtaining results extreme observed assuming null hypothesis true. \\(p\\)-value smaller .05 reject null hypothesis difference. case decide evidence difference (although follow logical necessity).apply NHST data set statistical model observed partitions data three parts (Equation (5.1)): intercept representing overall mean, effect independent variable (.e., difference condition means intercept), residuals representing idiosyncratic aspects explained parts model. partitioning allows us zoom part data interested , effect independent variable, experimental manipulation.estimate statistical model data used function aov_car() afex package. aov_car() allows us specify statistical model using formula form dv ~ iv + Error(pid) (pid refers variable data participant identifier) mimicking mathematical specification statistical model. default output returns ANOVA table provides null hypothesis significance test iv, independent variable. returned table called ANOVA table statistical models contain factors called analysis variance ANOVA. present case, statistical model single factor, note taking condition, two levels, laptop versus longhand. returned ANOVA table, \\(p\\)-value experimental factor, additional inferential statistical information degrees freedom, df, \\(F\\)-value.can also use object returned aov_car() plotting using function afex_plot(). function produces plot combining individual-level data points condition means. provides comprehensive display data experiment. function returns ggplot2 object, plot can easily modified create figure can used results report.can also use object returned aov_car follow-analyses using emmeans. emmeans can easily obtain condition means (estimated marginal means) dependent variable. Based condition means can calculate observed effect size (.e., mean difference).Applying statistical model data Urry et al. (2021) showed non significant difference, \\(F(1, 140) = 0.52\\), \\(p = .471\\). suggests difference memory performance watching talk taking notes either laptop longhand format.","code":""},{"path":"case-study-1-more-results-from-note-taking-studies.html","id":"case-study-1-more-results-from-note-taking-studies","chapter":"6 Case Study 1: More Results from Note Taking Studies","heading":"6 Case Study 1: More Results from Note Taking Studies","text":"chapter apply learned previous chapter - analyse experimental data one experimental manipulation two conditions. , take look data Urry et al. (2021). Additionally, analyse data Mueller Oppenheimer (2014). study first published study investigating question note taking laptop longhand format basis Urry et al. (2021) planned study. data Mueller Oppenheimer (2014) perform full analysis starting reading data. addition performing statistical hypothesis test, calculate descriptive statistics.start analysis chapter way previous chapter, loading three packages generally use, afex, emmeans, tidyverse, set nicer ggplot2 theme. probably good idea restart R (unless, course, just starting R). RStudio can conveniently done menu clicking Session Restart R. R environments might need restart program. benefit restarting R create blank R session packages loaded objects exist workspace. blank sessions ensures , obtained set results, can recreate later using code. , blank R session avoids potential problems due analyses performed previous session still lingering. Restarting R generally done starting new analysis one completely done analysis. latter case, makes sense restart R rerun code one saved ones script ensure results replicate based code script (require additional code saved).","code":"\nlibrary(\"afex\")\n#> Loading required package: lme4\n#> Loading required package: Matrix\n#> ************\n#> Welcome to afex. For support visit: http://afex.singmann.science/\n#> - Functions for ANOVAs: aov_car(), aov_ez(), and aov_4()\n#> - Methods for calculating p-values with mixed(): 'S', 'KR', 'LRT', and 'PB'\n#> - 'afex_aov' and 'mixed' objects can be passed to emmeans() for follow-up tests\n#> - NEWS: emmeans() for ANOVA models now uses model = 'multivariate' as default.\n#> - Get and set global package options with: afex_options()\n#> - Set orthogonal sum-to-zero contrasts globally: set_sum_contrasts()\n#> - For example analyses see: browseVignettes(\"afex\")\n#> ************\n#> \n#> Attaching package: 'afex'\n#> The following object is masked from 'package:lme4':\n#> \n#>     lmer\nlibrary(\"emmeans\")\nlibrary(\"tidyverse\")\n#> -- Attaching packages ------------------- tidyverse 1.3.1 --\n#> v ggplot2 3.3.5     v purrr   0.3.4\n#> v tibble  3.1.4     v dplyr   1.0.7\n#> v tidyr   1.1.3     v stringr 1.4.0\n#> v readr   2.0.1     v forcats 0.5.1\n#> -- Conflicts ---------------------- tidyverse_conflicts() --\n#> x tidyr::expand() masks Matrix::expand()\n#> x dplyr::filter() masks stats::filter()\n#> x dplyr::lag()    masks stats::lag()\n#> x tidyr::pack()   masks Matrix::pack()\n#> x tidyr::unpack() masks Matrix::unpack()\ntheme_set(theme_bw(base_size = 15) + \n            theme(legend.position=\"bottom\", \n                  panel.grid.major.x = element_blank()))"},{"path":"case-study-1-more-results-from-note-taking-studies.html","id":"conceptual-memory-data-from-urry-et-al.-2021","chapter":"6 Case Study 1: More Results from Note Taking Studies","heading":"6.1 Conceptual Memory Data from Urry et al. (2021)","text":"quick reminder, Urry et al. (2021) showed participants short lectures (TED talks) video participants allowed take notes. One group participants, laptop condition, take notes laptop, whereas participants longhand condition take notes pen paper. lecture participants quizzed two aspects content lecture, factual questions conceptual questions. previous chapter analysed overall memory score average performance factual questions conceptual questions. , concerned memory performance conceptual questions. begin analysis loading data (part afex can loaded data()) getting overview variables using str():, participants identifier variable pid note taking condition variable condition. can also guess conceptual memory scores aptly name variable conceptual (unsure , also check documentation data ?laptop_urry).Usually, data sufficiently prepared (.e., performed sanity checks identified DV IV), first step analysis plotting data. done using ggplot2 directly. However, cases present one clear statistical model going estimate often bit less effort plot data afex_plot(). Thus, start estimating statistical model conceptual memory performance data Urry et al. (2021) save estimated model object mc_urry. , use aov_car() laptopt_urry data specify model using formula interface. DV considering conceptual, IV condition, participant identifier pid. Consequently, formula conceptual ~ condition + Error(pid). , looking inferential statistical results, use model object plot data using afex_plot.\nFigure 6.1: Conceptual memory scores Urry et al. (2021) across note taking conditions\ngoal behind beginning plotting data allows see whether data “looks alright.” , check whether features stand clear outliers unusual pattern data. case, try figure can find reason issue deal . , data looks alright, continue consider results significance test:ANOVA table reveals significance test effect condition significant \\(p = .319\\). Thus, line finding evidence difference overall memory performance, also evidence difference memory conceptual information.can also use emmeans see condition means (estimated marginal means). line Figure 6.1 (afex_plot internally also uses emmeans shows exactly means graphical form), memory score laptop condition descriptively around 3.5 points higher score longhand condition.moving next data set, let us consider report analysis research report. example write:shown Figure 6.1, participants’ conceptual memory scores (scale 0 100) descriptively slightly larger laptop condition compared longhand condition. analysed scores ANOVA one factor, note taking condition, two levels (laptop vs. longhand). effect note taking condition significant, \\(F(1, 140) = 1.00\\), \\(p = .319\\). indicates data provide evidence difference memory conceptual information based notes taken lectures.","code":"\ndata(\"laptop_urry\")\nstr(laptop_urry)\n#> 'data.frame':    142 obs. of  6 variables:\n#>  $ pid       : Factor w/ 142 levels \"1\",\"2\",\"4\",\"5\",..: 1 2 3 4 5 6 7 8 9 10 ...\n#>  $ condition : Factor w/ 2 levels \"laptop\",\"longhand\": 1 2 2 1 2 2 1 2 2 1 ...\n#>  $ talk      : Factor w/ 5 levels \"algorithms\",\"ideas\",..: 4 4 2 5 1 3 5 2 5 4 ...\n#>  $ overall   : num  65.8 75.8 50 89 75.6 ...\n#>  $ factual   : num  61.7 68.3 33.3 85.7 69.2 ...\n#>  $ conceptual: num  70 83.3 66.7 92.3 82.1 ...\nmc_urry <- aov_car(conceptual ~ condition + Error(pid), laptop_urry)\n#> Contrasts set to contr.sum for the following variables: condition\nafex_plot(mc_urry, \"condition\")\nmc_urry\n#> Anova Table (Type 3 tests)\n#> \n#> Response: conceptual\n#>      Effect     df    MSE    F  ges p.value\n#> 1 condition 1, 140 441.76 1.00 .007    .319\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\nemmeans(mc_urry, \"condition\")\n#>  condition emmean   SE  df lower.CL upper.CL\n#>  laptop      73.5 2.55 140     68.5     78.6\n#>  longhand    70.0 2.44 140     65.2     74.8\n#> \n#> Confidence level used: 0.95"},{"path":"case-study-1-more-results-from-note-taking-studies.html","id":"why-are-experiments-replicated","chapter":"6 Case Study 1: More Results from Note Taking Studies","heading":"6.2 Why are Experiments Replicated?","text":"experiment Urry et al. (2021) first experiment investigating effect note taking lectures memory. contrast, study replication Mueller Oppenheimer (2014). replication act rerunning existing study see one can obtain (replicate) results previous study.discussed , inferences NHST never conclusive probabilistic require multiple inferential steps. Replications one important tools science overcoming least probabilistic uncertainties associated inferences draw experimental data. example consider several independent otherwise similar possible experiments – , replications experiment – obtain significant result (.e., indicate data incompatible null hypothesis). pattern dramatically increase confidence null hypothesis likely false.addition gain confidence specific results, good practical reasons replicating existing experiment. example, beginning work new topic generally good idea replicate experiment one wants build . one already problems replicating exists shows topic maybe simple portrayed literature.Another excellent reason performing replication one simply believe existing result. Remember, one key components scientific method scepticism (least according Wikipedia). results difficult believe, reasonable sceptical position take require evidence. replication one way (best way) produce evidence. believing existing experiments also imply one questions integrity researchers experiment. many completely harmless reasons study might replicate. example, researchers might just obtained significant results chance (happens 5% cases, discussed next chapters).Sadly, replicating existing experiments publishing results, still norm psychology related disciplines. Quite contrary, situation dire many fields currently considered replication crisis. example, large scale effort replicate 100 studies psychology (Open Science Collaboration 2015) showed less 50% replicated successfully. Similarly sobering results since observed across social sciences (Camerer et al. 2018, 2016; Klein et al. 2018). Much written problem right place rehash arguments. best summary situation book Chris Chambers (Chambers 2017). important thing realise science cumulative endeavour. Every new experiment builds existing research. existing research never replicated, confidence research somewhat low. questions foundations new work builds non-replicated research. move forward researchers need replicate work important research, value replications done others (especially work), let findings replicate fade obscurity.","code":""},{"path":"case-study-1-more-results-from-note-taking-studies.html","id":"conceptual-memory-data-from-mueller-and-oppenheimer-2014","chapter":"6 Case Study 1: More Results from Note Taking Studies","heading":"6.3 Conceptual Memory Data from Mueller and Oppenheimer (2014)","text":"Urry et al. (2021) direct replication Mueller Oppenheimer (2014), design uses materials (.e., TED talks questions). Participants watched short lecture videos (projected onto screen) take notes either laptop longhand format. 30 minutes lecture asked factual conceptual questions lectures. answers coded first author. previous analysis, transform answers memory index 0 (= memory) 100 (= perfect memory). line analysis Urry et al. (2021) , interested conceptual memory .experiment Urry et al. (2021) direct replication Experiment 1 Mueller Oppenheimer (2014). focus Experiment 2 Mueller Oppenheimer (2014), also direct replication Experiment 1 included additional experimental manipulation ignore . reason focussing Experiment 2 instead Experiment 1 data Experiment 2 come bit interesting (feel free rerun analysis reported Experiment 1 see mean). However, provide incomplete picture research question whether mode taking note lectures affects memory, consider overall evidence (.e., 3 experiments Mueller Oppenheimer, experiment Urry et al., data) end chapter.Luckily us, data Mueller Oppenheimer (2014), including data Experiment 2, available online Open Science Framework (OSF). OSF one visible developments resulting replication crisis. free website allows researchers share data materials associated research. replication crisis OSF rare get access data underlying published studies. Nowadays many researchers depose (anonymised) data published studies OSF include links data papers. allows researcher, us, reanalyse existing data ensure reported results can reproduced.38","code":""},{"path":"case-study-1-more-results-from-note-taking-studies.html","id":"preparing-the-data","chapter":"6 Case Study 1: More Results from Note Taking Studies","heading":"6.3.1 Preparing the Data","text":"get habit downloading data OSF reanalysing , going now. file need called Study 2 abbreviated data.csv can found following OSF link: https://osf.io/t43ua/ Please go ahead download now put folder can access . already done copied folder data. use tidyverse function read_csv(), always returns tibble (tidyverse version data.frame), read data, object mo2014. use glimpse() function (also tiydverse function) get overview data (similar str() less verbose tibbles).can see data 153 participants 22 columns. Many columns names immediately clear. uncommon. important task getting new data set trying figure variables mean. One usually also data experiments. example, software running experiments often collects variables needed analysis. Consequently, first analysis step usually figure needed .however, note 153 participants final number participants reported Mueller Oppenheimer (2014). Instead, removed two participants analysis resulting. Studying OSF repository detail (particular published SPPS script output Output Syntax - Study 2.doc) shows participants number 194 237 needs removed (information can found variable filter_$ current data set). file also shows two relevant notetype conditions 1 = longhand 2 = laptop remove notetype == 3 analysis. moving , filter data remove observations. use filter() tidyverse combination pipe operator %>% (.e., pipe tibble filter() retain rows want). Importantly, overwrite mo2014 filtered tibble need filtered observations (use , woul dhave read data ). see many participants remain using nrow() (returns number rows data).reported 99 participants matches 99 participants reported OSF two conditions, laptop versus longhand.Sadly, OSF include codebook describing variables particular data set (earlier version data set variables overlaps degree present one). However, looking data information OSF things clear: participantid participant identifier variable, notetype condition identifier coding experimental condition, whichtalk identifies TED talk participants saw (mapping talks numbers also given SPPS output).first step, can transform relevant indicator variables, participant experimental condition variable, factors analysis. Transforming variable factor guarantees none analyses incorrectly treats one factors (.e., categorical variables) numerical variable (e.g., taking mean numbers participant identifier column reasonable statistical operation). However, instead overwriting existing variables, create new variable name analysis Urry et al. (2021), pid condition. also assign human understandable labels instead using 1 2 condition codes. make easier understand pattern results. use factor() inside mutate() tidyverse combination pipe operator %>%.Looking data reveals newly created variables added end tibble.next step calculate dependent variable, memory scores 0 100 used analysis Urry et al. (2021). can see two question types, factual conceptual, multiple measures. index score raw score well perfect variants types scores. perfect presumably means maximal possible value obtained score observation (.e., row). data also contains number \\(z\\)-transformed variants scores (variables Z…), ignore (original paper used z-scores, difficult interpret results qualitatively , ignore ). focus index score gives participant maximal 1 point per question (information given paper/OSF). Let us take look first six observations relevant variables.can see number questions per question type talk differs (indicated difference perfect indexscore values across rows), total number items appears always ten (perfectfactindexscore + perfectconceptindexscore = 10 row). also aligns list items found OSF show ten questions per talk number factual conceptual questions differing across talks. information calculate memory scores. However, moving makes sense run quick sanity check see indeed number questions per row ten. , create new variable sum two perfect index scores, using mutate() (adds variable existing data), see whether sum always equal 10, using summarise() (case reduces data one row). number question per observation/row sums ten, return TRUE.Fortunately, check passes feel assumption meaning variables supported go ahead calculate memory score.preparing data analysis, running analysis, important regularly include sanity checks ones analysis. analysis involves assumptions underlying data – example, variable means, values variable can possibly take , observations included data. Based assumption calculate variables data perform analysis. However, humans fallible data analysis experience shows assumptions sometimes (regularly) false. Sometimes one misunderstood (misremembers) meaning variable, might data entry error, data still includes observations excluded (e.g., test runs researchers instead participants). example, study Lewandowsky, Gignac, Oberauer (2013) age one participants recorded 32,757 years error uncovered publication manuscript. Luckily error affect conclusion drawn data, publish correction (Lewandowsky, Gignac, Oberauer 2015). Publishing correction nothing dramatic (paper published corrections errors discovered publication), course prefer . errors affect conclusion substantially, sometimes correction enough paper retracted. Regular sanity assumptions checks ones analysis way minimise chance errors final analysis.Based positive outcome sanity check now convinced understood variables data can now calculate memory score 0 100. , divide index score perfect…indexscore multiply results 100. simplify coming analysis, create new tibble, mo2014a, retains variables really need analysis using select(). take another look first six rows data using head(). shows data now ready reanalysis.","code":"\nmo2014 <- read_csv(\"data/Study 2 abbreviated data.csv\")\nglimpse(mo2014)\n#> Rows: 153\n#> Columns: 22\n#> $ participantid            <dbl> 103, 122, 142, 152, 172, ~\n#> $ notetype                 <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1~\n#> $ whichtalk                <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1~\n#> $ Wcount                   <dbl> 148, 273, 124, 149, 182, ~\n#> $ threeG                   <dbl> 0.08219178, 0.05535055, 0~\n#> $ factualindex             <dbl> 2.499, 2.833, 2.333, 4.49~\n#> $ conceptualindex          <dbl> 2.0, 1.5, 2.0, 2.0, 2.0, ~\n#> $ factualraw               <dbl> 6, 7, 5, 11, 12, 4, 7, 11~\n#> $ conceptualraw            <dbl> 4, 3, 4, 4, 4, 3, 4, 3, 4~\n#> $ perfectfactindexscore    <dbl> 7, 7, 7, 7, 7, 7, 7, 7, 7~\n#> $ perfectconceptindexscore <dbl> 3, 3, 3, 3, 3, 3, 3, 3, 3~\n#> $ perfectfactscore         <dbl> 14, 14, 14, 14, 14, 14, 1~\n#> $ perfectconceptscore      <dbl> 6, 6, 6, 6, 6, 6, 6, 6, 6~\n#> $ `filter_$`               <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1~\n#> $ ZFindexA                 <dbl> -0.17488644, 0.07568357, ~\n#> $ ZCindexA                 <dbl> 0.52136737, 0.03238307, 0~\n#> $ ZFrawA                   <dbl> 0.32848714, 0.68152314, -~\n#> $ ZCrawA                   <dbl> 0.9199338, 0.2942131, 0.9~\n#> $ ZFindexW                 <dbl> -0.76333431, -0.47725123,~\n#> $ ZCindexW                 <dbl> 0.79471336, 0.06811829, 0~\n#> $ ZFrawW                   <dbl> -0.6437995, -0.2476152, -~\n#> $ ZCrawW                   <dbl> 0.9647838, 0.1731663, 0.9~\nmo2014 <- mo2014 %>% \n  filter(notetype != 3, participantid != 194, participantid != 237)\nnrow(mo2014)\n#> [1] 99\nmo2014 <- mo2014 %>% \n  filter(notetype != 3) %>% \n  mutate(\n    pid = factor(participantid),\n    condition = factor(notetype, \n                       levels = c(2, 1),\n                       labels = c(\"laptop\", \"longhand\"))\n  )\nglimpse(mo2014)\n#> Rows: 99\n#> Columns: 24\n#> $ participantid            <dbl> 103, 122, 142, 152, 172, ~\n#> $ notetype                 <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1~\n#> $ whichtalk                <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1~\n#> $ Wcount                   <dbl> 148, 273, 124, 149, 182, ~\n#> $ threeG                   <dbl> 0.08219178, 0.05535055, 0~\n#> $ factualindex             <dbl> 2.499, 2.833, 2.333, 4.49~\n#> $ conceptualindex          <dbl> 2.0, 1.5, 2.0, 2.0, 2.0, ~\n#> $ factualraw               <dbl> 6, 7, 5, 11, 12, 4, 7, 11~\n#> $ conceptualraw            <dbl> 4, 3, 4, 4, 4, 3, 4, 3, 4~\n#> $ perfectfactindexscore    <dbl> 7, 7, 7, 7, 7, 7, 7, 7, 7~\n#> $ perfectconceptindexscore <dbl> 3, 3, 3, 3, 3, 3, 3, 3, 3~\n#> $ perfectfactscore         <dbl> 14, 14, 14, 14, 14, 14, 1~\n#> $ perfectconceptscore      <dbl> 6, 6, 6, 6, 6, 6, 6, 6, 6~\n#> $ `filter_$`               <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1~\n#> $ ZFindexA                 <dbl> -0.17488644, 0.07568357, ~\n#> $ ZCindexA                 <dbl> 0.52136737, 0.03238307, 0~\n#> $ ZFrawA                   <dbl> 0.32848714, 0.68152314, -~\n#> $ ZCrawA                   <dbl> 0.9199338, 0.2942131, 0.9~\n#> $ ZFindexW                 <dbl> -0.76333431, -0.47725123,~\n#> $ ZCindexW                 <dbl> 0.79471336, 0.06811829, 0~\n#> $ ZFrawW                   <dbl> -0.6437995, -0.2476152, -~\n#> $ ZCrawW                   <dbl> 0.9647838, 0.1731663, 0.9~\n#> $ pid                      <fct> 103, 122, 142, 152, 172, ~\n#> $ condition                <fct> longhand, longhand, longh~\nmo2014 %>% \n  select(pid, condition, factualindex, conceptualindex, \n         perfectfactindexscore, perfectconceptindexscore) %>% \n  head()\n#> # A tibble: 6 x 6\n#>   pid   condition factualindex conceptualindex\n#>   <fct> <fct>            <dbl>           <dbl>\n#> 1 103   longhand          2.50             2  \n#> 2 122   longhand          2.83             1.5\n#> 3 142   longhand          2.33             2  \n#> 4 152   longhand          4.50             2  \n#> 5 172   longhand          5.00             2  \n#> 6 183   longhand          1.83             1.5\n#> # ... with 2 more variables: perfectfactindexscore <dbl>,\n#> #   perfectconceptindexscore <dbl>\nmo2014 %>% \n  mutate(sum_p_index = perfectfactindexscore + perfectconceptindexscore) %>% \n  summarise(check = all(sum_p_index == 10))\n#> # A tibble: 1 x 1\n#>   check\n#>   <lgl>\n#> 1 TRUE\nmo2014a <- mo2014 %>% \n  mutate(\n    factual = factualindex / perfectfactindexscore * 100,\n    conceptual = conceptualindex / perfectconceptindexscore * 100\n  ) %>% \n  select(pid, condition, factual, conceptual)\nhead(mo2014a)\n#> # A tibble: 6 x 4\n#>   pid   condition factual conceptual\n#>   <fct> <fct>       <dbl>      <dbl>\n#> 1 103   longhand     35.7       66.7\n#> 2 122   longhand     40.5       50  \n#> 3 142   longhand     33.3       66.7\n#> 4 152   longhand     64.3       66.7\n#> 5 172   longhand     71.4       66.7\n#> 6 183   longhand     26.2       50"},{"path":"case-study-1-more-results-from-note-taking-studies.html","id":"descriptive-statistics","chapter":"6 Case Study 1: More Results from Note Taking Studies","heading":"6.3.2 Descriptive Statistics","text":"performing inferential statistical analysis data, obtain descriptive statistics. provide us overview data. addition, descriptive analysis another way check data minimise chances errors problems.first thing, want calculate number participants per condition. , use tidyverse functions explain steps detail. generally start tidyverse analyses data, tibble mo2014a, followed pipe %>%. pipe “pipes” tibble next function. obtaining descriptives statistics often want get conditional factor/categorical variable data. example, now want calculate number observations per condition. can done piping tibble group_by() function condition variable interest, condition. results “grouped” tibble ensures following operations tibble performed grouped (.e., conditioned ) grouping variable. can now pipe grouped tibble count() function get number observations per note taking condition.another sanity check, can compare number values reported OSF data (\\(N\\) condition reported original paper). numbers match, increases confidence data preparation.next descriptive statistic, calculate condition means DV interest, conceptual memory scores. also calculate standard deviation get idea spread data. use piping tidyverse get result. time final function pipe summarise() allows calculate summary statistics.shows conceptual memory 10 points larger longhand compared laptop condition. also see difference around 5 points SD.side note, added another calculation summarise() call. example, n = n() also calculated number participants per condition, previous code .One important part descriptive analysis always plot data. plot data points usually best way see something wrong data. , used afex_plot() estimated model aov_car(), can also invoke ggplot2 directly. , also pipe data ggplot() build figure layer layer. important part mapping variables data aesthetics aes() function. call directly ggplot() call mimic figures seen far, mapping condition \\(x\\)-axis DV, conceptual memory, \\(y\\)-axis. Figure 5.1, begin violin plot (geom_violin()) different quantiles. violin plot shows shape distribution. combine individual data points, whcih show using geom_beeswarm() ggbeeswarm package (call function without loading package beforehand using package::function()). add mean (standard error, explained later) using stat_summary() red. plot see data already spans full range \\(y\\)-axis, need use coord_cartesian(ylim = c(0, 100)).\nFigure 6.2: Conceptual memory scores Mueller Oppenheimer (2014). plot combines individual data points black means red.\nlooking figure, see got status message console, “summary function supplied, defaulting `mean_se()`.” message always shown using stat_summary() without additional argument can safely ignored (.e., just indicates red point shows mean red error bars show standard error). plot show anything unusual. plot just reinforces previous descriptive results: mean memory score, also three displayed quantiles, larger longhand laptop condition. Taken together, descriptive analysis suggests nothing preventing us running inferential analysis.","code":"\nmo2014a %>% \n  group_by(condition) %>% \n  count()\n#> # A tibble: 2 x 2\n#> # Groups:   condition [2]\n#>   condition     n\n#>   <fct>     <int>\n#> 1 laptop       51\n#> 2 longhand     48\nmo2014a %>% \n  group_by(condition) %>% \n  summarise(\n    mean = mean(conceptual),\n    sd = sd(conceptual)\n  )\n#> # A tibble: 2 x 3\n#>   condition  mean    sd\n#>   <fct>     <dbl> <dbl>\n#> 1 laptop     35    23.5\n#> 2 longhand   46.5  28.9\nmo2014a %>% \n  ggplot(aes(x = condition, y = conceptual)) +\n  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +\n  ggbeeswarm::geom_beeswarm() +\n  stat_summary(colour = \"red\")  \n#> No summary function supplied, defaulting to `mean_se()`"},{"path":"case-study-1-more-results-from-note-taking-studies.html","id":"inferential-analysis","chapter":"6 Case Study 1: More Results from Note Taking Studies","heading":"6.3.3 Inferential Analysis","text":"inferential analysis conceptual scores uses exactly call previous analysis, new data set, mo2014a.Looking ANOVA table results shows \\(p\\)-value smaller .05; analysis reveals significant effect condition. indicates data provides evidence null hypothesis difference note taking conditions. Consequently, justified saying data provides evidence difference. make easy detect significant result, afex, like statistical software tools, indicates significant effect \\(p < .05\\) one * next \\(F\\)-value (case \\(p < .01\\) indication **, case \\(p < .001\\) ***, case effect significant, \\(p < .1\\), +).\nFigure 6.3: afex_plot() figure conceptual memory scores Mueller Oppenheimer (2014, Experiment 2) show significant difference two note taking conditions.\n","code":"\nmc_mo <- aov_car(conceptual ~ condition + Error(pid), mo2014a)\n#> Contrasts set to contr.sum for the following variables: condition\nmc_mo\n#> Anova Table (Type 3 tests)\n#> \n#> Response: conceptual\n#>      Effect    df    MSE      F  ges p.value\n#> 1 condition 1, 97 688.89 4.77 * .047    .031\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\nafex_plot(mc_mo, \"condition\")"},{"path":"case-study-1-more-results-from-note-taking-studies.html","id":"summary-4","chapter":"6 Case Study 1: More Results from Note Taking Studies","heading":"6.4 Summary","text":"One reality research significant results generally researchers looking . results significant happy, experiment “worked” can publish . significant, generally problems publishing results.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
