@Book{xie2015,
  title = {Dynamic Documents with {R} and knitr},
  author = {Yihui Xie},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2015},
  edition = {2nd},
  note = {ISBN 978-1498716963},
  url = {http://yihui.org/knitr/},
}

@article{urry2021,
	title = {Don{\textquoteright}t Ditch the Laptop Just Yet: A Direct Replication of Mueller and Oppenheimer{\textquoteright}s (2014) Study 1 Plus Mini Meta-Analyses Across Similar Studies},
	author = {{Urry}, {Heather L.} and {Crittle}, {Chelsea S.} and {Floerke}, {Victoria A.} and {Leonard}, {Michael Z.} and {Perry}, {Clinton S.} and {Akdilek}, {Naz} and {Albert}, {Erica R.} and {Block}, {Avram J.} and {Bollinger}, {Caroline Ackerley} and {Bowers}, {Emily M.} and {Brody}, {Renee S.} and {Burk}, {Kelly C.} and {Burnstein}, {Ally} and {Chan}, {Allissa K.} and {Chan}, {Petrina C.} and {Chang}, {Lena J.} and {Chen}, {Emily} and {Chiarawongse}, {Chakrapand Paul} and {Chin}, {Gregory} and {Chin}, {Kathy} and {Cooper}, {Ben G.} and {Corneilson}, {Katherine Adele} and {Danielson}, {Amanda M.} and {Davis}, {Elizabeth S.} and {Devis}, {Ycar} and {Dong}, {Melissa} and {Dossett}, {Elizabeth K.} and {Dulchin}, {Nick} and {Duong}, {Vincent N.} and {Ewing}, {Ben} and {Fuller}, {Julia Mansfield} and {Gartman}, {Thomas E.} and {Goldberg}, {Chad R.} and {Greenfield}, {Jesse} and {Groh}, {Selena} and {Hamilton}, {Ross A.} and {Hodge}, {Will} and {Van Hong}, {Dylan} and {Insler}, {Joshua E.} and {Jahan}, {Aava B.} and {Jimbo}, {Jessica Paola} and {Kahn}, {Emma M.} and {Knight}, {Daniel} and {Konstantin}, {Grace E.} and {Kornick}, {Caitlin} and {Kramer}, {Zachary J.} and {Lauz√©}, {Meghan S.} and {Linnehan}, {Misha S.} and {Lombardi}, {Tommaso} and {Long}, {Hayley} and {Lotstein}, {Alec J.} and {Lyncee}, {Myrna-Nahisha A.} and {Lyons}, {Monica Gabriella} and {Maayan}, {Eli} and {May}, {Nicole Marie} and {McCall}, {Elizabeth C.} and {Montgomery-Walsh}, {Rhea Ann Charlotte} and {Morscher}, {Michael C.} and {Moser}, {Amelia D.} and {Mueller}, {Alexandra S.} and {Mujica}, {Christin A.} and {Na}, {Elim} and {Newman}, {Isabelle R.} and {O{\textquoteright}Brien}, {Meghan K.} and {Ochoa Castillo}, {Katherine Alexandra} and {Onipede}, {Zaenab Ayotola} and {Pace}, {Danielle A.} and {Park}, {Jasper H.} and {Perdikari}, {Angeliki} and {Perloff}, {Catherine E.} and {Perry}, {Rachel C.} and {Pillai}, {Akash A.} and {Rajpal}, {Avni} and {Ranalli}, {Emma} and {Schreier}, {Jillian E.} and {Shangguan}, {Justin R.} and {Silver}, {Micaela Jen} and {Spratt}, {Avery Glennon} and {Stein}, {Rachel E.} and {Steinhauer}, {Grant J.} and {Valera}, {Devon K.} and {Vervoordt}, {Samantha M.} and {Walton}, {Lena} and {Weinflash}, {Noah W.} and {Weinstock}, {Karen} and {Yuan}, {Jiaqi} and {Zarrella}, {Dominique T.} and {Zarrow}, {Jonah E.}},
	year = {2021},
	month = {02},
	date = {2021-02-04},
	journal = {Psychological Science},
	pages = {0956797620965541},
	doi = {10.1177/0956797620965541},
	url = {https://doi.org/10.1177/0956797620965541},
	note = {Publisher: SAGE Publications Inc},
	langid = {en}
}

@book{kline2015,
	title = {Principles and practice of structural equation modeling},
	author = {{Kline}, {Rex B}},
	year = {2015},
	date = {2015},
	publisher = {Guilford Press},
	langid = {English}
}

@book{lee2013,
	title = {Bayesian cognitive modeling: a practical course},
	author = {{Lee}, {Michael D.} and {Wagenmakers}, {Eric-Jan}},
	year = {2013},
	date = {2013},
	publisher = {Cambridge University Press},
	address = {Cambridge},
	langid = {English}
}

@book{lenth2021,
	title = {emmeans: Estimated Marginal Means, aka Least-Squares Means},
	author = {{Lenth}, {Russell}},
	year = {2021},
	date = {2021},
	url = {https://CRAN.R-project.org/package=emmeans},
	note = {R package version 1.1, https://CRAN.R-project.org/package=emmeans}
}

@book{wickham2017,
	title = {R for Data Science: Import, Tidy, Transform, Visualize, and Model Data},
	author = {{Wickham}, {Hadley} and {Grolemund}, {Garrett}},
	year = {2017},
	date = {2017},
	publisher = {O'Reilly},
	address = {Sebastopol  CA},
	langid = {English}
}

@article{meehl1978,
	title = {Theoretical Risks and Tabular Asterisks: Sir Karl, Sir Ronald, and the Slow Progress of Soft Psychology},
	author = {{Meehl}, {Paul E}},
	year = {1978},
	date = {1978},
	journal = {Journal of Consulting and Clinical Psychology},
	pages = {806--834},
	volume = {46},
	doi = {10.1037//0022-006X.46.4.806}
}

@article{cohen1994,
	title = {The earth is round (p < .05)},
	author = {{Cohen}, {Jacob}},
	year = {1994},
	date = {1994},
	journal = {American Psychologist},
	pages = {997--1003},
	volume = {49},
	number = {12},
	doi = {10.1037/0003-066X.49.12.997},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0003-066X.49.12.997}
}

@article{rozeboom1960,
	title = {The fallacy of the null-hypothesis significance test.},
	author = {{Rozeboom}, {William W.}},
	year = {1960},
	month = {09},
	date = {1960-09},
	journal = {Psychological Bulletin},
	pages = {416--428},
	volume = {57},
	number = {5},
	doi = {10.1037/h0042040},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/h0042040},
	langid = {en}
}

@article{nickerson2000,
	title = {Null hypothesis significance testing: a review of an old and continuing controversy.},
	author = {{Nickerson}, {Raymond S.}},
	year = {2000},
	date = {2000},
	journal = {Psychological methods},
	pages = {241},
	volume = {5},
	number = {2}
}

@article{wagenmakers2007,
	title = {A practical solution to the pervasive problems of p values},
	author = {{Wagenmakers}, {Eric-Jan}},
	year = {2007},
	month = {10},
	date = {2007-10},
	journal = {Psychonomic Bulletin \& Review},
	pages = {779--804},
	volume = {14},
	number = {5},
	doi = {10.3758/BF03194105},
	url = {http://www.springerlink.com/content/7403110087u84w72/}
}


@book{howellStatisticalMethodsPsychology2013,
  title = {Statistical Methods for Psychology},
  author = {Howell, David C},
  year = {2013},
  publisher = {{Wadsworth Cengage Learning}},
  address = {{Belmont, CA}},
  language = {English}
}




@book{howell2013,
	title = {Statistical methods for psychology},
	author = {{Howell}, {David C}},
	year = {2013},
	date = {2013},
	publisher = {Wadsworth Cengage Learning},
	address = {Belmont, CA},
	langid = {English}
}


@book{foxCompanionAppliedRegression2019,
  title = {An {{R}} Companion to Applied Regression},
  author = {Fox, John and Weisberg, Sanford},
  year = {2019},
  edition = {Third},
  publisher = {{Sage}},
  address = {{Thousand Oaks CA}}
}





@article{olejnikGeneralizedEtaOmega2003,
  title = {Generalized {{Eta}} and {{Omega Squared Statistics}}: {{Measures}} of {{Effect Size}} for {{Some Common Research Designs}}.},
  shorttitle = {Generalized {{Eta}} and {{Omega Squared Statistics}}},
  author = {Olejnik, Stephen and Algina, James},
  year = {2003},
  volume = {8},
  pages = {434--447},
  doi = {10.1037/1082-989X.8.4.434},
  file = {C\:\\Users\\singm\\Zotero\\storage\\8GTRS7CB\\OlejnikS2003a.pdf},
  journal = {Psychological Methods},
  number = {4}
}





@article{bakemanRecommendedEffectSize2005,
  title = {Recommended Effect Size Statistics for Repeated Measures Designs},
  author = {Bakeman, Roger},
  year = {2005},
  month = aug,
  volume = {37},
  pages = {379--384},
  doi = {10.3758/BF03192707},
  file = {C\:\\Users\\singm\\Zotero\\storage\\K4IUREZI\\Bakeman - 2005 - Recommended effect size statistics for repeated me.pdf},
  journal = {Behavior Research Methods},
  number = {3}
}





@article{lakensCalculatingReportingEffect2013,
  title = {Calculating and Reporting Effect Sizes to Facilitate Cumulative Science: A Practical Primer for t-Tests and {{ANOVAs}}},
  shorttitle = {Calculating and Reporting Effect Sizes to Facilitate Cumulative Science},
  author = {Lakens, Daniel},
  year = {2013},
  volume = {4},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2013.00863},
  abstract = {Effect sizes are the most important outcome of empirical studies. Most articles on effect sizes highlight their importance to communicate the practical significance of results. For scientists themselves, effect sizes are most useful because they facilitate cumulative science. Effect sizes can be used to determine the sample size for follow-up studies, or examining effects across studies. This article aims to provide a practical primer on how to calculate and report effect sizes for t-tests and ANOVA's such that effect sizes can be used in a-priori power analyses and meta-analyses. Whereas many articles about effect sizes focus on between-subjects designs and address within-subjects designs only briefly, I provide a detailed overview of the similarities and differences between within- and between-subjects designs. I suggest that some research questions in experimental psychology examine inherently intra-individual effects, which makes effect sizes that incorporate the correlation between measures the best summary of the results. Finally, a supplementary spreadsheet is provided to make it as easy as possible for researchers to incorporate effect size calculations into their workflow.},
  file = {C\:\\Users\\singm\\Zotero\\storage\\EDWBSNWA\\Lakens - 2013 - Calculating and reporting effect sizes to facilita.pdf},
  journal = {Frontiers in Psychology},
  keywords = {Cohen's d,effect sizes,eta-squared,power analysis,sample size planning},
  language = {English}
}





@article{baguleyStandardizedSimpleEffect2009,
  title = {Standardized or Simple Effect Size: {{What}} Should Be Reported?},
  shorttitle = {Standardized or Simple Effect Size},
  author = {Baguley, Thom},
  year = {2009},
  month = aug,
  volume = {100},
  pages = {603--617},
  issn = {2044-8295},
  doi = {10.1348/000712608X377117},
  abstract = {It is regarded as best practice for psychologists to report effect size when disseminating quantitative research findings. Reporting of effect size in the psychological literature is patchy \textendash{} though this may be changing \textendash{} and when reported it is far from clear that appropriate effect size statistics are employed. This paper considers the practice of reporting point estimates of standardized effect size and explores factors such as reliability, range restriction and differences in design that distort standardized effect size unless suitable corrections are employed. For most purposes simple (unstandardized) effect size is more robust and versatile than standardized effect size. Guidelines for deciding what effect size metric to use and how to report it are outlined. Foremost among these are: (i) a preference for simple effect size over standardized effect size, and (ii) the use of confidence intervals to indicate a plausible range of values the effect might take. Deciding on the appropriate effect size statistic to report always requires careful thought and should be influenced by the goals of the researcher, the context of the research and the potential needs of readers.},
  file = {C\:\\Users\\singm\\Zotero\\storage\\AJU59HS7\\Baguley - 2009 - Standardized or simple effect size What should be.pdf},
  journal = {British Journal of Psychology},
  language = {en},
  number = {3}
}





@article{lenthPracticalGuidelinesEffective2001,
  title = {Some {{Practical Guidelines}} for {{Effective Sample Size Determination}}},
  author = {Lenth, Russell V},
  year = {2001},
  month = aug,
  volume = {55},
  pages = {187--193},
  issn = {0003-1305},
  doi = {10.1198/000313001317098149},
  abstract = {Sample size determination is often an important step in planning a statistical study\textemdash and it is usually a difficult one. Among the important hurdles to be surpassed, one must obtain an estimate of one or more error variances and specify an effect size of importance. There is the temptation to take some shortcuts. This article offers some suggestions for successful and meaningful sample size determination. Also discussed is the possibility that sample size may not be the main issue, that the real goal is to design a high-quality study. Finally, criticism is made of some ill-advised shortcuts relating to power and sample size.},
  file = {C\:\\Users\\singm\\Zotero\\storage\\KVHWN4JW\\Lenth - 2001 - Some Practical Guidelines for Effective Sample Siz.pdf},
  journal = {The American Statistician},
  number = {3}
}




@book{baguleySeriousStatsGuide2012,
	title = {Serious stats : a guide to advanced statistics for the behavioral sciences},
	author = {{Baguley}, {Thomas}},
	year = {2012},
	date = {2012},
	publisher = {Palgrave Macmillan},
	address = {Houndmills, Basingstoke, Hampshire; New York},
	langid = {English}
}


@article{muellerPenMightierKeyboard2014,
  title = {The {{Pen Is Mightier Than}} the {{Keyboard}}: {{Advantages}} of {{Longhand Over Laptop Note Taking}}},
  shorttitle = {The {{Pen Is Mightier Than}} the {{Keyboard}}},
  author = {Mueller, Pam A. and Oppenheimer, Daniel M.},
  year = {2014},
  month = jun,
  volume = {25},
  pages = {1159--1168},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1177/0956797614524581},
  abstract = {Taking notes on laptops rather than in longhand is increasingly common. Many researchers have suggested that laptop note taking is less effective than longhand note taking for learning. Prior studies have primarily focused on students' capacity for multitasking and distraction when using laptops. The present research suggests that even when laptops are used solely to take notes, they may still be impairing learning because their use results in shallower processing. In three studies, we found that students who took notes on laptops performed worse on conceptual questions than students who took notes longhand. We show that whereas taking more notes can be beneficial, laptop note takers' tendency to transcribe lectures verbatim rather than processing information and reframing it in their own words is detrimental to learning.},
  file = {C\:\\Users\\singm\\Zotero\\storage\\6GKDIP3S\\Mueller and Oppenheimer - 2014 - The Pen Is Mightier Than the Keyboard Advantages .pdf},
  journal = {Psychological Science},
  keywords = {academic achievement,cognitive processes,educational psychology,memory,open data,open materials},
  language = {en},
  number = {6}
}





@article{opensciencecollaborationEstimatingReproducibilityPsychological2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  month = aug,
  volume = {349},
  pages = {aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  abstract = {Empirically analyzing empirical evidence One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study. Science, this issue 10.1126/science.aac4716 Structured Abstract INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error. RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science. RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {$<$} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that ``we already know this'' belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. Download high-res image Open in new tab Download Powerpoint Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects. Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired. A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.},
  copyright = {Copyright \textcopyright{} 2015, American Association for the Advancement of Science},
  file = {C\:\\Users\\singm\\Zotero\\storage\\XDF2PK2F\\Collaboration - 2015 - Estimating the reproducibility of psychological sc.pdf},
  journal = {Science},
  language = {en},
  number = {6251}
}





@article{camererEvaluatingReplicabilitySocial2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  year = {2018},
  month = aug,
  pages = {1},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  abstract = {Camerer et al. carried out replications of 21 Science and Nature social science experiments, successfully replicating 13 out of 21 (62\%). Effect sizes of replications were about half of the size of the originals.},
  copyright = {2018 The Author(s)},
  file = {C\:\\Users\\singm\\Zotero\\storage\\N3DCDTGL\\Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf},
  journal = {Nature Human Behaviour},
  language = {en}
}





@article{kleinManyLabsInvestigating2018,
  title = {Many {{Labs}} 2: {{Investigating Variation}} in {{Replicability Across Samples}} and {{Settings}}},
  shorttitle = {Many {{Labs}} 2},
  author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Adams, Reginald B. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Batra, Rishtee and Berkics, Mih{\'a}ly and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and R{\'e}dei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Dalla Rosa, Anna and Davis, William E. and {de Bruijn}, Maaike and De Schutter, Leander and Devos, Thierry and {de Vries}, Marieke and Do{\u g}ulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-{\'A}ngel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and G{\'o}mez, {\'A}ngel and Gonz{\'a}lez, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and {Innes-Ker}, {\AA}se H. and {Jim{\'e}nez-Leal}, William and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Kamilo{\u g}lu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Kne{\v z}evi{\'c}, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Dani{\"e}l and Lazarevi{\'c}, Ljiljana B. and Levitan, Carmel A. and Lewis, Neil A. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Me{\dj}edovi{\'c}, Janko and {Mena-Pacheco}, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, F{\'e}lix and Lee Nichols, Austin and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, G{\'a}bor and Osowiecka, Malgorzata and Packard, Grant and {P{\'e}rez-S{\'a}nchez}, Rolando and Petrovi{\'c}, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Sch{\"o}nbrodt, Felix D. and Sekerdej, Maciej B. and Sirlop{\'u}, David and Skorinko, Jeanine L. M. and Smith, Michael A. and {Smith-Castro}, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M. and {van der Hulst}, Marije and {van Lange}, Paul A. M. and {van 't Veer}, Anna Elisabeth and {V{\'a}squez- Echeverr{\'i}a}, Alejandro and Ann Vaughn, Leigh and V{\'a}zquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
  year = {2018},
  month = dec,
  volume = {1},
  pages = {443--490},
  issn = {2515-2459},
  doi = {10.1177/2515245918810225},
  abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p {$<$} .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p {$<$} .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen's ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small ({$<$} 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
  file = {C\:\\Users\\singm\\Zotero\\storage\\HNSXPBH7\\Klein et al. - 2018 - Many Labs 2 Investigating Variation in Replicabil.pdf},
  journal = {Advances in Methods and Practices in Psychological Science},
  language = {en},
  number = {4}
}





@article{camererEvaluatingReplicabilityLaboratory2016,
  title = {Evaluating Replicability of Laboratory Experiments in Economics},
  author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
  year = {2016},
  month = mar,
  volume = {351},
  pages = {1433--1436},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaf0918},
  abstract = {Another social science looks at itself Experimental economists have joined the reproducibility discussion by replicating selected published experiments from two top-tier journals in economics. Camerer et al. found that two-thirds of the 18 studies examined yielded replicable estimates of effect size and direction. This proportion is somewhat lower than unaffiliated experts were willing to bet in an associated prediction market, but roughly in line with expectations from sample sizes and P values. Science, this issue p. 1433 The replicability of some scientific findings has recently been called into question. To contribute data about replicability in economics, we replicated 18 studies published in the American Economic Review and the Quarterly Journal of Economics between 2011 and 2014. All of these replications followed predefined analysis plans that were made publicly available beforehand, and they all have a statistical power of at least 90\% to detect the original effect size at the 5\% significance level. We found a significant effect in the same direction as in the original study for 11 replications (61\%); on average, the replicated effect size is 66\% of the original. The replicability rate varies between 67\% and 78\% for four additional replicability indicators, including a prediction market measure of peer beliefs. By several metrics, economics experiments do replicate, although not as often as predicted. By several metrics, economics experiments do replicate, although not as often as predicted.},
  chapter = {Report},
  copyright = {Copyright \textcopyright{} 2016, American Association for the Advancement of Science},
  file = {C\:\\Users\\singm\\Zotero\\storage\\XNLKF9G5\\Camerer et al. - 2016 - Evaluating replicability of laboratory experiments.pdf},
  journal = {Science},
  language = {en},
  number = {6280},
  pmid = {26940865}
}





@book{chambersSevenDeadlySins2017,
  title = {The {{Seven Deadly Sins}} of {{Psychology}}: {{A Manifesto}} for {{Reforming}} the {{Culture}} of {{Scientific Practice}}},
  shorttitle = {The {{Seven Deadly Sins}} of {{Psychology}}},
  author = {Chambers, Chris},
  year = {2017},
  month = may,
  publisher = {{Princeton University Press}},
  address = {{Princeton}},
  abstract = {Why psychology is in peril as a scientific discipline--and how to save it Psychological science has made extraordinary discoveries about the human mind, but can we trust everything its practitioners are telling us? In recent years, it has become increasingly apparent that a lot of research in psychology is based on weak evidence, questionable practices, and sometimes even fraud. The Seven Deadly Sins of Psychology diagnoses the ills besetting the discipline today and proposes sensible, practical solutions to ensure that it remains a legitimate and reliable science in the years ahead. In this unflinchingly candid manifesto, Chris Chambers draws on his own experiences as a working scientist to reveal a dark side to psychology that few of us ever see. Using the seven deadly sins as a metaphor, he shows how practitioners are vulnerable to powerful biases that undercut the scientific method, how they routinely torture data until it produces outcomes that can be published in prestigious journals, and how studies are much less reliable than advertised. He reveals how a culture of secrecy denies the public and other researchers access to the results of psychology experiments, how fraudulent academics can operate with impunity, and how an obsession with bean counting creates perverse incentives for academics. Left unchecked, these problems threaten the very future of psychology as a science--but help is here. Outlining a core set of best practices that can be applied across the sciences, Chambers demonstrates how all these sins can be corrected by embracing open science, an emerging philosophy that seeks to make research and its outcomes as transparent as possible.},
  isbn = {978-0-691-15890-7},
  language = {English}
}





@article{lewandowskyRoleConspiracistIdeation2013,
  title = {The Role of Conspiracist Ideation and Worldviews in Predicting Rejection of Science},
  author = {Lewandowsky, Stephan and Gignac, Gilles E. and Oberauer, Klaus},
  year = {2013},
  volume = {8},
  pages = {e75637},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0075637},
  abstract = {BACKGROUND: Among American Conservatives, but not Liberals, trust in science has been declining since the 1970's. Climate science has become particularly polarized, with Conservatives being more likely than Liberals to reject the notion that greenhouse gas emissions are warming the globe. Conversely, opposition to genetically-modified (GM) foods and vaccinations is often ascribed to the political Left although reliable data are lacking. There are also growing indications that rejection of science is suffused by conspiracist ideation, that is the general tendency to endorse conspiracy theories including the specific beliefs that inconvenient scientific findings constitute a "hoax." METHODOLOGY/PRINCIPAL FINDINGS: We conducted a propensity weighted internet-panel survey of the U.S. population and show that conservatism and free-market worldview strongly predict rejection of climate science, in contrast to their weaker and opposing effects on acceptance of vaccinations. The two worldview variables do not predict opposition to GM. Conspiracist ideation, by contrast, predicts rejection of all three scientific propositions, albeit to greatly varying extents. Greater endorsement of a diverse set of conspiracy theories predicts opposition to GM foods, vaccinations, and climate science. CONCLUSIONS: Free-market worldviews are an important predictor of the rejection of scientific findings that have potential regulatory implications, such as climate science, but not necessarily of other scientific issues. Conspiracist ideation, by contrast, is associated with the rejection of all scientific propositions tested. We highlight the manifold cognitive reasons why conspiracist ideation would stand in opposition to the scientific method. The involvement of conspiracist ideation in the rejection of science has implications for science communicators.},
  file = {C\:\\Users\\singm\\Zotero\\storage\\K7BECK3V\\Lewandowsky et al. - 2013 - The role of conspiracist ideation and worldviews i.pdf},
  journal = {PloS One},
  keywords = {Attitude,Culture,Data Collection,Female,Humans,Male,Models; Statistical,Politics,Rejection; Psychology,Science},
  language = {eng},
  number = {10},
  pmcid = {PMC3788812},
  pmid = {24098391}
}





@article{lewandowskyCorrectionRoleConspiracist2015,
  title = {Correction: {{The Role}} of {{Conspiracist Ideation}} and {{Worldviews}} in {{Predicting Rejection}} of {{Science}}},
  shorttitle = {Correction},
  author = {Lewandowsky, Stephan and Gignac, Gilles E. and Oberauer, Klaus},
  year = {2015},
  month = aug,
  volume = {10},
  pages = {e0134773},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0134773},
  file = {C\:\\Users\\singm\\Zotero\\storage\\ZET3ZVXK\\Lewandowsky et al. - 2015 - Correction The Role of Conspiracist Ideation and .pdf},
  journal = {PLOS ONE},
  language = {en},
  number = {8}
}



